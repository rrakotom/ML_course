{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from helpers import *\n",
    "\n",
    "height, weight, gender = load_data(sub_sample=False, add_outlier=False)\n",
    "x, mean_x, std_x = standardize(height)\n",
    "y, tx = build_model_data(x, weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10000,), (10000, 2))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape, tx.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NB: throughout this laboratory the data has the following format: \n",
    "  * there are **N = 10000** data entries\n",
    "  * **y** represents the column vector containing weight information -- that which we wish to predict/the output (see also the first page of $\\texttt{exercise02.pdf}$). Its **shape** is **(N,)**.\n",
    "  * **tx** represents the matrix $\\tilde{X}$ formed by laterally concatenating a column vector of 1s to the column vector of height information -- the input data (see also the first page of $\\texttt{exercise02.pdf}$). Its **shape** is **(N,2)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Computing the Cost Function\n",
    "Fill in the `compute_cost` function below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(y, tx, w):\n",
    "\n",
    "    \"\"\"Calculate the loss using either MSE\n",
    "\n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        w: numpy array of shape=(2,). The vector of model parameters.\n",
    "\n",
    "    Returns:\n",
    "        the value of the loss (a scalar), corresponding to the input parameters w.\n",
    "    \"\"\"\n",
    "    error = y - np.dot(tx, w)\n",
    "    return np.dot(error.T, error)/len(error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Grid Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill in the function `grid_search()` below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from costs import *\n",
    "\n",
    "def grid_search(y, tx, grid_w0, grid_w1):\n",
    "    \"\"\"Algorithm for grid search.\n",
    "        \n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        grid_w0: numpy array of shape=(num_grid_pts_w0, ). A 1D array containing num_grid_pts_w0 values of parameter w0 to be tested in the grid search.\n",
    "        grid_w1: numpy array of shape=(num_grid_pts_w1, ). A 1D array containing num_grid_pts_w1 values of parameter w1 to be tested in the grid search.\n",
    "        \n",
    "    Returns:\n",
    "        losses: numpy array of shape=(num_grid_pts_w0, num_grid_pts_w1). A 2D array containing the loss value for each combination of w0 and w1\n",
    "    \"\"\"\n",
    "\n",
    "    losses = np.zeros((len(grid_w0), len(grid_w1)))\n",
    "    for i in range(len(grid_w0)):\n",
    "        for j in range(len(grid_w1)):\n",
    "            losses[i,j] = compute_loss_mse(y, tx, np.array([grid_w0[i], grid_w1[j]]).T)\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us play with the grid search demo now!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid Search: loss*=15.558703368609525, w0*=72.72727272727272, w1*=13.636363636363626, execution time=1.978 seconds\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x432 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from grid_search import generate_w, get_best_parameters\n",
    "from plots import grid_visualization\n",
    "\n",
    "# Generate the grid of parameters to be swept\n",
    "grid_w0, grid_w1 = generate_w(num_intervals=100)\n",
    "\n",
    "# Start the grid search\n",
    "start_time = datetime.datetime.now()\n",
    "grid_losses = grid_search(y, tx, grid_w0, grid_w1)\n",
    "\n",
    "# Select the best combinaison\n",
    "loss_star, w0_star, w1_star = get_best_parameters(grid_w0, grid_w1, grid_losses)\n",
    "end_time = datetime.datetime.now()\n",
    "execution_time = (end_time - start_time).total_seconds()\n",
    "\n",
    "# Print the results\n",
    "print(\"Grid Search: loss*={l}, w0*={w0}, w1*={w1}, execution time={t:.3f} seconds\".format(\n",
    "      l=loss_star, w0=w0_star, w1=w1_star, t=execution_time))\n",
    "\n",
    "# Plot the results\n",
    "fig = grid_visualization(grid_losses, grid_w0, grid_w1, mean_x, std_x, height, weight)\n",
    "fig.set_size_inches(10.0,6.0)\n",
    "fig.savefig(\"grid_plot\")  # Optional saving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, please fill in the functions `compute_gradient` below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(y, tx, w):\n",
    "    \"\"\"Computes the gradient at w.\n",
    "        \n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        w: numpy array of shape=(2, ). The vector of model parameters.\n",
    "        \n",
    "    Returns:\n",
    "        An numpy array of shape (2, ) (same shape as w), containing the gradient of the loss at w.\n",
    "    \"\"\"\n",
    "    error = y - np.dot(tx, w)\n",
    "    return -np.dot(tx.T, error)/len(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please fill in the functions `gradient_descent` below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(y, tx, initial_w, max_iters, gamma):\n",
    "    \"\"\"The Gradient Descent (GD) algorithm.\n",
    "        \n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        initial_w: numpy array of shape=(2, ). The initial guess (or the initialization) for the model parameters\n",
    "        max_iters: a scalar denoting the total number of iterations of GD\n",
    "        gamma: a scalar denoting the stepsize\n",
    "        \n",
    "    Returns:\n",
    "        losses: a list of length max_iters containing the loss value (scalar) for each iteration of GD\n",
    "        ws: a list of length max_iters containing the model parameters as numpy arrays of shape (2, ), for each iteration of GD \n",
    "    \"\"\"\n",
    "    # Define parameters to store w and loss\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        loss = compute_loss(y, tx, w)\n",
    "        \n",
    "        # store w and loss\n",
    "        ws.append(w)\n",
    "        losses.append(loss)\n",
    "        \n",
    "        #update w by gradient\n",
    "        w = w - gamma*compute_gradient(y, tx, w)\n",
    "        \n",
    "        print(\"GD iter. {bi}/{ti}: loss={l}, w0={w0}, w1={w1}\".format(\n",
    "              bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]))\n",
    "\n",
    "    return losses, ws"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your gradient descent function through gradient descent demo shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GD iter. 0/49: loss=5584.473425518336, w0=51.3057454014736, w1=9.435798704492266\n",
      "GD iter. 1/49: loss=530.6049242179216, w0=66.69746902191571, w1=12.266538315839991\n",
      "GD iter. 2/49: loss=75.75675910088255, w0=71.31498610804834, w1=13.115760199244328\n",
      "GD iter. 3/49: loss=34.820424240348956, w0=72.70024123388814, w1=13.37052676426563\n",
      "GD iter. 4/49: loss=31.136154102900914, w0=73.11581777164007, w1=13.446956733772023\n",
      "GD iter. 5/49: loss=30.80456979053059, w0=73.24049073296565, w1=13.469885724623941\n",
      "GD iter. 6/49: loss=30.77472720241726, w0=73.27789262136334, w1=13.476764421879516\n",
      "GD iter. 7/49: loss=30.772041369487056, w0=73.28911318788263, w1=13.478828031056189\n",
      "GD iter. 8/49: loss=30.77179964452334, w0=73.29247935783842, w1=13.47944711380919\n",
      "GD iter. 9/49: loss=30.771777889276603, w0=73.29348920882516, w1=13.47963283863509\n",
      "GD iter. 10/49: loss=30.7717759313044, w0=73.29379216412119, w1=13.479688556082861\n",
      "GD iter. 11/49: loss=30.771775755086903, w0=73.29388305071, w1=13.479705271317192\n",
      "GD iter. 12/49: loss=30.77177573922733, w0=73.29391031668663, w1=13.479710285887492\n",
      "GD iter. 13/49: loss=30.771775737799967, w0=73.29391849647962, w1=13.479711790258582\n",
      "GD iter. 14/49: loss=30.771775737671508, w0=73.29392095041752, w1=13.479712241569908\n",
      "GD iter. 15/49: loss=30.771775737659947, w0=73.29392168659889, w1=13.479712376963306\n",
      "GD iter. 16/49: loss=30.771775737658906, w0=73.2939219074533, w1=13.479712417581325\n",
      "GD iter. 17/49: loss=30.771775737658817, w0=73.29392197370962, w1=13.479712429766732\n",
      "GD iter. 18/49: loss=30.771775737658807, w0=73.29392199358652, w1=13.479712433422353\n",
      "GD iter. 19/49: loss=30.771775737658814, w0=73.2939219995496, w1=13.47971243451904\n",
      "GD iter. 20/49: loss=30.771775737658807, w0=73.29392200133852, w1=13.479712434848047\n",
      "GD iter. 21/49: loss=30.771775737658807, w0=73.29392200187519, w1=13.479712434946748\n",
      "GD iter. 22/49: loss=30.771775737658796, w0=73.29392200203618, w1=13.479712434976358\n",
      "GD iter. 23/49: loss=30.771775737658796, w0=73.29392200208449, w1=13.479712434985242\n",
      "GD iter. 24/49: loss=30.771775737658796, w0=73.29392200209898, w1=13.479712434987906\n",
      "GD iter. 25/49: loss=30.771775737658807, w0=73.29392200210333, w1=13.479712434988706\n",
      "GD iter. 26/49: loss=30.7717757376588, w0=73.29392200210464, w1=13.479712434988945\n",
      "GD iter. 27/49: loss=30.7717757376588, w0=73.29392200210502, w1=13.479712434989018\n",
      "GD iter. 28/49: loss=30.771775737658796, w0=73.29392200210513, w1=13.47971243498904\n",
      "GD iter. 29/49: loss=30.771775737658796, w0=73.29392200210518, w1=13.479712434989047\n",
      "GD iter. 30/49: loss=30.771775737658782, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 31/49: loss=30.77177573765879, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 32/49: loss=30.77177573765879, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 33/49: loss=30.77177573765879, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 34/49: loss=30.77177573765879, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 35/49: loss=30.77177573765879, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 36/49: loss=30.77177573765879, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 37/49: loss=30.77177573765879, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 38/49: loss=30.77177573765879, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 39/49: loss=30.77177573765879, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 40/49: loss=30.77177573765879, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 41/49: loss=30.77177573765879, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 42/49: loss=30.77177573765879, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 43/49: loss=30.77177573765879, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 44/49: loss=30.77177573765879, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 45/49: loss=30.77177573765879, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 46/49: loss=30.77177573765879, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 47/49: loss=30.77177573765879, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 48/49: loss=30.77177573765879, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 49/49: loss=30.77177573765879, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD: execution time=0.087 seconds\n"
     ]
    }
   ],
   "source": [
    "# from gradient_descent import *\n",
    "from plots import gradient_descent_visualization\n",
    "\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters = 50\n",
    "gamma = 0.7\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.array([0, 0])\n",
    "\n",
    "# Start gradient descent.\n",
    "start_time = datetime.datetime.now()\n",
    "gd_losses, gd_ws = gradient_descent(y, tx, w_initial, max_iters, gamma)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"GD: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1caf5b541fb4aa48e0b56552e6f5f61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='n_iter', max=51, min=1), Output()), _dom_classes=('widge…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_figure(n_iter)>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Time Visualization\n",
    "from ipywidgets import IntSlider, interact\n",
    "def plot_figure(n_iter):\n",
    "    fig = gradient_descent_visualization(\n",
    "        gd_losses, gd_ws, grid_losses, grid_w0, grid_w1, mean_x, std_x, height, weight, n_iter)\n",
    "    fig.set_size_inches(10.0, 6.0)\n",
    "\n",
    "interact(plot_figure, n_iter=IntSlider(min=1, max=len(gd_ws)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 4. Stochastic gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def compute_stoch_gradient(y, tx, w, n = random.randint(1, len(y))):\n",
    "    \"\"\"Compute a stochastic gradient at w from just few examples n and their corresponding y_n labels.\n",
    "        \n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        w: numpy array of shape=(2, ). The vector of model parameters.\n",
    "        \n",
    "    Returns:\n",
    "        A numpy array of shape (2, ) (same shape as w), containing the stochastic gradient of the loss at w.\n",
    "    \"\"\"\n",
    "    for i,j in batch_iter(y, tx, n):\n",
    "        y = i\n",
    "        tx = j\n",
    "     \n",
    "    return compute_gradient(y, tx, w)\n",
    "\n",
    "\n",
    "def stochastic_gradient_descent(y, tx, initial_w, batch_size, max_iters, gamma):\n",
    "    \"\"\"The Stochastic Gradient Descent algorithm (SGD).\n",
    "            \n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        initial_w: numpy array of shape=(2, ). The initial guess (or the initialization) for the model parameters\n",
    "        batch_size: a scalar denoting the number of data points in a mini-batch used for computing the stochastic gradient\n",
    "        max_iters: a scalar denoting the total number of iterations of SGD\n",
    "        gamma: a scalar denoting the stepsize\n",
    "        \n",
    "    Returns:\n",
    "        losses: a list of length max_iters containing the loss value (scalar) for each iteration of SGD\n",
    "        ws: a list of length max_iters containing the model parameters as numpy arrays of shape (2, ), for each iteration of SGD \n",
    "    \"\"\"\n",
    "    \n",
    "    # Define parameters to store w and loss\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    \n",
    "    for n_iter in range(max_iters):\n",
    "        loss = compute_loss(y, tx, w)\n",
    "        \n",
    "        # store w and loss\n",
    "        ws.append(w)\n",
    "        losses.append(loss)\n",
    "        \n",
    "        #update w by gradient\n",
    "        w = w - gamma*compute_stoch_gradient(y, tx, w, batch_size)\n",
    "\n",
    "        print(\"SGD iter. {bi}/{ti}: loss={l}, w0={w0}, w1={w1}\".format(\n",
    "              bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]))\n",
    "    return losses, ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD iter. 0/49: loss=5584.473425518336, w0=4.415780187511653, w1=-6.8248722198156875\n",
      "SGD iter. 1/49: loss=5187.246353573045, w0=8.231032673049352, w1=-11.83907409971299\n",
      "SGD iter. 2/49: loss=4904.99229517243, w0=14.11656165560694, w1=-12.583440378411577\n",
      "SGD iter. 3/49: loss=4212.01968789163, w0=20.514119278021262, w1=-10.185431550754304\n",
      "SGD iter. 4/49: loss=3376.5183911968375, w0=29.310909441771642, w1=1.8160199150737828\n",
      "SGD iter. 5/49: loss=2101.3188928192435, w0=32.0762984577965, w1=0.3128225089162828\n",
      "SGD iter. 6/49: loss=1903.0312567033238, w0=35.98479447817287, w1=0.3682468054888608\n",
      "SGD iter. 7/49: loss=1594.6533032882664, w0=38.564137558377226, w1=-2.8874465291264126\n",
      "SGD iter. 8/49: loss=1504.8135958020916, w0=41.23838279744327, w1=-5.309163933037414\n",
      "SGD iter. 9/49: loss=1411.3512446122584, w0=40.36762082644168, w1=-3.2360818116792744\n",
      "SGD iter. 10/49: loss=1394.3308621451076, w0=43.72512813344013, w1=-3.12065818629251\n",
      "SGD iter. 11/49: loss=1180.6576513491702, w0=48.350877851050235, w1=0.7376187560280938\n",
      "SGD iter. 12/49: loss=815.2881785825515, w0=48.61090835735384, w1=0.3725605070080491\n",
      "SGD iter. 13/49: loss=811.8203699878153, w0=52.316837219649194, w1=4.022143599849392\n",
      "SGD iter. 14/49: loss=560.2554699794099, w0=56.132568599598635, w1=8.834899355886726\n",
      "SGD iter. 15/49: loss=346.8581148831818, w0=58.690206867068326, w1=12.544997072371556\n",
      "SGD iter. 16/49: loss=244.91396429207654, w0=61.21214233998863, w1=10.86915088839472\n",
      "SGD iter. 17/49: loss=183.55620713014895, w0=61.66352850932861, w1=10.15425538224523\n",
      "SGD iter. 18/49: loss=177.09649314412218, w0=62.19199500972507, w1=9.901317439613319\n",
      "SGD iter. 19/49: loss=166.82946942472708, w0=63.21838007569226, w1=9.169587570851565\n",
      "SGD iter. 20/49: loss=150.86549719301968, w0=63.80675788903452, w1=8.693528925093398\n",
      "SGD iter. 21/49: loss=143.68561123639165, w0=64.4770887862115, w1=9.451299248695884\n",
      "SGD iter. 22/49: loss=124.73643649404566, w0=65.56027457843693, w1=9.8912645968493\n",
      "SGD iter. 23/49: loss=103.45803609831925, w0=66.99358755325724, w1=8.951883348063065\n",
      "SGD iter. 24/49: loss=90.96722614541189, w0=67.83995708266409, w1=7.7315436422736346\n",
      "SGD iter. 25/49: loss=93.5589535497003, w0=67.5640925958799, w1=8.07381617095673\n",
      "SGD iter. 26/49: loss=92.8264351795814, w0=68.4159837068704, w1=7.306123612544727\n",
      "SGD iter. 27/49: loss=92.67925669838621, w0=69.3986166933347, w1=7.332613184806492\n",
      "SGD iter. 28/49: loss=83.73200837778923, w0=70.26788845550905, w1=7.946075981268132\n",
      "SGD iter. 29/49: loss=70.549787164733, w0=71.47534896444333, w1=8.530466021612902\n",
      "SGD iter. 30/49: loss=58.57402369128614, w0=71.10665268881472, w1=8.722047754617318\n",
      "SGD iter. 31/49: loss=58.19129599737759, w0=71.5141293310371, w1=8.859100584685756\n",
      "SGD iter. 32/49: loss=55.28949156080971, w0=71.7409052495704, w1=8.784109474080793\n",
      "SGD iter. 33/49: loss=55.23232393780287, w0=71.25015931631019, w1=9.428962432447463\n",
      "SGD iter. 34/49: loss=51.35731723659743, w0=71.06359935460976, w1=9.613706135780413\n",
      "SGD iter. 35/49: loss=50.69211955511062, w0=70.76367983693977, w1=9.545946004711668\n",
      "SGD iter. 36/49: loss=52.648419480016976, w0=70.95247661517686, w1=9.71184948285151\n",
      "SGD iter. 37/49: loss=50.4509334637173, w0=71.1529876946116, w1=9.56901790341994\n",
      "SGD iter. 38/49: loss=50.648907165906365, w0=70.43880419791071, w1=10.383561618927796\n",
      "SGD iter. 39/49: loss=48.50962328928383, w0=68.9837430364034, w1=10.601845091702097\n",
      "SGD iter. 40/49: loss=57.63153889959443, w0=68.9341915389652, w1=10.661111564476755\n",
      "SGD iter. 41/49: loss=57.72353631614232, w0=69.85296880221031, w1=12.190517748815468\n",
      "SGD iter. 42/49: loss=44.273957600383774, w0=70.67598398384318, w1=12.448912349422919\n",
      "SGD iter. 43/49: loss=38.687924021523536, w0=70.92895340282314, w1=12.915199728734239\n",
      "SGD iter. 44/49: loss=36.68352680877204, w0=71.10904948370303, w1=13.143862831637582\n",
      "SGD iter. 45/49: loss=35.658238615399114, w0=70.59384999440503, w1=13.403453854771294\n",
      "SGD iter. 46/49: loss=38.06797995548158, w0=70.71628575054618, w1=13.710853825385563\n",
      "SGD iter. 47/49: loss=37.46941072536442, w0=71.14378295434666, w1=13.50169126045684\n",
      "SGD iter. 48/49: loss=35.39535673112369, w0=71.3499730056843, w1=13.094082452682839\n",
      "SGD iter. 49/49: loss=34.69942392159806, w0=72.26818045897704, w1=12.265197640650312\n",
      "SGD: execution time=0.259 seconds\n"
     ]
    }
   ],
   "source": [
    "# from stochastic_gradient_descent import *\n",
    "\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters = 50\n",
    "gamma = 0.1\n",
    "batch_size = 1\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.array([0, 0])\n",
    "\n",
    "# Start SGD.\n",
    "start_time = datetime.datetime.now()\n",
    "sgd_losses, sgd_ws = stochastic_gradient_descent(\n",
    "    y, tx, w_initial, batch_size, max_iters, gamma)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"SGD: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c26a67a8270c4e62b9ce076f5daba33f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='n_iter', max=51, min=1), Output()), _dom_classes=('widge…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_figure(n_iter)>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Time Visualization\n",
    "from ipywidgets import IntSlider, interact\n",
    "def plot_figure(n_iter):\n",
    "    fig = gradient_descent_visualization(\n",
    "        sgd_losses, sgd_ws, grid_losses, grid_w0, grid_w1, mean_x, std_x, height, weight, n_iter)\n",
    "    fig.set_size_inches(10.0, 6.0)\n",
    "\n",
    "interact(plot_figure, n_iter=IntSlider(min=1, max=len(sgd_ws)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Effect of Outliers and MAE Cost Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from helpers import *\n",
    "\n",
    "height, weight, gender = load_data(sub_sample=True, add_outlier=True)\n",
    "x, mean_x, std_x = standardize(height)\n",
    "y, tx = build_model_data(x, weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((202,), (202, 2))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape, tx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD iter. 0/49: loss=5739.670229071707, w0=68.55235655491501, w1=99.15004846754914\n",
      "SGD iter. 1/49: loss=7926.561935412722, w0=130.8028257987606, w1=33.86581834549257\n",
      "SGD iter. 2/49: loss=3871.975015159421, w0=103.5693839045139, w1=48.73239805482594\n",
      "SGD iter. 3/49: loss=2423.306314293478, w0=44.79749884083386, w1=-30.770199963053237\n",
      "SGD iter. 4/49: loss=2736.278286569666, w0=80.2901138107718, w1=-20.063376317769915\n",
      "SGD iter. 5/49: loss=1137.6810471212004, w0=89.2060597334921, w1=-16.68189298577732\n",
      "SGD iter. 6/49: loss=1129.2485195169431, w0=104.24395153806037, w1=-0.9573153258592999\n",
      "SGD iter. 7/49: loss=1186.2743337804056, w0=76.1212336949691, w1=21.304108812521747\n",
      "SGD iter. 8/49: loss=241.53478117913002, w0=78.10148992768025, w1=19.085563458813265\n",
      "SGD iter. 9/49: loss=212.94533219548057, w0=70.57711751966482, w1=12.587385320172562\n",
      "SGD iter. 10/49: loss=146.4565918694685, w0=74.18187391447219, w1=12.420387215705922\n",
      "SGD iter. 11/49: loss=133.79406077853912, w0=70.91870587486456, w1=14.053541907222897\n",
      "SGD iter. 12/49: loss=150.89052084918157, w0=66.74059416275148, w1=10.780322622924402\n",
      "SGD iter. 13/49: loss=185.61429841409011, w0=68.2906935858824, w1=10.335777277818911\n",
      "SGD iter. 14/49: loss=165.72525177643408, w0=78.52035955163001, w1=26.67854871058603\n",
      "SGD iter. 15/49: loss=396.41060023680836, w0=66.1651579675856, w1=11.969900396180577\n",
      "SGD iter. 16/49: loss=195.18753917798688, w0=71.16410206455616, w1=11.253786662883183\n",
      "SGD iter. 17/49: loss=140.34086952616457, w0=67.69412261674893, w1=14.079143688896787\n",
      "SGD iter. 18/49: loss=181.75274912160558, w0=67.71619590617625, w1=14.052611011332942\n",
      "SGD iter. 19/49: loss=181.31101988013702, w0=67.35932166682854, w1=14.145328898248286\n",
      "SGD iter. 20/49: loss=186.54002017620147, w0=71.08784077566328, w1=21.403964067261644\n",
      "SGD iter. 21/49: loss=248.2592481796129, w0=70.53954529016296, w1=21.705749145514194\n",
      "SGD iter. 22/49: loss=258.1772138729302, w0=68.86877302582884, w1=19.807824113850984\n",
      "SGD iter. 23/49: loss=235.85569015123085, w0=160.79855822699716, w1=-344.0108274259583\n",
      "SGD iter. 24/49: loss=133711.54978504125, w0=326.1490828648202, w1=-196.73355867528602\n",
      "SGD iter. 25/49: loss=106844.56196607686, w0=279.64913863839115, w1=-237.86165636120973\n",
      "SGD iter. 26/49: loss=104345.0390620244, w0=205.05887468480267, w1=-266.6469396628095\n",
      "SGD iter. 27/49: loss=94397.7228006776, w0=262.20744835420044, w1=-223.40171803575146\n",
      "SGD iter. 28/49: loss=90488.91200880031, w0=329.67256954460373, w1=-143.08568803589492\n",
      "SGD iter. 29/49: loss=89218.81075507741, w0=60.62588760629495, w1=81.49623973792725\n",
      "SGD iter. 30/49: loss=5277.3477475703, w0=129.2928705045691, w1=-2.2623991892829025\n",
      "SGD iter. 31/49: loss=3358.4872549547963, w0=97.19076892309184, w1=-7.482215894182907\n",
      "SGD iter. 32/49: loss=1009.4162721614282, w0=68.91785634179696, w1=22.379343544620877\n",
      "SGD iter. 33/49: loss=287.0799560150987, w0=66.6422190099831, w1=21.225180999354336\n",
      "SGD iter. 34/49: loss=290.84273167685694, w0=76.90634945545625, w1=11.278549493672108\n",
      "SGD iter. 35/49: loss=139.97815755490726, w0=68.41408665088322, w1=24.499729120891416\n",
      "SGD iter. 36/49: loss=345.12776255536363, w0=75.70404904105841, w1=26.698844113079616\n",
      "SGD iter. 37/49: loss=379.89805798479796, w0=66.64802794135566, w1=15.323861068756111\n",
      "SGD iter. 38/49: loss=205.30979558039593, w0=69.34283495035552, w1=13.684701791535923\n",
      "SGD iter. 39/49: loss=161.2082869969236, w0=66.47010277760691, w1=11.434143885387957\n",
      "SGD iter. 40/49: loss=189.74595203580716, w0=77.9220298213464, w1=15.984272107692727\n",
      "SGD iter. 41/49: loss=171.21283766922716, w0=71.16736738005578, w1=5.790509649795375\n",
      "SGD iter. 42/49: loss=167.77757984754768, w0=83.67063399017437, w1=21.49546926980151\n",
      "SGD iter. 43/49: loss=333.4993852580129, w0=72.77620788524767, w1=28.215658322322223\n",
      "SGD iter. 44/49: loss=428.70831846295647, w0=84.00232556873911, w1=22.087385075919236\n",
      "SGD iter. 45/49: loss=352.71368198994253, w0=70.91130043796106, w1=5.645575313493904\n",
      "SGD iter. 46/49: loss=170.86975189144516, w0=75.36360677459385, w1=11.739733794311533\n",
      "SGD iter. 47/49: loss=134.03735814349653, w0=65.69050061435807, w1=10.961606816918346\n",
      "SGD iter. 48/49: loss=202.04607443699638, w0=77.48089729482072, w1=25.77115295749801\n",
      "SGD iter. 49/49: loss=360.66795592184883, w0=70.44450279943739, w1=22.871180722833795\n",
      "GD: execution time=0.031 seconds\n"
     ]
    }
   ],
   "source": [
    "from plots import gradient_descent_visualization\n",
    "\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters = 50\n",
    "gamma = 0.7\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.array([0, 0])\n",
    "\n",
    "# Start gradient descent.\n",
    "start_time = datetime.datetime.now()\n",
    "sgd_losses, sgd_ws = stochastic_gradient_descent(\n",
    "    y, tx, w_initial, batch_size, max_iters, gamma)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"GD: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5194d32268f7448d9e9efff3a4c2fbce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='n_iter', max=51, min=1), Output()), _dom_classes=('widge…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_figure(n_iter)>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Time Visualization\n",
    "from ipywidgets import IntSlider, interact\n",
    "def plot_figure(n_iter):\n",
    "    fig = gradient_descent_visualization(\n",
    "        gd_losses, gd_ws, grid_losses, grid_w0, grid_w1, mean_x, std_x, height, weight, n_iter)\n",
    "    fig.set_size_inches(10.0, 6.0)\n",
    "\n",
    "interact(plot_figure, n_iter=IntSlider(min=1, max=len(gd_ws)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 6. Subgradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss_mae(y, tx, w):\n",
    "\n",
    "    \"\"\"Calculate the loss using MAE.\n",
    "\n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        w: numpy array of shape=(2,). The vector of model parameters.\n",
    "\n",
    "    Returns:\n",
    "        the value of the loss (a scalar), corresponding to the input parameters w.\n",
    "    \"\"\"\n",
    "    error = y - np.dot(tx, w)\n",
    "    return np.sum(np.abs(error))/len(error)\n",
    "\n",
    "def compute_subgradient_mae(y, tx, w):\n",
    "    \"\"\"Compute a subgradient of the MAE at w.\n",
    "        \n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        w: numpy array of shape=(2, ). The vector of model parameters.\n",
    "        \n",
    "    Returns:\n",
    "        A numpy array of shape (2, ) (same shape as w), containing the subgradient of the MAE at w.\n",
    "    \"\"\"\n",
    "    error = y - np.dot(tx, w)\n",
    "    return -np.dot(tx.T, np.sign(error))/len(error)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subgradient_descent(y, tx, initial_w, max_iters, gamma):\n",
    "    \"\"\"The SubGradient Descent (SubGD) algorithm.\n",
    "        \n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        initial_w: numpy array of shape=(2, ). The initial guess (or the initialization) for the model parameters\n",
    "        max_iters: a scalar denoting the total number of iterations of GD\n",
    "        gamma: a scalar denoting the stepsize\n",
    "        \n",
    "    Returns:\n",
    "        losses: a list of length max_iters containing the loss value (scalar) for each iteration of SubGD\n",
    "        ws: a list of length max_iters containing the model parameters as numpy arrays of shape (2, ), for each iteration of SubGD \n",
    "    \"\"\"\n",
    "    # Define parameters to store w and loss\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        loss = compute_loss(y, tx, w)\n",
    "        \n",
    "        # store w and loss\n",
    "        ws.append(w)\n",
    "        losses.append(loss)\n",
    "        \n",
    "        #update w by gradient\n",
    "        w = w - gamma*compute_subgradient_mae(y, tx, w)\n",
    "        \n",
    "        print(\"SubGD iter. {bi}/{ti}: loss={l}, w0={w0}, w1={w1}\".format(\n",
    "              bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]))\n",
    "\n",
    "    return losses, ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SubGD iter. 0/499: loss=5739.670229071707, w0=0.7, w1=8.756471895211877e-16\n",
      "SubGD iter. 1/499: loss=5636.465300874807, w0=1.4, w1=1.7512943790423754e-15\n",
      "SubGD iter. 2/499: loss=5534.240372677911, w0=2.0999999999999996, w1=2.626941568563563e-15\n",
      "SubGD iter. 3/499: loss=5432.995444481015, w0=2.8, w1=3.502588758084751e-15\n",
      "SubGD iter. 4/499: loss=5332.730516284118, w0=3.5, w1=4.378235947605939e-15\n",
      "SubGD iter. 5/499: loss=5233.445588087222, w0=4.2, w1=5.253883137127127e-15\n",
      "SubGD iter. 6/499: loss=5135.140659890325, w0=4.9, w1=6.1295303266483146e-15\n",
      "SubGD iter. 7/499: loss=5037.815731693427, w0=5.6000000000000005, w1=7.0051775161695025e-15\n",
      "SubGD iter. 8/499: loss=4941.470803496531, w0=6.300000000000001, w1=7.88082470569069e-15\n",
      "SubGD iter. 9/499: loss=4846.105875299633, w0=7.000000000000001, w1=8.756471895211878e-15\n",
      "SubGD iter. 10/499: loss=4751.720947102735, w0=7.700000000000001, w1=9.632119084733065e-15\n",
      "SubGD iter. 11/499: loss=4658.316018905839, w0=8.4, w1=1.0507766274254253e-14\n",
      "SubGD iter. 12/499: loss=4565.891090708942, w0=9.1, w1=1.1383413463775441e-14\n",
      "SubGD iter. 13/499: loss=4474.446162512046, w0=9.799999999999999, w1=1.2259060653296629e-14\n",
      "SubGD iter. 14/499: loss=4383.981234315148, w0=10.499999999999998, w1=1.3134707842817817e-14\n",
      "SubGD iter. 15/499: loss=4294.496306118252, w0=11.199999999999998, w1=1.4010355032339005e-14\n",
      "SubGD iter. 16/499: loss=4205.991377921354, w0=11.899999999999997, w1=1.488600222186019e-14\n",
      "SubGD iter. 17/499: loss=4118.466449724458, w0=12.599999999999996, w1=1.576164941138138e-14\n",
      "SubGD iter. 18/499: loss=4031.9215215275603, w0=13.299999999999995, w1=1.6637296600902567e-14\n",
      "SubGD iter. 19/499: loss=3946.356593330664, w0=13.999999999999995, w1=1.7512943790423755e-14\n",
      "SubGD iter. 20/499: loss=3861.7716651337673, w0=14.699999999999994, w1=1.8388590979944943e-14\n",
      "SubGD iter. 21/499: loss=3778.1667369368693, w0=15.399999999999993, w1=1.926423816946613e-14\n",
      "SubGD iter. 22/499: loss=3695.5418087399744, w0=16.099999999999994, w1=2.013988535898732e-14\n",
      "SubGD iter. 23/499: loss=3613.8968805430763, w0=16.799999999999994, w1=2.1015532548508507e-14\n",
      "SubGD iter. 24/499: loss=3533.2319523461797, w0=17.499999999999993, w1=2.1891179738029695e-14\n",
      "SubGD iter. 25/499: loss=3453.547024149282, w0=18.199999999999992, w1=2.2766826927550882e-14\n",
      "SubGD iter. 26/499: loss=3374.8420959523855, w0=18.89999999999999, w1=2.364247411707207e-14\n",
      "SubGD iter. 27/499: loss=3297.1171677554894, w0=19.59999999999999, w1=2.4518121306593258e-14\n",
      "SubGD iter. 28/499: loss=3220.3722395585924, w0=20.29999999999999, w1=2.5393768496114446e-14\n",
      "SubGD iter. 29/499: loss=3144.6073113616953, w0=20.99999999999999, w1=2.6269415685635634e-14\n",
      "SubGD iter. 30/499: loss=3069.8223831647983, w0=21.69999999999999, w1=2.7145062875156822e-14\n",
      "SubGD iter. 31/499: loss=2996.017454967901, w0=22.399999999999988, w1=2.802071006467801e-14\n",
      "SubGD iter. 32/499: loss=2923.1925267710044, w0=23.099999999999987, w1=2.8896357254199195e-14\n",
      "SubGD iter. 33/499: loss=2851.347598574107, w0=23.799999999999986, w1=2.977200444372038e-14\n",
      "SubGD iter. 34/499: loss=2780.482670377211, w0=24.499999999999986, w1=3.064765163324157e-14\n",
      "SubGD iter. 35/499: loss=2710.597742180314, w0=25.199999999999985, w1=3.152329882276276e-14\n",
      "SubGD iter. 36/499: loss=2641.6928139834176, w0=25.899999999999984, w1=3.2398946012283946e-14\n",
      "SubGD iter. 37/499: loss=2573.7678857865203, w0=26.599999999999984, w1=3.3274593201805134e-14\n",
      "SubGD iter. 38/499: loss=2506.8229575896235, w0=27.299999999999983, w1=3.415024039132632e-14\n",
      "SubGD iter. 39/499: loss=2440.858029392726, w0=27.999999999999982, w1=3.502588758084751e-14\n",
      "SubGD iter. 40/499: loss=2375.873101195829, w0=28.69999999999998, w1=3.59015347703687e-14\n",
      "SubGD iter. 41/499: loss=2311.868172998932, w0=29.39999999999998, w1=3.6777181959889886e-14\n",
      "SubGD iter. 42/499: loss=2248.8432448020358, w0=30.09999999999998, w1=3.7652829149411074e-14\n",
      "SubGD iter. 43/499: loss=2186.7983166051386, w0=30.79999999999998, w1=3.852847633893226e-14\n",
      "SubGD iter. 44/499: loss=2125.733388408242, w0=31.49999999999998, w1=3.940412352845345e-14\n",
      "SubGD iter. 45/499: loss=2065.6484602113446, w0=32.19999999999998, w1=4.027977071797464e-14\n",
      "SubGD iter. 46/499: loss=2006.5435320144472, w0=32.899999999999984, w1=4.1155417907495825e-14\n",
      "SubGD iter. 47/499: loss=1948.4186038175503, w0=33.59999999999999, w1=4.2031065097017013e-14\n",
      "SubGD iter. 48/499: loss=1891.273675620653, w0=34.29999999999999, w1=4.29067122865382e-14\n",
      "SubGD iter. 49/499: loss=1835.108747423756, w0=34.99999999999999, w1=4.378235947605939e-14\n",
      "SubGD iter. 50/499: loss=1779.9238192268592, w0=35.699999999999996, w1=4.465800666558058e-14\n",
      "SubGD iter. 51/499: loss=1725.7188910299617, w0=36.4, w1=4.5533653855101765e-14\n",
      "SubGD iter. 52/499: loss=1672.4939628330646, w0=37.1, w1=4.640930104462295e-14\n",
      "SubGD iter. 53/499: loss=1620.2490346361672, w0=37.800000000000004, w1=4.728494823414414e-14\n",
      "SubGD iter. 54/499: loss=1568.98410643927, w0=38.50000000000001, w1=4.816059542366533e-14\n",
      "SubGD iter. 55/499: loss=1518.6991782423734, w0=39.20000000000001, w1=4.9036242613186517e-14\n",
      "SubGD iter. 56/499: loss=1469.3942500454755, w0=39.90000000000001, w1=4.9911889802707705e-14\n",
      "SubGD iter. 57/499: loss=1421.0693218485785, w0=40.600000000000016, w1=5.078753699222889e-14\n",
      "SubGD iter. 58/499: loss=1373.7243936516816, w0=41.30000000000002, w1=5.166318418175008e-14\n",
      "SubGD iter. 59/499: loss=1327.3594654547846, w0=42.00000000000002, w1=5.253883137127127e-14\n",
      "SubGD iter. 60/499: loss=1281.9745372578873, w0=42.700000000000024, w1=5.3414478560792456e-14\n",
      "SubGD iter. 61/499: loss=1237.5696090609904, w0=43.40000000000003, w1=5.4290125750313644e-14\n",
      "SubGD iter. 62/499: loss=1194.144680864093, w0=44.10000000000003, w1=5.516577293983483e-14\n",
      "SubGD iter. 63/499: loss=1151.699752667196, w0=44.80000000000003, w1=5.604142012935602e-14\n",
      "SubGD iter. 64/499: loss=1110.234824470299, w0=45.500000000000036, w1=5.691706731887721e-14\n",
      "SubGD iter. 65/499: loss=1069.7498962734019, w0=46.20000000000004, w1=5.779271450839839e-14\n",
      "SubGD iter. 66/499: loss=1030.2449680765048, w0=46.90000000000004, w1=5.866836169791957e-14\n",
      "SubGD iter. 67/499: loss=991.7200398796078, w0=47.59306930693074, w1=0.01114784567828894\n",
      "SubGD iter. 68/499: loss=954.2961338587947, w0=48.279207920792125, w1=0.03308574108991741\n",
      "SubGD iter. 69/499: loss=917.9530476343554, w0=48.96534653465351, w1=0.055023636501545875\n",
      "SubGD iter. 70/499: loss=882.5524963472895, w0=49.63069306930698, w1=0.1053832638830964\n",
      "SubGD iter. 71/499: loss=848.4881653628598, w0=50.28910891089114, w1=0.16746568532795278\n",
      "SubGD iter. 72/499: loss=815.388905558163, w0=50.947524752475296, w1=0.22954810677280915\n",
      "SubGD iter. 73/499: loss=783.1643770484698, w0=51.59207920792084, w1=0.312425129327494\n",
      "SubGD iter. 74/499: loss=751.9911057697458, w0=52.22277227722777, w1=0.41195013288401805\n",
      "SubGD iter. 75/499: loss=721.9139070185913, w0=52.84653465346539, w1=0.5208167847923948\n",
      "SubGD iter. 76/499: loss=692.7496495086654, w0=53.4564356435644, w1=0.6457900912636185\n",
      "SubGD iter. 77/499: loss=664.6235402152506, w0=54.0594059405941, w1=0.7796904498577408\n",
      "SubGD iter. 78/499: loss=637.366744953699, w0=54.655445544554496, w1=0.9197570104995888\n",
      "SubGD iter. 79/499: loss=611.0172068604652, w0=55.24455445544559, w1=1.067092029785011\n",
      "SubGD iter. 80/499: loss=585.5333468347008, w0=55.819801980198065, w1=1.2261255948210965\n",
      "SubGD iter. 81/499: loss=561.0630602323158, w0=56.36732673267331, w1=1.410709342622233\n",
      "SubGD iter. 82/499: loss=537.7933683507077, w0=56.900990099009945, w1=1.605853732220289\n",
      "SubGD iter. 83/499: loss=515.4678401050918, w0=57.42772277227727, w1=1.808762802293982\n",
      "SubGD iter. 84/499: loss=493.8753380594149, w0=57.933663366336674, w1=2.0285064197514897\n",
      "SubGD iter. 85/499: loss=473.28704689184457, w0=58.43267326732677, w1=2.2494370848672975\n",
      "SubGD iter. 86/499: loss=453.5030996608995, w0=58.91089108910895, w1=2.4837982986028537\n",
      "SubGD iter. 87/499: loss=434.71477792822577, w0=59.382178217821824, w1=2.7260245553531703\n",
      "SubGD iter. 88/499: loss=416.56644513986305, w0=59.83960396039608, w1=2.978742333469156\n",
      "SubGD iter. 89/499: loss=399.2047829839497, w0=60.262376237623805, w1=3.251528669355458\n",
      "SubGD iter. 90/499: loss=383.0321364744009, w0=60.67821782178222, w1=3.5270865794243\n",
      "SubGD iter. 91/499: loss=367.50971317033503, w0=61.087128712871326, w1=3.806459183951836\n",
      "SubGD iter. 92/499: loss=352.6097216808267, w0=61.49603960396043, w1=4.085831788479371\n",
      "SubGD iter. 93/499: loss=338.20024452934194, w0=61.891089108910926, w1=4.373839384328629\n",
      "SubGD iter. 94/499: loss=324.5035510476589, w0=62.27920792079211, w1=4.666037469532069\n",
      "SubGD iter. 95/499: loss=311.39484599428675, w0=62.65346534653469, w1=4.959829093241791\n",
      "SubGD iter. 96/499: loss=299.05505358992394, w0=63.02079207920796, w1=5.257057192056662\n",
      "SubGD iter. 97/499: loss=287.2813817524322, w0=63.38118811881192, w1=5.5604343163524295\n",
      "SubGD iter. 98/499: loss=276.03497715257095, w0=63.74158415841588, w1=5.863811440648197\n",
      "SubGD iter. 99/499: loss=265.2324185225261, w0=64.08811881188123, w1=6.172402175278572\n",
      "SubGD iter. 100/499: loss=255.0994488495431, w0=64.42772277227726, w1=6.4863693105165225\n",
      "SubGD iter. 101/499: loss=245.4817467743714, w0=64.7673267326733, w1=6.800336445754473\n",
      "SubGD iter. 102/499: loss=236.291857123052, w0=65.10693069306933, w1=7.1143035809924235\n",
      "SubGD iter. 103/499: loss=227.5297798955851, w0=65.44653465346536, w1=7.428270716230374\n",
      "SubGD iter. 104/499: loss=219.19551509197052, w0=65.76534653465349, w1=7.747893210218651\n",
      "SubGD iter. 105/499: loss=211.59667085503045, w0=66.070297029703, w1=8.073669686866932\n",
      "SubGD iter. 106/499: loss=204.59046217619957, w0=66.37524752475251, w1=8.399446163515213\n",
      "SubGD iter. 107/499: loss=197.9825037317053, w0=66.6663366336634, w1=8.73297028041742\n",
      "SubGD iter. 108/499: loss=191.94206363616908, w0=66.9574257425743, w1=9.066494397319628\n",
      "SubGD iter. 109/499: loss=186.2935659523967, w0=67.23465346534658, w1=9.398630319470323\n",
      "SubGD iter. 110/499: loss=181.23079345062098, w0=67.51188118811886, w1=9.730766241621017\n",
      "SubGD iter. 111/499: loss=176.54235991095803, w0=67.78910891089114, w1=10.062902163771712\n",
      "SubGD iter. 112/499: loss=172.22826533340788, w0=68.06633663366343, w1=10.363999289979459\n",
      "SubGD iter. 113/499: loss=168.3291938928825, w0=68.32970297029709, w1=10.66046690927365\n",
      "SubGD iter. 114/499: loss=164.92748121456762, w0=68.59306930693076, w1=10.943174379960851\n",
      "SubGD iter. 115/499: loss=161.84261312272287, w0=68.85643564356442, w1=11.225881850648053\n",
      "SubGD iter. 116/499: loss=159.05631571338645, w0=69.11287128712878, w1=11.504395843582245\n",
      "SubGD iter. 117/499: loss=156.6332679443231, w0=69.35544554455453, w1=11.78820189306779\n",
      "SubGD iter. 118/499: loss=154.63527137702067, w0=69.58415841584166, w1=12.06091146519101\n",
      "SubGD iter. 119/499: loss=153.01726462505545, w0=69.80594059405948, w1=12.324245668386087\n",
      "SubGD iter. 120/499: loss=151.68738119863244, w0=70.0277227722773, w1=12.587579871581164\n",
      "SubGD iter. 121/499: loss=150.59456224650432, w0=70.25643564356443, w1=12.824765405096523\n",
      "SubGD iter. 122/499: loss=149.59163964002855, w0=70.47821782178225, w1=13.065616959310187\n",
      "SubGD iter. 123/499: loss=148.87043467320052, w0=70.69306930693077, w1=13.302953389983951\n",
      "SubGD iter. 124/499: loss=148.3943964418497, w0=70.89405940594067, w1=13.525403099312957\n",
      "SubGD iter. 125/499: loss=148.13675798790996, w0=71.08811881188126, w1=13.742945617944251\n",
      "SubGD iter. 126/499: loss=148.07353395486248, w0=71.27524752475254, w1=13.953548196006883\n",
      "SubGD iter. 127/499: loss=148.17837949345383, w0=71.46237623762383, w1=14.164150774069515\n",
      "SubGD iter. 128/499: loss=148.44196623418017, w0=71.62178217821788, w1=14.349779559473214\n",
      "SubGD iter. 129/499: loss=148.8329525633205, w0=71.75346534653471, w1=14.516890107612351\n",
      "SubGD iter. 130/499: loss=149.34192305667625, w0=71.87128712871292, w1=14.670791185324227\n",
      "SubGD iter. 131/499: loss=149.90589676476776, w0=71.95445544554461, w1=14.780276456654562\n",
      "SubGD iter. 132/499: loss=150.35559341773651, w0=72.0376237623763, w1=14.889761727984897\n",
      "SubGD iter. 133/499: loss=150.843098057831, w0=72.10693069306937, w1=14.985916181776767\n",
      "SubGD iter. 134/499: loss=151.31706104340293, w0=72.17623762376245, w1=15.082070635568638\n",
      "SubGD iter. 135/499: loss=151.81912228822705, w0=72.24554455445552, w1=15.178225089360508\n",
      "SubGD iter. 136/499: loss=152.34928179230337, w0=72.30099009900998, w1=15.25972348971595\n",
      "SubGD iter. 137/499: loss=152.83227502042283, w0=72.34950495049513, w1=15.335091856448178\n",
      "SubGD iter. 138/499: loss=153.30571236012923, w0=72.39801980198028, w1=15.410460223180406\n",
      "SubGD iter. 139/499: loss=153.79521786287268, w0=72.43267326732682, w1=15.469961786755766\n",
      "SubGD iter. 140/499: loss=154.20493737591542, w0=72.46039603960405, w1=15.51864528583285\n",
      "SubGD iter. 141/499: loss=154.54924434705026, w0=72.48811881188128, w1=15.561592159086528\n",
      "SubGD iter. 142/499: loss=154.84785974250684, w0=72.5019801980199, w1=15.597828332032567\n",
      "SubGD iter. 143/499: loss=155.13363200857273, w0=72.52277227722782, w1=15.624722856626754\n",
      "SubGD iter. 144/499: loss=155.3151099450636, w0=72.55049504950505, w1=15.642690329098041\n",
      "SubGD iter. 145/499: loss=155.39547131530236, w0=72.56435643564366, w1=15.664356578291132\n",
      "SubGD iter. 146/499: loss=155.5537361072187, w0=72.58514851485158, w1=15.677095775361325\n",
      "SubGD iter. 147/499: loss=155.60976227626065, w0=72.6059405940595, w1=15.689834972431518\n",
      "SubGD iter. 148/499: loss=155.6669776407022, w0=72.62673267326743, w1=15.70257416950171\n",
      "SubGD iter. 149/499: loss=155.7253822005433, w0=72.64059405940604, w1=15.724240418694801\n",
      "SubGD iter. 150/499: loss=155.8883554271598, w0=72.66138613861396, w1=15.736979615764994\n",
      "SubGD iter. 151/499: loss=155.94907761771404, w0=72.66831683168327, w1=15.74811029423132\n",
      "SubGD iter. 152/499: loss=156.03442940440513, w0=72.67524752475258, w1=15.759240972697647\n",
      "SubGD iter. 153/499: loss=156.12012504411533, w0=72.68217821782189, w1=15.770371651163973\n",
      "SubGD iter. 154/499: loss=156.2061645368446, w0=72.68217821782189, w1=15.774323911906727\n",
      "SubGD iter. 155/499: loss=156.24361183520213, w0=72.68217821782189, w1=15.77827617264948\n",
      "SubGD iter. 156/499: loss=156.28109037428962, w0=72.68217821782189, w1=15.782228433392234\n",
      "SubGD iter. 157/499: loss=156.31860015410703, w0=72.68217821782189, w1=15.786180694134988\n",
      "SubGD iter. 158/499: loss=156.35614117465443, w0=72.68217821782189, w1=15.790132954877741\n",
      "SubGD iter. 159/499: loss=156.39371343593177, w0=72.68217821782189, w1=15.794085215620495\n",
      "SubGD iter. 160/499: loss=156.43131693793904, w0=72.68217821782189, w1=15.798037476363248\n",
      "SubGD iter. 161/499: loss=156.46895168067633, w0=72.68217821782189, w1=15.801989737106002\n",
      "SubGD iter. 162/499: loss=156.50661766414353, w0=72.68217821782189, w1=15.805941997848755\n",
      "SubGD iter. 163/499: loss=156.54431488834072, w0=72.68217821782189, w1=15.809894258591509\n",
      "SubGD iter. 164/499: loss=156.58204335326786, w0=72.68217821782189, w1=15.813846519334263\n",
      "SubGD iter. 165/499: loss=156.61980305892493, w0=72.68217821782189, w1=15.817798780077016\n",
      "SubGD iter. 166/499: loss=156.65759400531198, w0=72.68217821782189, w1=15.82175104081977\n",
      "SubGD iter. 167/499: loss=156.69541619242898, w0=72.68217821782189, w1=15.825703301562523\n",
      "SubGD iter. 168/499: loss=156.73326962027593, w0=72.68217821782189, w1=15.829655562305277\n",
      "SubGD iter. 169/499: loss=156.77115428885287, w0=72.68217821782189, w1=15.83360782304803\n",
      "SubGD iter. 170/499: loss=156.80907019815973, w0=72.68217821782189, w1=15.837560083790784\n",
      "SubGD iter. 171/499: loss=156.84701734819657, w0=72.68217821782189, w1=15.841512344533538\n",
      "SubGD iter. 172/499: loss=156.88499573896334, w0=72.68217821782189, w1=15.845464605276291\n",
      "SubGD iter. 173/499: loss=156.9230053704601, w0=72.68217821782189, w1=15.849416866019045\n",
      "SubGD iter. 174/499: loss=156.96104624268682, w0=72.68217821782189, w1=15.853369126761798\n",
      "SubGD iter. 175/499: loss=156.9991183556435, w0=72.68217821782189, w1=15.857321387504552\n",
      "SubGD iter. 176/499: loss=157.03722170933008, w0=72.68217821782189, w1=15.861273648247305\n",
      "SubGD iter. 177/499: loss=157.07535630374667, w0=72.68217821782189, w1=15.865225908990059\n",
      "SubGD iter. 178/499: loss=157.11352213889322, w0=72.68217821782189, w1=15.869178169732812\n",
      "SubGD iter. 179/499: loss=157.15171921476968, w0=72.68217821782189, w1=15.873130430475566\n",
      "SubGD iter. 180/499: loss=157.18994753137616, w0=72.68217821782189, w1=15.87708269121832\n",
      "SubGD iter. 181/499: loss=157.2282070887126, w0=72.68217821782189, w1=15.881034951961073\n",
      "SubGD iter. 182/499: loss=157.26649788677892, w0=72.68217821782189, w1=15.884987212703827\n",
      "SubGD iter. 183/499: loss=157.30481992557523, w0=72.68217821782189, w1=15.88893947344658\n",
      "SubGD iter. 184/499: loss=157.3431732051015, w0=72.68217821782189, w1=15.892891734189334\n",
      "SubGD iter. 185/499: loss=157.38155772535777, w0=72.68217821782189, w1=15.896843994932087\n",
      "SubGD iter. 186/499: loss=157.41997348634393, w0=72.68217821782189, w1=15.900796255674841\n",
      "SubGD iter. 187/499: loss=157.4584204880601, w0=72.68217821782189, w1=15.904748516417595\n",
      "SubGD iter. 188/499: loss=157.4968987305062, w0=72.68217821782189, w1=15.908700777160348\n",
      "SubGD iter. 189/499: loss=157.5354082136823, w0=72.68217821782189, w1=15.912653037903102\n",
      "SubGD iter. 190/499: loss=157.57394893758828, w0=72.67524752475258, w1=15.910526938117364\n",
      "SubGD iter. 191/499: loss=157.5724670109089, w0=72.67524752475258, w1=15.914479198860118\n",
      "SubGD iter. 192/499: loss=157.61102216974342, w0=72.67524752475258, w1=15.918431459602871\n",
      "SubGD iter. 193/499: loss=157.64960856930793, w0=72.66831683168327, w1=15.916305359817134\n",
      "SubGD iter. 194/499: loss=157.64819814063907, w0=72.66831683168327, w1=15.920257620559887\n",
      "SubGD iter. 195/499: loss=157.68679897513212, w0=72.66831683168327, w1=15.924209881302641\n",
      "SubGD iter. 196/499: loss=157.72543105035507, w0=72.66831683168327, w1=15.928162142045394\n",
      "SubGD iter. 197/499: loss=157.76409436630803, w0=72.66138613861396, w1=15.926036042259657\n",
      "SubGD iter. 198/499: loss=157.76273862984831, w0=72.66138613861396, w1=15.92998830300241\n",
      "SubGD iter. 199/499: loss=157.80141638072976, w0=72.66138613861396, w1=15.933940563745164\n",
      "SubGD iter. 200/499: loss=157.8401253723412, w0=72.65445544554466, w1=15.931814463959427\n",
      "SubGD iter. 201/499: loss=157.83884113389203, w0=72.65445544554466, w1=15.93576672470218\n",
      "SubGD iter. 202/499: loss=157.87756456043195, w0=72.65445544554466, w1=15.939718985444934\n",
      "SubGD iter. 203/499: loss=157.91631922770185, w0=72.65445544554466, w1=15.943671246187687\n",
      "SubGD iter. 204/499: loss=157.9551051357017, w0=72.64752475247535, w1=15.94154514640195\n",
      "SubGD iter. 205/499: loss=157.9538755894617, w0=72.64752475247535, w1=15.945497407144703\n",
      "SubGD iter. 206/499: loss=157.99267593239009, w0=72.64752475247535, w1=15.949449667887457\n",
      "SubGD iter. 207/499: loss=158.0315075160484, w0=72.64059405940604, w1=15.94732356810172\n",
      "SubGD iter. 208/499: loss=158.03034946781892, w0=72.64059405940604, w1=15.951275828844473\n",
      "SubGD iter. 209/499: loss=158.0691954864058, w0=72.64059405940604, w1=15.955228089587226\n",
      "SubGD iter. 210/499: loss=158.1080727457226, w0=72.64059405940604, w1=15.95918035032998\n",
      "SubGD iter. 211/499: loss=158.14698124576938, w0=72.63366336633673, w1=15.957054250544243\n",
      "SubGD iter. 212/499: loss=158.14587788974904, w0=72.63366336633673, w1=15.961006511286996\n",
      "SubGD iter. 213/499: loss=158.18480082472433, w0=72.63366336633673, w1=15.96495877202975\n",
      "SubGD iter. 214/499: loss=158.22375500042958, w0=72.62673267326743, w1=15.962832672244012\n",
      "SubGD iter. 215/499: loss=158.2227231424198, w0=72.63366336633673, w1=15.967301372051416\n",
      "SubGD iter. 216/499: loss=158.24685882383065, w0=72.62673267326743, w1=15.965175272265679\n",
      "SubGD iter. 217/499: loss=158.2458170046181, w0=72.63366336633673, w1=15.969643972073083\n",
      "SubGD iter. 218/499: loss=158.26997362278146, w0=72.62673267326743, w1=15.967517872287345\n",
      "SubGD iter. 219/499: loss=158.2689218423661, w0=72.63366336633673, w1=15.97198657209475\n",
      "SubGD iter. 220/499: loss=158.29309939728196, w0=72.62673267326743, w1=15.969860472309012\n",
      "SubGD iter. 221/499: loss=158.2920376556638, w0=72.63366336633673, w1=15.974329172116416\n",
      "SubGD iter. 222/499: loss=158.31623614733226, w0=72.62673267326743, w1=15.972203072330679\n",
      "SubGD iter. 223/499: loss=158.31516444451123, w0=72.62673267326743, w1=15.970593411609592\n",
      "SubGD iter. 224/499: loss=158.29927225334356, w0=72.63366336633673, w1=15.975062111416996\n",
      "SubGD iter. 225/499: loss=158.32347729558342, w0=72.62673267326743, w1=15.972936011631258\n",
      "SubGD iter. 226/499: loss=158.32240247615826, w0=72.62673267326743, w1=15.971326350910171\n",
      "SubGD iter. 227/499: loss=158.3065079254234, w0=72.63366336633673, w1=15.975795050717576\n",
      "SubGD iter. 228/499: loss=158.33071951823467, w0=72.62673267326743, w1=15.973668950931838\n",
      "SubGD iter. 229/499: loss=158.3296415822053, w0=72.62673267326743, w1=15.972059290210751\n",
      "SubGD iter. 230/499: loss=158.31374467190324, w0=72.62673267326743, w1=15.970449629489664\n",
      "SubGD iter. 231/499: loss=158.29785294361645, w0=72.63366336633673, w1=15.974918329297068\n",
      "SubGD iter. 232/499: loss=158.32205670081805, w0=72.62673267326743, w1=15.972792229511331\n",
      "SubGD iter. 233/499: loss=158.32098249278314, w0=72.62673267326743, w1=15.971182568790244\n",
      "SubGD iter. 234/499: loss=158.30508840492914, w0=72.63366336633673, w1=15.975651268597648\n",
      "SubGD iter. 235/499: loss=158.32929871270215, w0=72.62673267326743, w1=15.97352516881191\n",
      "SubGD iter. 236/499: loss=158.32822138806307, w0=72.62673267326743, w1=15.971915508090824\n",
      "SubGD iter. 237/499: loss=158.31232494064184, w0=72.63366336633673, w1=15.976384207898228\n",
      "SubGD iter. 238/499: loss=158.3365417989863, w0=72.62673267326743, w1=15.97425810811249\n",
      "SubGD iter. 239/499: loss=158.33546135774301, w0=72.62673267326743, w1=15.972648447391403\n",
      "SubGD iter. 240/499: loss=158.31956255075463, w0=72.62673267326743, w1=15.971038786670317\n",
      "SubGD iter. 241/499: loss=158.30366892578147, w0=72.63366336633673, w1=15.97550748647772\n",
      "SubGD iter. 242/499: loss=158.3278779485162, w0=72.62673267326743, w1=15.973381386691983\n",
      "SubGD iter. 243/499: loss=158.32680123526742, w0=72.62673267326743, w1=15.971771725970896\n",
      "SubGD iter. 244/499: loss=158.31090525072707, w0=72.63366336633673, w1=15.9762404257783\n",
      "SubGD iter. 245/499: loss=158.33512082403323, w0=72.62673267326743, w1=15.974114325992563\n",
      "SubGD iter. 246/499: loss=158.33404099418024, w0=72.62673267326743, w1=15.972504665271476\n",
      "SubGD iter. 247/499: loss=158.31814265007267, w0=72.62673267326743, w1=15.970895004550389\n",
      "SubGD iter. 248/499: loss=158.3022494879804, w0=72.63366336633673, w1=15.975363704357793\n",
      "SubGD iter. 249/499: loss=158.3264572256769, w0=72.62673267326743, w1=15.973237604572056\n",
      "SubGD iter. 250/499: loss=158.32538112381832, w0=72.62673267326743, w1=15.971627943850969\n",
      "SubGD iter. 251/499: loss=158.30948560215884, w0=72.63366336633673, w1=15.976096643658373\n",
      "SubGD iter. 252/499: loss=158.3336998904268, w0=72.62673267326743, w1=15.973970543872635\n",
      "SubGD iter. 253/499: loss=158.33262067196404, w0=72.62673267326743, w1=15.972360883151548\n",
      "SubGD iter. 254/499: loss=158.31672279073734, w0=72.62673267326743, w1=15.970751222430462\n",
      "SubGD iter. 255/499: loss=158.30083009152594, w0=72.63366336633673, w1=15.975219922237866\n",
      "SubGD iter. 256/499: loss=158.32503654418417, w0=72.62673267326743, w1=15.973093822452128\n",
      "SubGD iter. 257/499: loss=158.32396105371586, w0=72.62673267326743, w1=15.971484161731041\n",
      "SubGD iter. 258/499: loss=158.30806599493727, w0=72.63366336633673, w1=15.975952861538445\n",
      "SubGD iter. 259/499: loss=158.33227899816694, w0=72.62673267326743, w1=15.973826761752708\n",
      "SubGD iter. 260/499: loss=158.33120039109446, w0=72.62673267326743, w1=15.972217101031621\n",
      "SubGD iter. 261/499: loss=158.3153029727486, w0=72.62673267326743, w1=15.970607440310534\n",
      "SubGD iter. 262/499: loss=158.29941073641803, w0=72.63366336633673, w1=15.975076140117938\n",
      "SubGD iter. 263/499: loss=158.32361590403804, w0=72.62673267326743, w1=15.9729500403322\n",
      "SubGD iter. 264/499: loss=158.32254102496003, w0=72.62673267326743, w1=15.971340379611114\n",
      "SubGD iter. 265/499: loss=158.30664642906225, w0=72.63366336633673, w1=15.975809079418518\n",
      "SubGD iter. 266/499: loss=158.33085814725362, w0=72.62673267326743, w1=15.97368297963278\n",
      "SubGD iter. 267/499: loss=158.32978015157144, w0=72.62673267326743, w1=15.972073318911693\n",
      "SubGD iter. 268/499: loss=158.31388319610647, w0=72.62673267326743, w1=15.970463658190607\n",
      "SubGD iter. 269/499: loss=158.2979914226568, w0=72.63366336633673, w1=15.97493235799801\n",
      "SubGD iter. 270/499: loss=158.3221953052385, w0=72.62673267326743, w1=15.972806258212273\n",
      "SubGD iter. 271/499: loss=158.32112103755074, w0=72.62673267326743, w1=15.971196597491186\n",
      "SubGD iter. 272/499: loss=158.30522690453384, w0=72.63366336633673, w1=15.97566529729859\n",
      "SubGD iter. 273/499: loss=158.32943733768695, w0=72.62673267326743, w1=15.973539197512853\n",
      "SubGD iter. 274/499: loss=158.32835995339502, w0=72.62673267326743, w1=15.971929536791766\n",
      "SubGD iter. 275/499: loss=158.31246346081093, w0=72.63366336633673, w1=15.97639823659917\n",
      "SubGD iter. 276/499: loss=158.3366804445355, w0=72.62673267326743, w1=15.974272136813433\n",
      "SubGD iter. 277/499: loss=158.33559994363938, w0=72.62673267326743, w1=15.972662476092346\n",
      "SubGD iter. 278/499: loss=158.31970109148807, w0=72.62673267326743, w1=15.971052815371259\n",
      "SubGD iter. 279/499: loss=158.30380742135202, w0=72.63366336633673, w1=15.975521515178663\n",
      "SubGD iter. 280/499: loss=158.32801656946688, w0=72.62673267326743, w1=15.973395415392925\n",
      "SubGD iter. 281/499: loss=158.32693979656523, w0=72.62673267326743, w1=15.971785754671838\n",
      "SubGD iter. 282/499: loss=158.31104376686199, w0=72.63366336633673, w1=15.976254454479243\n",
      "SubGD iter. 283/499: loss=158.3352594655483, w0=72.62673267326743, w1=15.974128354693505\n",
      "SubGD iter. 284/499: loss=158.33417957604243, w0=72.62673267326743, w1=15.972518693972418\n",
      "SubGD iter. 285/499: loss=158.31828118677197, w0=72.62673267326743, w1=15.970909033251331\n",
      "SubGD iter. 286/499: loss=158.3023879795168, w0=72.63366336633673, w1=15.975377733058735\n",
      "SubGD iter. 287/499: loss=158.32659584259338, w0=72.62673267326743, w1=15.973251633272998\n",
      "SubGD iter. 288/499: loss=158.325519681082, w0=72.62673267326743, w1=15.971641972551911\n",
      "SubGD iter. 289/499: loss=158.30962411425963, w0=72.63366336633673, w1=15.976110672359315\n",
      "SubGD iter. 290/499: loss=158.33383852790766, w0=72.62673267326743, w1=15.973984572573578\n",
      "SubGD iter. 291/499: loss=158.3327592497921, w0=72.62673267326743, w1=15.97237491185249\n",
      "SubGD iter. 292/499: loss=158.3168613234025, w0=72.62673267326743, w1=15.970765251131404\n",
      "SubGD iter. 293/499: loss=158.3009685790282, w0=72.63366336633673, w1=15.975233950938808\n",
      "SubGD iter. 294/499: loss=158.3251751570665, w0=72.62673267326743, w1=15.97310785115307\n",
      "SubGD iter. 295/499: loss=158.3240996069454, w0=72.62673267326743, w1=15.971498190431983\n",
      "SubGD iter. 296/499: loss=158.30820450300388, w0=72.63366336633673, w1=15.975966890239388\n",
      "SubGD iter. 297/499: loss=158.33241763161362, w0=72.62673267326743, w1=15.97384079045365\n",
      "SubGD iter. 298/499: loss=158.33133896488832, w0=72.62673267326743, w1=15.972231129732563\n",
      "SubGD iter. 299/499: loss=158.3154415013796, w0=72.62673267326743, w1=15.970621469011476\n",
      "SubGD iter. 300/499: loss=158.2995492198862, w0=72.63366336633673, w1=15.97509016881888\n",
      "SubGD iter. 301/499: loss=158.32375451288624, w0=72.62673267326743, w1=15.972964069033143\n",
      "SubGD iter. 302/499: loss=158.32267957415536, w0=72.62673267326743, w1=15.971354408312056\n",
      "SubGD iter. 303/499: loss=158.30678493309472, w0=72.63366336633673, w1=15.97582310811946\n",
      "SubGD iter. 304/499: loss=158.3309967766662, w0=72.62673267326743, w1=15.973697008333723\n",
      "SubGD iter. 305/499: loss=158.3299187213312, w0=72.62673267326743, w1=15.972087347612636\n",
      "SubGD iter. 306/499: loss=158.3140217207033, w0=72.62673267326743, w1=15.970477686891549\n",
      "SubGD iter. 307/499: loss=158.29812990209072, w0=72.63366336633673, w1=15.974946386698953\n",
      "SubGD iter. 308/499: loss=158.32233391005255, w0=72.62673267326743, w1=15.972820286913215\n",
      "SubGD iter. 309/499: loss=158.32125958271195, w0=72.62673267326743, w1=15.971210626192129\n",
      "SubGD iter. 310/499: loss=158.30536540453213, w0=72.63366336633673, w1=15.975679325999533\n",
      "SubGD iter. 311/499: loss=158.32957596306537, w0=72.62673267326743, w1=15.973553226213795\n",
      "SubGD iter. 312/499: loss=158.32849851912061, w0=72.62673267326743, w1=15.971943565492708\n",
      "SubGD iter. 313/499: loss=158.31260198137363, w0=72.63366336633673, w1=15.976412265300112\n",
      "SubGD iter. 314/499: loss=158.33681909047826, w0=72.62673267326743, w1=15.974286165514375\n",
      "SubGD iter. 315/499: loss=158.3357385299293, w0=72.62673267326743, w1=15.972676504793288\n",
      "SubGD iter. 316/499: loss=158.3198396326151, w0=72.62673267326743, w1=15.971066844072201\n",
      "SubGD iter. 317/499: loss=158.30394591731618, w0=72.63366336633673, w1=15.975535543879605\n",
      "SubGD iter. 318/499: loss=158.32815519081112, w0=72.62673267326743, w1=15.973409444093868\n",
      "SubGD iter. 319/499: loss=158.32707835825664, w0=72.62673267326743, w1=15.97179978337278\n",
      "SubGD iter. 320/499: loss=158.3111822833905, w0=72.63366336633673, w1=15.976268483180185\n",
      "SubGD iter. 321/499: loss=158.3353981074569, w0=72.62673267326743, w1=15.974142383394447\n",
      "SubGD iter. 322/499: loss=158.33431815829826, w0=72.62673267326743, w1=15.97253272267336\n",
      "SubGD iter. 323/499: loss=158.31841972386488, w0=72.62673267326743, w1=15.970923061952274\n",
      "SubGD iter. 324/499: loss=158.3025264714468, w0=72.63366336633673, w1=15.975391761759678\n",
      "SubGD iter. 325/499: loss=158.32673445990346, w0=72.62673267326743, w1=15.97326566197394\n",
      "SubGD iter. 326/499: loss=158.3256582387393, w0=72.62673267326743, w1=15.971656001252853\n",
      "SubGD iter. 327/499: loss=158.309762626754, w0=72.63366336633673, w1=15.976124701060257\n",
      "SubGD iter. 328/499: loss=158.3339771657821, w0=72.62673267326743, w1=15.97399860127452\n",
      "SubGD iter. 329/499: loss=158.33289782801373, w0=72.62673267326743, w1=15.972388940553433\n",
      "SubGD iter. 330/499: loss=158.31699985646125, w0=72.62673267326743, w1=15.970779279832346\n",
      "SubGD iter. 331/499: loss=158.30110706692403, w0=72.63366336633673, w1=15.97524797963975\n",
      "SubGD iter. 332/499: loss=158.3253137703425, w0=72.62673267326743, w1=15.973121879854013\n",
      "SubGD iter. 333/499: loss=158.32423816056854, w0=72.62673267326743, w1=15.971512219132926\n",
      "SubGD iter. 334/499: loss=158.3083430114641, w0=72.63366336633673, w1=15.97598091894033\n",
      "SubGD iter. 335/499: loss=158.33255626545392, w0=72.62673267326743, w1=15.973854819154592\n",
      "SubGD iter. 336/499: loss=158.33147753907582, w0=72.62673267326743, w1=15.972245158433505\n",
      "SubGD iter. 337/499: loss=158.3155800304042, w0=72.62673267326743, w1=15.970635497712419\n",
      "SubGD iter. 338/499: loss=158.29968770374785, w0=72.63366336633673, w1=15.975104197519823\n",
      "SubGD iter. 339/499: loss=158.323893122128, w0=72.62673267326743, w1=15.972978097734085\n",
      "SubGD iter. 340/499: loss=158.32281812374436, w0=72.62673267326743, w1=15.971368437012998\n",
      "SubGD iter. 341/499: loss=158.3069234375208, w0=72.63366336633673, w1=15.975837136820402\n",
      "SubGD iter. 342/499: loss=158.33113540647236, w0=72.62673267326743, w1=15.973711037034665\n",
      "SubGD iter. 343/499: loss=158.3300572914845, w0=72.62673267326743, w1=15.972101376313578\n",
      "SubGD iter. 344/499: loss=158.31416024569378, w0=72.62673267326743, w1=15.970491715592491\n",
      "SubGD iter. 345/499: loss=158.29826838191826, w0=72.63366336633673, w1=15.974960415399895\n",
      "SubGD iter. 346/499: loss=158.32247251526016, w0=72.62673267326743, w1=15.972834315614158\n",
      "SubGD iter. 347/499: loss=158.32139812826674, w0=72.62673267326743, w1=15.97122465489307\n",
      "SubGD iter. 348/499: loss=158.30550390492408, w0=72.63366336633673, w1=15.975693354700475\n",
      "SubGD iter. 349/499: loss=158.3297145888374, w0=72.62673267326743, w1=15.973567254914737\n",
      "SubGD iter. 350/499: loss=158.3286370852398, w0=72.62673267326743, w1=15.97195759419365\n",
      "SubGD iter. 351/499: loss=158.3127405023299, w0=72.63366336633673, w1=15.976426294001055\n",
      "SubGD iter. 352/499: loss=158.33695773681464, w0=72.62673267326743, w1=15.974300194215317\n",
      "SubGD iter. 353/499: loss=158.33587711661286, w0=72.62673267326743, w1=15.97269053349423\n",
      "SubGD iter. 354/499: loss=158.3199781741358, w0=72.62673267326743, w1=15.971080872773143\n",
      "SubGD iter. 355/499: loss=158.30408441367396, w0=72.63366336633673, w1=15.975549572580547\n",
      "SubGD iter. 356/499: loss=158.32829381254902, w0=72.62673267326743, w1=15.97342347279481\n",
      "SubGD iter. 357/499: loss=158.3272169203417, w0=72.62673267326743, w1=15.971813812073723\n",
      "SubGD iter. 358/499: loss=158.31132080031261, w0=72.63366336633673, w1=15.976282511881127\n",
      "SubGD iter. 359/499: loss=158.33553674975914, w0=72.62673267326743, w1=15.97415641209539\n",
      "SubGD iter. 360/499: loss=158.33445674094764, w0=72.62673267326743, w1=15.972546751374303\n",
      "SubGD iter. 361/499: loss=158.3185582613514, w0=72.62673267326743, w1=15.970937090653216\n",
      "SubGD iter. 362/499: loss=158.3026649637704, w0=72.63366336633673, w1=15.97540579046062\n",
      "SubGD iter. 363/499: loss=158.32687307760725, w0=72.62673267326743, w1=15.973279690674882\n",
      "SubGD iter. 364/499: loss=158.32579679679017, w0=72.62673267326743, w1=15.971670029953795\n",
      "SubGD iter. 365/499: loss=158.309901139642, w0=72.63366336633673, w1=15.9761387297612\n",
      "SubGD iter. 366/499: loss=158.3341158040502, w0=72.62673267326743, w1=15.974012629975462\n",
      "SubGD iter. 367/499: loss=158.33303640662896, w0=72.62673267326743, w1=15.972402969254375\n",
      "SubGD iter. 368/499: loss=158.3171383899136, w0=72.62673267326743, w1=15.970793308533288\n",
      "SubGD iter. 369/499: loss=158.30124555521348, w0=72.63366336633673, w1=15.975262008340692\n",
      "SubGD iter. 370/499: loss=158.32545238401204, w0=72.62673267326743, w1=15.973135908554955\n",
      "SubGD iter. 371/499: loss=158.32437671458524, w0=72.62673267326743, w1=15.971526247833868\n",
      "SubGD iter. 372/499: loss=158.3084815203179, w0=72.63366336633673, w1=15.975994947641272\n",
      "SubGD iter. 373/499: loss=158.33269489968788, w0=72.62673267326743, w1=15.973868847855535\n",
      "SubGD iter. 374/499: loss=158.33161611365693, w0=72.62673267326743, w1=15.972259187134448\n",
      "SubGD iter. 375/499: loss=158.3157185598224, w0=72.62673267326743, w1=15.97064952641336\n",
      "SubGD iter. 376/499: loss=158.29982618800315, w0=72.63366336633673, w1=15.975118226220765\n",
      "SubGD iter. 377/499: loss=158.32403173176345, w0=72.62673267326743, w1=15.972992126435027\n",
      "SubGD iter. 378/499: loss=158.32295667372694, w0=72.62673267326743, w1=15.97138246571394\n",
      "SubGD iter. 379/499: loss=158.30706194234045, w0=72.63366336633673, w1=15.975851165521345\n",
      "SubGD iter. 380/499: loss=158.33127403667217, w0=72.62673267326743, w1=15.973725065735607\n",
      "SubGD iter. 381/499: loss=158.33019586203147, w0=72.62673267326743, w1=15.97211540501452\n",
      "SubGD iter. 382/499: loss=158.3142987710778, w0=72.62673267326743, w1=15.970505744293433\n",
      "SubGD iter. 383/499: loss=158.2984068621394, w0=72.63366336633673, w1=15.974974444100837\n",
      "SubGD iter. 384/499: loss=158.32261112086144, w0=72.62673267326743, w1=15.9728483443151\n",
      "SubGD iter. 385/499: loss=158.3215366742152, w0=72.62673267326743, w1=15.971238683594013\n",
      "SubGD iter. 386/499: loss=158.3056424057096, w0=72.63366336633673, w1=15.975707383401417\n",
      "SubGD iter. 387/499: loss=158.32985321500303, w0=72.62673267326743, w1=15.97358128361568\n",
      "SubGD iter. 388/499: loss=158.3287756517526, w0=72.62673267326743, w1=15.971971622894593\n",
      "SubGD iter. 389/499: loss=158.3128790236798, w0=72.63366336633673, w1=15.976440322701997\n",
      "SubGD iter. 390/499: loss=158.33709638354466, w0=72.62673267326743, w1=15.97431422291626\n",
      "SubGD iter. 391/499: loss=158.33601570369007, w0=72.62673267326743, w1=15.972704562195172\n",
      "SubGD iter. 392/499: loss=158.32011671605005, w0=72.62673267326743, w1=15.971094901474086\n",
      "SubGD iter. 393/499: loss=158.3042229104253, w0=72.63366336633673, w1=15.97556360128149\n",
      "SubGD iter. 394/499: loss=158.32843243468048, w0=72.62673267326743, w1=15.973437501495752\n",
      "SubGD iter. 395/499: loss=158.3273554828203, w0=72.62673267326743, w1=15.971827840774665\n",
      "SubGD iter. 396/499: loss=158.3114593176284, w0=72.63366336633673, w1=15.97629654058207\n",
      "SubGD iter. 397/499: loss=158.335675392455, w0=72.62673267326743, w1=15.974170440796332\n",
      "SubGD iter. 398/499: loss=158.33459532399067, w0=72.62673267326743, w1=15.972560780075245\n",
      "SubGD iter. 399/499: loss=158.31869679923153, w0=72.62673267326743, w1=15.970951119354158\n",
      "SubGD iter. 400/499: loss=158.30280345648765, w0=72.63366336633673, w1=15.975419819161562\n",
      "SubGD iter. 401/499: loss=158.32701169570456, w0=72.62673267326743, w1=15.973293719375825\n",
      "SubGD iter. 402/499: loss=158.32593535523466, w0=72.62673267326743, w1=15.971684058654738\n",
      "SubGD iter. 403/499: loss=158.31003965292356, w0=72.63366336633673, w1=15.976152758462142\n",
      "SubGD iter. 404/499: loss=158.3342544427119, w0=72.62673267326743, w1=15.974026658676404\n",
      "SubGD iter. 405/499: loss=158.33317498563784, w0=72.62673267326743, w1=15.972416997955317\n",
      "SubGD iter. 406/499: loss=158.31727692375958, w0=72.62673267326743, w1=15.97080733723423\n",
      "SubGD iter. 407/499: loss=158.30138404389655, w0=72.63366336633673, w1=15.975276037041635\n",
      "SubGD iter. 408/499: loss=158.32559099807523, w0=72.62673267326743, w1=15.973149937255897\n",
      "SubGD iter. 409/499: loss=158.3245152689956, w0=72.62673267326743, w1=15.97154027653481\n",
      "SubGD iter. 410/499: loss=158.3086200295654, w0=72.63366336633673, w1=15.976008976342214\n",
      "SubGD iter. 411/499: loss=158.33283353431545, w0=72.62673267326743, w1=15.973882876556477\n",
      "SubGD iter. 412/499: loss=158.33175468863163, w0=72.62673267326743, w1=15.97227321583539\n",
      "SubGD iter. 413/499: loss=158.31585708963422, w0=72.62673267326743, w1=15.970663555114303\n",
      "SubGD iter. 414/499: loss=158.29996467265207, w0=72.63366336633673, w1=15.975132254921707\n",
      "SubGD iter. 415/499: loss=158.32417034179244, w0=72.62673267326743, w1=15.97300615513597\n",
      "SubGD iter. 416/499: loss=158.32309522410313, w0=72.62673267326743, w1=15.971396494414883\n",
      "SubGD iter. 417/499: loss=158.30720044755373, w0=72.63366336633673, w1=15.975865194222287\n",
      "SubGD iter. 418/499: loss=158.33141266726554, w0=72.62673267326743, w1=15.97373909443655\n",
      "SubGD iter. 419/499: loss=158.33033443297205, w0=72.62673267326743, w1=15.972129433715462\n",
      "SubGD iter. 420/499: loss=158.3144372968555, w0=72.62673267326743, w1=15.970519772994376\n",
      "SubGD iter. 421/499: loss=158.29854534275418, w0=72.63366336633673, w1=15.97498847280178\n",
      "SubGD iter. 422/499: loss=158.3227497268563, w0=72.62673267326743, w1=15.972862373016042\n",
      "SubGD iter. 423/499: loss=158.32167522055724, w0=72.62673267326743, w1=15.971252712294955\n",
      "SubGD iter. 424/499: loss=158.30578090688874, w0=72.63366336633673, w1=15.97572141210236\n",
      "SubGD iter. 425/499: loss=158.32999184156228, w0=72.62673267326743, w1=15.973595312316622\n",
      "SubGD iter. 426/499: loss=158.328914218659, w0=72.62673267326743, w1=15.971985651595535\n",
      "SubGD iter. 427/499: loss=158.31301754542332, w0=72.63366336633673, w1=15.97645435140294\n",
      "SubGD iter. 428/499: loss=158.33723503066827, w0=72.62673267326743, w1=15.974328251617202\n",
      "SubGD iter. 429/499: loss=158.33615429116082, w0=72.62673267326743, w1=15.972718590896115\n",
      "SubGD iter. 430/499: loss=158.32025525835792, w0=72.62673267326743, w1=15.971108930175028\n",
      "SubGD iter. 431/499: loss=158.30436140757027, w0=72.63366336633673, w1=15.975577629982432\n",
      "SubGD iter. 432/499: loss=158.32857105720558, w0=72.62673267326743, w1=15.973451530196694\n",
      "SubGD iter. 433/499: loss=158.3274940456926, w0=72.62673267326743, w1=15.971841869475607\n",
      "SubGD iter. 434/499: loss=158.31159783533775, w0=72.63366336633673, w1=15.976310569283012\n",
      "SubGD iter. 435/499: loss=158.33581403554447, w0=72.62673267326743, w1=15.974184469497274\n",
      "SubGD iter. 436/499: loss=158.3347339074273, w0=72.62673267326743, w1=15.972574808776187\n",
      "SubGD iter. 437/499: loss=158.31883533750522, w0=72.62673267326743, w1=15.9709651480551\n",
      "SubGD iter. 438/499: loss=158.30294194959845, w0=72.63366336633673, w1=15.975433847862504\n",
      "SubGD iter. 439/499: loss=158.32715031419548, w0=72.62673267326743, w1=15.973307748076767\n",
      "SubGD iter. 440/499: loss=158.32607391407277, w0=72.62673267326743, w1=15.97169808735568\n",
      "SubGD iter. 441/499: loss=158.3101781665988, w0=72.63366336633673, w1=15.976166787163084\n",
      "SubGD iter. 442/499: loss=158.33439308176725, w0=72.62673267326743, w1=15.974040687377347\n",
      "SubGD iter. 443/499: loss=158.33331356504033, w0=72.62673267326743, w1=15.97243102665626\n",
      "SubGD iter. 444/499: loss=158.31741545799915, w0=72.62673267326743, w1=15.970821365935173\n",
      "SubGD iter. 445/499: loss=158.30152253297322, w0=72.63366336633673, w1=15.975290065742577\n",
      "SubGD iter. 446/499: loss=158.32572961253197, w0=72.62673267326743, w1=15.97316396595684\n",
      "SubGD iter. 447/499: loss=158.32465382379954, w0=72.62673267326743, w1=15.971554305235752\n",
      "SubGD iter. 448/499: loss=158.30875853920642, w0=72.63366336633673, w1=15.976023005043157\n",
      "SubGD iter. 449/499: loss=158.33297216933659, w0=72.62673267326743, w1=15.97389690525742\n",
      "SubGD iter. 450/499: loss=158.33189326399994, w0=72.62673267326743, w1=15.972287244536332\n",
      "SubGD iter. 451/499: loss=158.31599561983967, w0=72.62673267326743, w1=15.970677583815245\n",
      "SubGD iter. 452/499: loss=158.3001031576946, w0=72.63366336633673, w1=15.97514628362265\n",
      "SubGD iter. 453/499: loss=158.32430895221506, w0=72.62673267326743, w1=15.973020183836912\n",
      "SubGD iter. 454/499: loss=158.3232337748729, w0=72.62673267326743, w1=15.971410523115825\n",
      "SubGD iter. 455/499: loss=158.30733895316064, w0=72.63366336633673, w1=15.97587922292323\n",
      "SubGD iter. 456/499: loss=158.33155129825258, w0=72.62673267326743, w1=15.973753123137492\n",
      "SubGD iter. 457/499: loss=158.33047300430619, w0=72.62673267326743, w1=15.972143462416405\n",
      "SubGD iter. 458/499: loss=158.31457582302673, w0=72.62673267326743, w1=15.970533801695318\n",
      "SubGD iter. 459/499: loss=158.29868382376256, w0=72.63366336633673, w1=15.975002501502722\n",
      "SubGD iter. 460/499: loss=158.32288833324478, w0=72.62673267326743, w1=15.972876401716984\n",
      "SubGD iter. 461/499: loss=158.32181376729287, w0=72.62673267326743, w1=15.971266740995897\n",
      "SubGD iter. 462/499: loss=158.30591940846148, w0=72.63366336633673, w1=15.975735440803302\n",
      "SubGD iter. 463/499: loss=158.33013046851514, w0=72.62673267326743, w1=15.973609341017564\n",
      "SubGD iter. 464/499: loss=158.32905278595902, w0=72.62673267326743, w1=15.971999680296477\n",
      "SubGD iter. 465/499: loss=158.3131560675604, w0=72.63366336633673, w1=15.976468380103881\n",
      "SubGD iter. 466/499: loss=158.3373736781855, w0=72.62673267326743, w1=15.974342280318144\n",
      "SubGD iter. 467/499: loss=158.33629287902522, w0=72.62673267326743, w1=15.972732619597057\n",
      "SubGD iter. 468/499: loss=158.32039380105942, w0=72.62673267326743, w1=15.97112295887597\n",
      "SubGD iter. 469/499: loss=158.30449990510888, w0=72.63366336633673, w1=15.975591658683374\n",
      "SubGD iter. 470/499: loss=158.32870968012426, w0=72.62673267326743, w1=15.973465558897637\n",
      "SubGD iter. 471/499: loss=158.32763260895845, w0=72.62673267326743, w1=15.97185589817655\n",
      "SubGD iter. 472/499: loss=158.3117363534407, w0=72.63366336633673, w1=15.976324597983954\n",
      "SubGD iter. 473/499: loss=158.33595267902749, w0=72.62673267326743, w1=15.974198498198216\n",
      "SubGD iter. 474/499: loss=158.33487249125747, w0=72.62673267326743, w1=15.97258883747713\n",
      "SubGD iter. 475/499: loss=158.31897387617258, w0=72.62673267326743, w1=15.970979176756043\n",
      "SubGD iter. 476/499: loss=158.3030804431029, w0=72.63366336633673, w1=15.975447876563447\n",
      "SubGD iter. 477/499: loss=158.32728893308, w0=72.62673267326743, w1=15.97332177677771\n",
      "SubGD iter. 478/499: loss=158.32621247330445, w0=72.62673267326743, w1=15.971712116056622\n",
      "SubGD iter. 479/499: loss=158.3103166806676, w0=72.63366336633673, w1=15.976180815864026\n",
      "SubGD iter. 480/499: loss=158.33453172121614, w0=72.62673267326743, w1=15.974054716078289\n",
      "SubGD iter. 481/499: loss=158.3334521448364, w0=72.62673267326743, w1=15.972445055357202\n",
      "SubGD iter. 482/499: loss=158.3175539926323, w0=72.62673267326743, w1=15.970835394636115\n",
      "SubGD iter. 483/499: loss=158.3016610224435, w0=72.63366336633673, w1=15.97530409444352\n",
      "SubGD iter. 484/499: loss=158.32586822738236, w0=72.62673267326743, w1=15.973177994657782\n",
      "SubGD iter. 485/499: loss=158.32479237899707, w0=72.62673267326743, w1=15.971568333936695\n",
      "SubGD iter. 486/499: loss=158.30889704924107, w0=72.63366336633673, w1=15.976037033744099\n",
      "SubGD iter. 487/499: loss=158.33311080475136, w0=72.62673267326743, w1=15.973910933958361\n",
      "SubGD iter. 488/499: loss=158.3320318397619, w0=72.62673267326743, w1=15.972301273237274\n",
      "SubGD iter. 489/499: loss=158.31613415043867, w0=72.62673267326743, w1=15.970691612516188\n",
      "SubGD iter. 490/499: loss=158.3002416431307, w0=72.63366336633673, w1=15.975160312323592\n",
      "SubGD iter. 491/499: loss=158.32444756303136, w0=72.62673267326743, w1=15.973034212537854\n",
      "SubGD iter. 492/499: loss=158.32337232603632, w0=72.62673267326743, w1=15.971424551816767\n",
      "SubGD iter. 493/499: loss=158.30747745916113, w0=72.63366336633673, w1=15.975893251624171\n",
      "SubGD iter. 494/499: loss=158.33168992963317, w0=72.62673267326743, w1=15.973767151838434\n",
      "SubGD iter. 495/499: loss=158.330611576034, w0=72.62673267326743, w1=15.972157491117347\n",
      "SubGD iter. 496/499: loss=158.31471434959158, w0=72.62673267326743, w1=15.97054783039626\n",
      "SubGD iter. 497/499: loss=158.29882230516452, w0=72.63366336633673, w1=15.975016530203664\n",
      "SubGD iter. 498/499: loss=158.32302694002686, w0=72.62673267326743, w1=15.972890430417927\n",
      "SubGD iter. 499/499: loss=158.3219523144221, w0=72.62673267326743, w1=15.97128076969684\n",
      "SubGD: execution time=0.187 seconds\n"
     ]
    }
   ],
   "source": [
    "# Define the parameters of the algorithm.\n",
    "max_iters = 500\n",
    "gamma = 0.7\n",
    "batch_size = 1\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.array([0, 0])\n",
    "\n",
    "# Start SubSGD.\n",
    "start_time = datetime.datetime.now()\n",
    "subgd_losses, subgd_ws = subgradient_descent(\n",
    "    y, tx, w_initial, max_iters, gamma)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"SubGD: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a161c1250f904cffb1a38137326c5d88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='n_iter', max=501, min=1), Output()), _dom_classes=('widg…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_figure(n_iter)>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ipywidgets import IntSlider, interact\n",
    "def plot_figure(n_iter):\n",
    "    fig = gradient_descent_visualization(\n",
    "        subgd_losses, subgd_ws, grid_losses, grid_w0, grid_w1, mean_x, std_x, height, weight, n_iter)\n",
    "    fig.set_size_inches(10.0, 6.0)\n",
    "\n",
    "interact(plot_figure, n_iter=IntSlider(min=1, max=len(subgd_ws)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic Subgradient Descent\n",
    "\n",
    "**NB** for the computation of the subgradient you can reuse the `compute_subgradient` method that you implemented above, just making sure that you pass in a minibatch as opposed to the full data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_stoch_subgradient_mae(y, tx, w, n = random.randint(1, len(y))):\n",
    "    \"\"\"Compute a stochastic gradient at w from just few examples n and their corresponding y_n labels.\n",
    "        \n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        w: numpy array of shape=(2, ). The vector of model parameters.\n",
    "        \n",
    "    Returns:\n",
    "        A numpy array of shape (2, ) (same shape as w), containing the stochastic gradient of the loss at w.\n",
    "    \"\"\"\n",
    "    for i,j in batch_iter(y, tx, n):\n",
    "        y = i\n",
    "        tx = j\n",
    "     \n",
    "    return compute_subgradient_mae(y, tx, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stochastic_subgradient_descent(y, tx, initial_w, batch_size, max_iters, gamma):\n",
    "    \"\"\"The Stochastic SubGradient Descent algorithm (SubSGD).\n",
    "            \n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        initial_w: numpy array of shape=(2, ). The initial guess (or the initialization) for the model parameters\n",
    "        batch_size: a scalar denoting the number of data points in a mini-batch used for computing the stochastic subgradient\n",
    "        max_iters: a scalar denoting the total number of iterations of SubSGD\n",
    "        gamma: a scalar denoting the stepsize\n",
    "        \n",
    "    Returns:\n",
    "        losses: a list of length max_iters containing the loss value (scalar) for each iteration of SubSGD\n",
    "        ws: a list of length max_iters containing the model parameters as numpy arrays of shape (2, ), for each iteration of SubSGD \n",
    "    \"\"\"\n",
    "    \n",
    "    # Define parameters to store w and loss\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    \n",
    "    for n_iter in range(max_iters):\n",
    "        loss = compute_loss_mae(y, tx, w)\n",
    "        \n",
    "        # store w and loss\n",
    "        ws.append(w)\n",
    "        losses.append(loss)\n",
    "        \n",
    "        #update w by gradient\n",
    "        w = w - gamma*compute_stoch_subgradient_mae(y, tx, w)\n",
    "        \n",
    "        print(\"SubSGD iter. {bi}/{ti}: loss={l}, w0={w0}, w1={w1}\".format(\n",
    "              bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]))\n",
    "    return losses, ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SubSGD iter. 0/499: loss=74.06780585492638, w0=0.7, w1=0.13802489287431957\n",
      "SubSGD iter. 1/499: loss=73.36780585492637, w0=1.4, w1=0.1585577415355719\n",
      "SubSGD iter. 2/499: loss=72.66780585492637, w0=2.0999999999999996, w1=0.15453902354043467\n",
      "SubSGD iter. 3/499: loss=71.96780585492638, w0=2.8, w1=0.13267357942763422\n",
      "SubSGD iter. 4/499: loss=71.26780585492638, w0=3.5, w1=0.33844217800399296\n",
      "SubSGD iter. 5/499: loss=70.56780585492638, w0=4.2, w1=0.2661186347074165\n",
      "SubSGD iter. 6/499: loss=69.86780585492637, w0=4.9, w1=0.3679274470378272\n",
      "SubSGD iter. 7/499: loss=69.16780585492637, w0=5.6000000000000005, w1=0.4593271988599945\n",
      "SubSGD iter. 8/499: loss=68.46780585492638, w0=6.300000000000001, w1=0.3920089716040031\n",
      "SubSGD iter. 9/499: loss=67.76780585492638, w0=7.000000000000001, w1=0.2846477995306054\n",
      "SubSGD iter. 10/499: loss=67.06780585492638, w0=7.700000000000001, w1=0.27762623054396496\n",
      "SubSGD iter. 11/499: loss=66.36780585492637, w0=8.4, w1=0.04674609371963645\n",
      "SubSGD iter. 12/499: loss=65.66780585492637, w0=9.1, w1=-0.19978633413792052\n",
      "SubSGD iter. 13/499: loss=64.96780585492637, w0=9.799999999999999, w1=-0.2266195550641573\n",
      "SubSGD iter. 14/499: loss=64.26780585492638, w0=10.499999999999998, w1=-0.1623184175072845\n",
      "SubSGD iter. 15/499: loss=63.567805854926384, w0=11.199999999999998, w1=0.039015153258153745\n",
      "SubSGD iter. 16/499: loss=62.867805854926374, w0=11.899999999999997, w1=-0.04199297891681861\n",
      "SubSGD iter. 17/499: loss=62.167805854926385, w0=12.599999999999996, w1=-0.10031074786061059\n",
      "SubSGD iter. 18/499: loss=61.46780585492638, w0=13.299999999999995, w1=0.09183133390512985\n",
      "SubSGD iter. 19/499: loss=60.76780585492638, w0=13.999999999999995, w1=0.041184396785221004\n",
      "SubSGD iter. 20/499: loss=60.067805854926384, w0=14.699999999999994, w1=0.11493465304460987\n",
      "SubSGD iter. 21/499: loss=59.367805854926395, w0=15.399999999999993, w1=0.3515486142467106\n",
      "SubSGD iter. 22/499: loss=58.667805854926385, w0=16.099999999999994, w1=0.42033015302320526\n",
      "SubSGD iter. 23/499: loss=57.96780585492639, w0=16.799999999999994, w1=0.34923009265934035\n",
      "SubSGD iter. 24/499: loss=57.26780585492637, w0=17.499999999999993, w1=0.3117421990131699\n",
      "SubSGD iter. 25/499: loss=56.567805854926384, w0=18.199999999999992, w1=0.1505229867004528\n",
      "SubSGD iter. 26/499: loss=55.867805854926374, w0=18.89999999999999, w1=0.21043843443511445\n",
      "SubSGD iter. 27/499: loss=55.167805854926385, w0=19.59999999999999, w1=0.03920896623707448\n",
      "SubSGD iter. 28/499: loss=54.46780585492639, w0=20.29999999999999, w1=-0.13104724160503362\n",
      "SubSGD iter. 29/499: loss=53.767805854926394, w0=20.99999999999999, w1=-0.16889597203462933\n",
      "SubSGD iter. 30/499: loss=53.067805854926384, w0=21.69999999999999, w1=-0.28753834971636716\n",
      "SubSGD iter. 31/499: loss=52.36780585492639, w0=22.399999999999988, w1=-0.2290945196728037\n",
      "SubSGD iter. 32/499: loss=51.667805854926385, w0=23.099999999999987, w1=-0.36975625213080476\n",
      "SubSGD iter. 33/499: loss=50.96780585492639, w0=23.799999999999986, w1=-0.6566686059872944\n",
      "SubSGD iter. 34/499: loss=50.267805854926394, w0=24.499999999999986, w1=-0.7491144414699525\n",
      "SubSGD iter. 35/499: loss=49.56780585492639, w0=25.199999999999985, w1=-0.798351344736576\n",
      "SubSGD iter. 36/499: loss=48.867805854926395, w0=25.899999999999984, w1=-0.5972185341107931\n",
      "SubSGD iter. 37/499: loss=48.1678058549264, w0=26.599999999999984, w1=-0.947035329594186\n",
      "SubSGD iter. 38/499: loss=47.4678058549264, w0=27.299999999999983, w1=-0.7837885216846392\n",
      "SubSGD iter. 39/499: loss=46.7678058549264, w0=27.999999999999982, w1=-0.8150336434424742\n",
      "SubSGD iter. 40/499: loss=46.067805854926405, w0=28.69999999999998, w1=-0.7891954113980131\n",
      "SubSGD iter. 41/499: loss=45.367805854926395, w0=29.39999999999998, w1=-0.653471767124623\n",
      "SubSGD iter. 42/499: loss=44.667805854926385, w0=30.09999999999998, w1=-0.7851812086865092\n",
      "SubSGD iter. 43/499: loss=43.9678058549264, w0=30.79999999999998, w1=-0.7202386109567628\n",
      "SubSGD iter. 44/499: loss=43.2678058549264, w0=31.49999999999998, w1=-0.7016699326411072\n",
      "SubSGD iter. 45/499: loss=42.567805854926405, w0=32.19999999999998, w1=-0.6472836590986581\n",
      "SubSGD iter. 46/499: loss=41.867805854926395, w0=32.899999999999984, w1=-0.6201571682793527\n",
      "SubSGD iter. 47/499: loss=41.167805854926385, w0=33.59999999999999, w1=-0.7475114806334741\n",
      "SubSGD iter. 48/499: loss=40.4678058549264, w0=34.29999999999999, w1=-0.7195773753101189\n",
      "SubSGD iter. 49/499: loss=39.767805854926394, w0=34.99999999999999, w1=-0.876743475877672\n",
      "SubSGD iter. 50/499: loss=39.06780585492639, w0=35.699999999999996, w1=-1.0481743281304112\n",
      "SubSGD iter. 51/499: loss=38.36780585492639, w0=36.4, w1=-1.2338447146728062\n",
      "SubSGD iter. 52/499: loss=37.66780585492638, w0=37.1, w1=-1.4327531620346408\n",
      "SubSGD iter. 53/499: loss=36.967805854926375, w0=37.800000000000004, w1=-1.0549760313973133\n",
      "SubSGD iter. 54/499: loss=36.26780585492637, w0=38.50000000000001, w1=-1.150275879717152\n",
      "SubSGD iter. 55/499: loss=35.56780585492638, w0=39.20000000000001, w1=-1.1877417795893648\n",
      "SubSGD iter. 56/499: loss=34.86780585492637, w0=39.90000000000001, w1=-1.3172836939984596\n",
      "SubSGD iter. 57/499: loss=34.16780585492637, w0=40.600000000000016, w1=-1.625499037363308\n",
      "SubSGD iter. 58/499: loss=33.46780585492636, w0=41.30000000000002, w1=-1.4778285110427098\n",
      "SubSGD iter. 59/499: loss=32.767805854926365, w0=42.00000000000002, w1=-1.5293718151491082\n",
      "SubSGD iter. 60/499: loss=32.067805854926355, w0=42.700000000000024, w1=-1.740816149839201\n",
      "SubSGD iter. 61/499: loss=31.36780585492636, w0=43.40000000000003, w1=-1.4471237631206348\n",
      "SubSGD iter. 62/499: loss=30.66780585492635, w0=44.10000000000003, w1=-1.3463530085254798\n",
      "SubSGD iter. 63/499: loss=29.967805854926354, w0=44.80000000000003, w1=-1.5574400406003384\n",
      "SubSGD iter. 64/499: loss=29.277371877564548, w0=45.41250000000003, w1=-1.6691469182557928\n",
      "SubSGD iter. 65/499: loss=28.682979197824622, w0=46.11250000000003, w1=-1.4747750664889296\n",
      "SubSGD iter. 66/499: loss=27.99395732860873, w0=46.812500000000036, w1=-1.7729917059626972\n",
      "SubSGD iter. 67/499: loss=27.34147059672879, w0=47.337500000000034, w1=-1.3498182312040625\n",
      "SubSGD iter. 68/499: loss=26.81012780982032, w0=48.03750000000004, w1=-1.3180644272549085\n",
      "SubSGD iter. 69/499: loss=26.151959435638474, w0=48.650000000000034, w1=-1.0932643414769498\n",
      "SubSGD iter. 70/499: loss=25.558120716127924, w0=49.26250000000003, w1=-1.0994531729848573\n",
      "SubSGD iter. 71/499: loss=24.993298469630915, w0=49.962500000000034, w1=-0.9283945578326642\n",
      "SubSGD iter. 72/499: loss=24.331529152896056, w0=50.57500000000003, w1=-1.106963233015561\n",
      "SubSGD iter. 73/499: loss=23.810806660821157, w0=51.10000000000003, w1=-0.7661487610936221\n",
      "SubSGD iter. 74/499: loss=23.290499140619307, w0=51.71250000000003, w1=-1.2209775312495301\n",
      "SubSGD iter. 75/499: loss=22.845628799010036, w0=52.237500000000026, w1=-1.0461331199999888\n",
      "SubSGD iter. 76/499: loss=22.36222511241127, w0=52.85000000000002, w1=-0.994549955982933\n",
      "SubSGD iter. 77/499: loss=21.839191312134755, w0=53.37500000000002, w1=-0.7627257698346582\n",
      "SubSGD iter. 78/499: loss=21.353451953572097, w0=53.98750000000002, w1=-0.6711157043483069\n",
      "SubSGD iter. 79/499: loss=20.828851972623948, w0=54.68750000000002, w1=-0.43226335196011917\n",
      "SubSGD iter. 80/499: loss=20.213232057005907, w0=55.21250000000002, w1=-0.022631236440009472\n",
      "SubSGD iter. 81/499: loss=19.697494649121467, w0=55.65000000000002, w1=0.13176709743578124\n",
      "SubSGD iter. 82/499: loss=19.323531267832724, w0=56.17500000000002, w1=0.5948635919527345\n",
      "SubSGD iter. 83/499: loss=18.794569597264598, w0=56.787500000000016, w1=0.6990174042442352\n",
      "SubSGD iter. 84/499: loss=18.319754896119846, w0=57.48750000000002, w1=0.9973800078821471\n",
      "SubSGD iter. 85/499: loss=17.728249616075708, w0=58.01250000000002, w1=1.3451550475286993\n",
      "SubSGD iter. 86/499: loss=17.24767782302421, w0=58.45000000000002, w1=1.6409943662771234\n",
      "SubSGD iter. 87/499: loss=16.85117871665355, w0=58.975000000000016, w1=2.0734125283970783\n",
      "SubSGD iter. 88/499: loss=16.351277646693195, w0=59.412500000000016, w1=2.478109266986787\n",
      "SubSGD iter. 89/499: loss=15.921287813973606, w0=59.850000000000016, w1=2.825427534612763\n",
      "SubSGD iter. 90/499: loss=15.521007087355672, w0=60.11250000000002, w1=3.27811667356239\n",
      "SubSGD iter. 91/499: loss=15.186863531810802, w0=60.55000000000002, w1=3.779255703430189\n",
      "SubSGD iter. 92/499: loss=14.729687079248512, w0=60.725000000000016, w1=4.009640317319122\n",
      "SubSGD iter. 93/499: loss=14.535211718430688, w0=61.07500000000002, w1=4.362314359665216\n",
      "SubSGD iter. 94/499: loss=14.189770182490006, w0=60.98750000000002, w1=4.718812445757856\n",
      "SubSGD iter. 95/499: loss=14.108903519621883, w0=61.68750000000002, w1=4.9045281278418384\n",
      "SubSGD iter. 96/499: loss=13.615573620463902, w0=62.03750000000002, w1=5.090812896471941\n",
      "SubSGD iter. 97/499: loss=13.33742257116418, w0=62.47500000000002, w1=5.458489121117945\n",
      "SubSGD iter. 98/499: loss=12.944393170098024, w0=62.91250000000002, w1=5.7961761142547\n",
      "SubSGD iter. 99/499: loss=12.568322691916027, w0=63.35000000000002, w1=6.023452106785068\n",
      "SubSGD iter. 100/499: loss=12.241686694277172, w0=63.700000000000024, w1=6.58486275549183\n",
      "SubSGD iter. 101/499: loss=11.819641871126917, w0=64.22500000000002, w1=7.020408397656431\n",
      "SubSGD iter. 102/499: loss=11.361543021250156, w0=64.66250000000002, w1=7.157348768121244\n",
      "SubSGD iter. 103/499: loss=11.081460780101796, w0=64.75000000000003, w1=7.532862259598421\n",
      "SubSGD iter. 104/499: loss=10.87503340868347, w0=65.27500000000003, w1=7.914680423074735\n",
      "SubSGD iter. 105/499: loss=10.44600441736238, w0=65.88750000000003, w1=8.020145323258436\n",
      "SubSGD iter. 106/499: loss=10.115964979772679, w0=65.97500000000004, w1=8.5526169967214\n",
      "SubSGD iter. 107/499: loss=9.832695538918763, w0=66.41250000000004, w1=8.96217090133532\n",
      "SubSGD iter. 108/499: loss=9.449498651291645, w0=66.50000000000004, w1=9.360037290215047\n",
      "SubSGD iter. 109/499: loss=9.233830927095742, w0=66.93750000000004, w1=9.675254747079062\n",
      "SubSGD iter. 110/499: loss=8.897574745719378, w0=67.55000000000004, w1=9.953338657871155\n",
      "SubSGD iter. 111/499: loss=8.518645559740005, w0=67.81250000000004, w1=10.204881684244517\n",
      "SubSGD iter. 112/499: loss=8.305817228928733, w0=68.25000000000004, w1=10.366237948723846\n",
      "SubSGD iter. 113/499: loss=8.070491641902564, w0=68.60000000000004, w1=10.556890815940974\n",
      "SubSGD iter. 114/499: loss=7.862846663382501, w0=68.95000000000003, w1=10.674274143856998\n",
      "SubSGD iter. 115/499: loss=7.694035010836791, w0=68.95000000000003, w1=11.202787562338754\n",
      "SubSGD iter. 116/499: loss=7.468996395874192, w0=69.38750000000003, w1=11.240262897854471\n",
      "SubSGD iter. 117/499: loss=7.308944438063049, w0=69.12500000000003, w1=11.55856778598629\n",
      "SubSGD iter. 118/499: loss=7.263917109572044, w0=69.12500000000003, w1=11.979900975380717\n",
      "SubSGD iter. 119/499: loss=7.1086401258723395, w0=69.21250000000003, w1=12.262989276800871\n",
      "SubSGD iter. 120/499: loss=6.979848753996024, w0=69.56250000000003, w1=12.484531652217376\n",
      "SubSGD iter. 121/499: loss=6.778416098254766, w0=69.56250000000003, w1=13.015936169420957\n",
      "SubSGD iter. 122/499: loss=6.603903450803723, w0=70.00000000000003, w1=12.959278298020443\n",
      "SubSGD iter. 123/499: loss=6.468606535592453, w0=70.17500000000003, w1=13.336648865626664\n",
      "SubSGD iter. 124/499: loss=6.2841293073925915, w0=70.61250000000003, w1=13.438631933920057\n",
      "SubSGD iter. 125/499: loss=6.113680444001025, w0=70.87500000000003, w1=13.457291274141996\n",
      "SubSGD iter. 126/499: loss=6.032379492859966, w0=71.13750000000003, w1=13.54761166460914\n",
      "SubSGD iter. 127/499: loss=5.931450449350583, w0=71.66250000000004, w1=13.5633748332496\n",
      "SubSGD iter. 128/499: loss=5.797855204346247, w0=71.92500000000004, w1=13.999748253464173\n",
      "SubSGD iter. 129/499: loss=5.616204889595505, w0=71.92500000000004, w1=13.811620513727444\n",
      "SubSGD iter. 130/499: loss=5.6657493346914, w0=71.75000000000004, w1=14.055409532885127\n",
      "SubSGD iter. 131/499: loss=5.6329591076519, w0=72.10000000000004, w1=14.163904484312214\n",
      "SubSGD iter. 132/499: loss=5.548605180478354, w0=72.27500000000003, w1=14.44608502121312\n",
      "SubSGD iter. 133/499: loss=5.47106140138129, w0=72.36250000000004, w1=14.757394123385504\n",
      "SubSGD iter. 134/499: loss=5.414681152595539, w0=72.53750000000004, w1=14.954846462248653\n",
      "SubSGD iter. 135/499: loss=5.376273199653963, w0=72.62500000000004, w1=15.352174436812504\n",
      "SubSGD iter. 136/499: loss=5.333702701449484, w0=72.53750000000004, w1=15.5054664383992\n",
      "SubSGD iter. 137/499: loss=5.32493335395096, w0=72.36250000000004, w1=15.503930060183238\n",
      "SubSGD iter. 138/499: loss=5.33162421705705, w0=72.18750000000004, w1=15.662411314991546\n",
      "SubSGD iter. 139/499: loss=5.3388771895851805, w0=72.18750000000004, w1=15.47065815234463\n",
      "SubSGD iter. 140/499: loss=5.347685236778786, w0=72.01250000000005, w1=15.180546256994122\n",
      "SubSGD iter. 141/499: loss=5.390701753977801, w0=71.92500000000004, w1=15.227363860243939\n",
      "SubSGD iter. 142/499: loss=5.396042275993902, w0=72.01250000000005, w1=15.328149006419864\n",
      "SubSGD iter. 143/499: loss=5.376486904496343, w0=71.75000000000004, w1=15.410945178714957\n",
      "SubSGD iter. 144/499: loss=5.4025521548055755, w0=71.92500000000004, w1=15.404102162503678\n",
      "SubSGD iter. 145/499: loss=5.380580628889637, w0=72.01250000000005, w1=15.586301359441912\n",
      "SubSGD iter. 146/499: loss=5.358177989673852, w0=72.18750000000004, w1=15.837260439723831\n",
      "SubSGD iter. 147/499: loss=5.334755473004832, w0=72.27500000000005, w1=15.85867319637834\n",
      "SubSGD iter. 148/499: loss=5.32732001724787, w0=72.18750000000004, w1=15.937250223505064\n",
      "SubSGD iter. 149/499: loss=5.332398414882249, w0=72.18750000000004, w1=15.989838803658733\n",
      "SubSGD iter. 150/499: loss=5.3311587448347835, w0=72.18750000000004, w1=16.101805930897594\n",
      "SubSGD iter. 151/499: loss=5.330707239319613, w0=72.18750000000004, w1=16.163285448129624\n",
      "SubSGD iter. 152/499: loss=5.332034076811235, w0=72.36250000000004, w1=15.825873621920937\n",
      "SubSGD iter. 153/499: loss=5.321702613980551, w0=72.62500000000004, w1=15.715035048893833\n",
      "SubSGD iter. 154/499: loss=5.313682963926155, w0=72.45000000000005, w1=15.92038355796444\n",
      "SubSGD iter. 155/499: loss=5.316374969423763, w0=72.36250000000004, w1=15.731337231793743\n",
      "SubSGD iter. 156/499: loss=5.324265932343001, w0=72.36250000000004, w1=15.83073258216808\n",
      "SubSGD iter. 157/499: loss=5.321576225834204, w0=72.62500000000004, w1=15.831307471545696\n",
      "SubSGD iter. 158/499: loss=5.311668262882616, w0=72.53750000000004, w1=16.068822908264675\n",
      "SubSGD iter. 159/499: loss=5.3126659293199605, w0=72.71250000000003, w1=15.851573485510448\n",
      "SubSGD iter. 160/499: loss=5.311242331825362, w0=72.71250000000003, w1=15.686176048475613\n",
      "SubSGD iter. 161/499: loss=5.3154091939462225, w0=72.71250000000003, w1=15.777832515073348\n",
      "SubSGD iter. 162/499: loss=5.311773592703687, w0=72.62500000000003, w1=15.745762902086264\n",
      "SubSGD iter. 163/499: loss=5.31312375224421, w0=72.36250000000003, w1=15.700519685878522\n",
      "SubSGD iter. 164/499: loss=5.325263105279221, w0=72.45000000000003, w1=15.755155110157242\n",
      "SubSGD iter. 165/499: loss=5.318903234332281, w0=72.53750000000004, w1=16.0390962205998\n",
      "SubSGD iter. 166/499: loss=5.312554823668078, w0=72.88750000000003, w1=16.11147143829304\n",
      "SubSGD iter. 167/499: loss=5.317238496889667, w0=72.80000000000003, w1=16.081616042063384\n",
      "SubSGD iter. 168/499: loss=5.313568403630379, w0=72.45000000000003, w1=16.113736163441395\n",
      "SubSGD iter. 169/499: loss=5.316299142404391, w0=72.36250000000003, w1=15.989672776987195\n",
      "SubSGD iter. 170/499: loss=5.320174461659392, w0=72.01250000000003, w1=16.184237081835583\n",
      "SubSGD iter. 171/499: loss=5.350050036207486, w0=71.92500000000003, w1=16.283224945462763\n",
      "SubSGD iter. 172/499: loss=5.363636636052112, w0=71.92500000000003, w1=16.431199139360913\n",
      "SubSGD iter. 173/499: loss=5.368391180851181, w0=71.92500000000003, w1=16.24091837968748\n",
      "SubSGD iter. 174/499: loss=5.362435490522902, w0=72.10000000000002, w1=16.26503727638792\n",
      "SubSGD iter. 175/499: loss=5.342689742432436, w0=72.10000000000002, w1=16.08522881417274\n",
      "SubSGD iter. 176/499: loss=5.338599334027119, w0=71.83750000000002, w1=16.173590581008746\n",
      "SubSGD iter. 177/499: loss=5.371786331685708, w0=71.83750000000002, w1=15.967686152672416\n",
      "SubSGD iter. 178/499: loss=5.365940402596249, w0=72.01250000000002, w1=16.137159078194383\n",
      "SubSGD iter. 179/499: loss=5.348959978172994, w0=72.01250000000002, w1=16.071723039763135\n",
      "SubSGD iter. 180/499: loss=5.3476473239119455, w0=72.18750000000001, w1=16.080825812657057\n",
      "SubSGD iter. 181/499: loss=5.330282637196041, w0=72.01250000000002, w1=16.351111601345153\n",
      "SubSGD iter. 182/499: loss=5.354922972526614, w0=72.27500000000002, w1=16.15411088235184\n",
      "SubSGD iter. 183/499: loss=5.324905380270344, w0=72.27500000000002, w1=16.078260034958003\n",
      "SubSGD iter. 184/499: loss=5.324193607070366, w0=72.36250000000003, w1=15.993048208716022\n",
      "SubSGD iter. 185/499: loss=5.320152462924734, w0=72.53750000000002, w1=15.587734239722746\n",
      "SubSGD iter. 186/499: loss=5.319845611281443, w0=72.27500000000002, w1=15.73412064899051\n",
      "SubSGD iter. 187/499: loss=5.330256093138897, w0=72.45000000000002, w1=15.601827404087121\n",
      "SubSGD iter. 188/499: loss=5.322392165758962, w0=72.53750000000002, w1=15.257112455480415\n",
      "SubSGD iter. 189/499: loss=5.3436480106408615, w0=72.18750000000003, w1=15.162276159290112\n",
      "SubSGD iter. 190/499: loss=5.3769152435077245, w0=72.27500000000003, w1=15.189241554292458\n",
      "SubSGD iter. 191/499: loss=5.366845069641784, w0=72.10000000000004, w1=15.33824912748851\n",
      "SubSGD iter. 192/499: loss=5.36615691093881, w0=72.27500000000003, w1=15.645703141144454\n",
      "SubSGD iter. 193/499: loss=5.333101177734185, w0=72.18750000000003, w1=15.859417722334383\n",
      "SubSGD iter. 194/499: loss=5.334233159614639, w0=72.45000000000003, w1=15.490262657584168\n",
      "SubSGD iter. 195/499: loss=5.327960883451831, w0=72.71250000000003, w1=15.513194687907525\n",
      "SubSGD iter. 196/499: loss=5.324862598676254, w0=72.71250000000003, w1=15.976951166147591\n",
      "SubSGD iter. 197/499: loss=5.311409213493204, w0=72.71250000000003, w1=16.005829150648974\n",
      "SubSGD iter. 198/499: loss=5.311496924174148, w0=72.71250000000003, w1=16.140077742135873\n",
      "SubSGD iter. 199/499: loss=5.311926211851963, w0=72.53750000000004, w1=16.321977783634157\n",
      "SubSGD iter. 200/499: loss=5.317282283393373, w0=72.62500000000004, w1=16.2918439069185\n",
      "SubSGD iter. 201/499: loss=5.314263140546724, w0=72.53750000000004, w1=16.14223870894145\n",
      "SubSGD iter. 202/499: loss=5.313534919447902, w0=72.80000000000004, w1=15.975691478858332\n",
      "SubSGD iter. 203/499: loss=5.312961881941777, w0=72.62500000000004, w1=15.828473755533395\n",
      "SubSGD iter. 204/499: loss=5.31169077383901, w0=72.80000000000004, w1=15.779063410475\n",
      "SubSGD iter. 205/499: loss=5.314270180914989, w0=72.71250000000003, w1=15.874462724134068\n",
      "SubSGD iter. 206/499: loss=5.3110979268437655, w0=72.80000000000004, w1=15.915481503671376\n",
      "SubSGD iter. 207/499: loss=5.312872834231646, w0=72.53750000000004, w1=16.09567455374408\n",
      "SubSGD iter. 208/499: loss=5.312956548649529, w0=72.36250000000004, w1=16.191558626497553\n",
      "SubSGD iter. 209/499: loss=5.320948220508943, w0=72.36250000000004, w1=16.32788562255016\n",
      "SubSGD iter. 210/499: loss=5.326204181459616, w0=72.45000000000005, w1=16.152987969119135\n",
      "SubSGD iter. 211/499: loss=5.316445848875913, w0=72.45000000000005, w1=16.022790518687664\n",
      "SubSGD iter. 212/499: loss=5.315959226458821, w0=72.62500000000004, w1=15.983272649416648\n",
      "SubSGD iter. 213/499: loss=5.3106004208119355, w0=72.45000000000005, w1=15.917850618261282\n",
      "SubSGD iter. 214/499: loss=5.316413727535044, w0=72.27500000000005, w1=16.020773780777752\n",
      "SubSGD iter. 215/499: loss=5.324754561925235, w0=72.10000000000005, w1=16.187043326619346\n",
      "SubSGD iter. 216/499: loss=5.340273332198105, w0=72.27500000000005, w1=15.972324673013084\n",
      "SubSGD iter. 217/499: loss=5.3252273316786685, w0=72.45000000000005, w1=16.010667169221506\n",
      "SubSGD iter. 218/499: loss=5.315913914560616, w0=72.62500000000004, w1=15.478403236206628\n",
      "SubSGD iter. 219/499: loss=5.325768165878434, w0=72.88750000000005, w1=15.750792275132762\n",
      "SubSGD iter. 220/499: loss=5.319611471318075, w0=72.80000000000004, w1=15.819620158769613\n",
      "SubSGD iter. 221/499: loss=5.312972507246817, w0=72.97500000000004, w1=15.91800731516158\n",
      "SubSGD iter. 222/499: loss=5.318928381887351, w0=72.97500000000004, w1=15.895063135810865\n",
      "SubSGD iter. 223/499: loss=5.319575851875698, w0=72.88750000000003, w1=16.028077982213443\n",
      "SubSGD iter. 224/499: loss=5.31568711357375, w0=72.88750000000003, w1=15.898516349614697\n",
      "SubSGD iter. 225/499: loss=5.31468204316284, w0=72.80000000000003, w1=15.913493351751125\n",
      "SubSGD iter. 226/499: loss=5.3128698938488395, w0=72.62500000000003, w1=15.992840858692675\n",
      "SubSGD iter. 227/499: loss=5.310622423055709, w0=72.62500000000003, w1=15.776967780637937\n",
      "SubSGD iter. 228/499: loss=5.312555859247895, w0=72.53750000000002, w1=15.64107234308407\n",
      "SubSGD iter. 229/499: loss=5.317628010234511, w0=72.36250000000003, w1=15.570959265208685\n",
      "SubSGD iter. 230/499: loss=5.329455332183102, w0=72.45000000000003, w1=15.858814256631884\n",
      "SubSGD iter. 231/499: loss=5.317317080216832, w0=72.62500000000003, w1=15.807575951895956\n",
      "SubSGD iter. 232/499: loss=5.311998825639873, w0=72.62500000000003, w1=15.920021449220876\n",
      "SubSGD iter. 233/499: loss=5.310963521661456, w0=72.71250000000003, w1=15.887675429473619\n",
      "SubSGD iter. 234/499: loss=5.311138057600897, w0=72.88750000000003, w1=15.927716044899405\n",
      "SubSGD iter. 235/499: loss=5.314786747845642, w0=72.80000000000003, w1=15.740724133537903\n",
      "SubSGD iter. 236/499: loss=5.315932234856835, w0=72.88750000000003, w1=15.691327173865279\n",
      "SubSGD iter. 237/499: loss=5.321735064719226, w0=72.80000000000003, w1=15.565737401015069\n",
      "SubSGD iter. 238/499: loss=5.324590157393207, w0=72.80000000000003, w1=15.904186431372365\n",
      "SubSGD iter. 239/499: loss=5.312856129353081, w0=72.80000000000003, w1=15.525105986471745\n",
      "SubSGD iter. 240/499: loss=5.326810657944075, w0=72.71250000000002, w1=15.654209654587168\n",
      "SubSGD iter. 241/499: loss=5.317156152452008, w0=72.53750000000002, w1=15.861345484234965\n",
      "SubSGD iter. 242/499: loss=5.313813001768823, w0=72.53750000000002, w1=15.822765154645584\n",
      "SubSGD iter. 243/499: loss=5.314403343775628, w0=72.80000000000003, w1=15.688024556512369\n",
      "SubSGD iter. 244/499: loss=5.318271830661639, w0=72.80000000000003, w1=15.675164167429173\n",
      "SubSGD iter. 245/499: loss=5.318842767167118, w0=72.88750000000003, w1=15.953618570348102\n",
      "SubSGD iter. 246/499: loss=5.315019124250579, w0=72.97500000000004, w1=16.109492041602024\n",
      "SubSGD iter. 247/499: loss=5.320998874127698, w0=73.15000000000003, w1=16.224323702899913\n",
      "SubSGD iter. 248/499: loss=5.332900668786679, w0=72.88750000000003, w1=16.43740528809655\n",
      "SubSGD iter. 249/499: loss=5.328476856183532, w0=72.80000000000003, w1=16.283667379712654\n",
      "SubSGD iter. 250/499: loss=5.317459252296565, w0=72.80000000000003, w1=16.311103873854986\n",
      "SubSGD iter. 251/499: loss=5.31804656993886, w0=72.80000000000003, w1=16.371226448280634\n",
      "SubSGD iter. 252/499: loss=5.319747971188239, w0=72.80000000000003, w1=15.794072368706322\n",
      "SubSGD iter. 253/499: loss=5.313734187130755, w0=72.71250000000002, w1=16.004984216543495\n",
      "SubSGD iter. 254/499: loss=5.311494357868118, w0=72.88750000000002, w1=16.16509032489719\n",
      "SubSGD iter. 255/499: loss=5.318386286322948, w0=72.53750000000002, w1=16.246148419866238\n",
      "SubSGD iter. 256/499: loss=5.314825576224803, w0=72.36250000000003, w1=16.432695783595463\n",
      "SubSGD iter. 257/499: loss=5.331168766822202, w0=72.36250000000003, w1=16.141355138827116\n",
      "SubSGD iter. 258/499: loss=5.319867716863365, w0=72.45000000000003, w1=15.981258904435835\n",
      "SubSGD iter. 259/499: loss=5.315897614306035, w0=72.36250000000003, w1=16.09017048332651\n",
      "SubSGD iter. 260/499: loss=5.319676410499874, w0=72.53750000000002, w1=16.31505923082668\n",
      "SubSGD iter. 261/499: loss=5.31697952202003, w0=72.88750000000002, w1=16.065475316769536\n",
      "SubSGD iter. 262/499: loss=5.316253883713224, w0=73.23750000000001, w1=16.183219310125793\n",
      "SubSGD iter. 263/499: loss=5.336365436291113, w0=73.15, w1=16.12705198660815\n",
      "SubSGD iter. 264/499: loss=5.330123505166551, w0=73.0625, w1=15.994410292909796\n",
      "SubSGD iter. 265/499: loss=5.32369485403521, w0=73.0625, w1=16.283856327474176\n",
      "SubSGD iter. 266/499: loss=5.330563254923502, w0=73.0625, w1=16.114864817766605\n",
      "SubSGD iter. 267/499: loss=5.325471689400742, w0=72.975, w1=16.232199156247557\n",
      "SubSGD iter. 268/499: loss=5.324222145082002, w0=72.88749999999999, w1=16.148283441067896\n",
      "SubSGD iter. 269/499: loss=5.318026510786986, w0=73.06249999999999, w1=16.188501089871952\n",
      "SubSGD iter. 270/499: loss=5.327405967276529, w0=72.97499999999998, w1=16.376948533781558\n",
      "SubSGD iter. 271/499: loss=5.330339260341609, w0=72.97499999999998, w1=16.17887770805527\n",
      "SubSGD iter. 272/499: loss=5.322821497078043, w0=72.88749999999997, w1=16.02410145029278\n",
      "SubSGD iter. 273/499: loss=5.315651439361454, w0=72.53749999999998, w1=16.21628105828195\n",
      "SubSGD iter. 274/499: loss=5.314454595377165, w0=72.18749999999999, w1=16.001040017436743\n",
      "SubSGD iter. 275/499: loss=5.330894698740078, w0=72.27499999999999, w1=15.659730182695656\n",
      "SubSGD iter. 276/499: loss=5.332647300379755, w0=72.5375, w1=15.771846725128865\n",
      "SubSGD iter. 277/499: loss=5.315248066485082, w0=72.625, w1=15.85194097119468\n",
      "SubSGD iter. 278/499: loss=5.3115043509701945, w0=73.0625, w1=15.89232484332147\n",
      "SubSGD iter. 279/499: loss=5.324999111327266, w0=73.325, w1=15.510645294759094\n",
      "SubSGD iter. 280/499: loss=5.352430850582233, w0=73.2375, w1=15.44805459907091\n",
      "SubSGD iter. 281/499: loss=5.347784204765416, w0=73.14999999999999, w1=15.8091871042363\n",
      "SubSGD iter. 282/499: loss=5.333005381809049, w0=73.14999999999999, w1=15.86624973518287\n",
      "SubSGD iter. 283/499: loss=5.332025385975343, w0=73.06249999999999, w1=15.867472898754501\n",
      "SubSGD iter. 284/499: loss=5.325579597887995, w0=72.97499999999998, w1=15.815753948713596\n",
      "SubSGD iter. 285/499: loss=5.321813906140387, w0=73.06249999999999, w1=15.807679161373477\n",
      "SubSGD iter. 286/499: loss=5.327239791249079, w0=72.88749999999999, w1=15.967874939416443\n",
      "SubSGD iter. 287/499: loss=5.315147020806383, w0=72.62499999999999, w1=16.184409828077214\n",
      "SubSGD iter. 288/499: loss=5.311942357129855, w0=72.62499999999999, w1=16.062707372016007\n",
      "SubSGD iter. 289/499: loss=5.310803342932286, w0=72.53749999999998, w1=16.322154453477765\n",
      "SubSGD iter. 290/499: loss=5.317290014606241, w0=72.53749999999998, w1=15.98884445967614\n",
      "SubSGD iter. 291/499: loss=5.31271256970535, w0=72.79999999999998, w1=16.074509046446412\n",
      "SubSGD iter. 292/499: loss=5.313504645442218, w0=72.79999999999998, w1=15.722185615489433\n",
      "SubSGD iter. 293/499: loss=5.316755251691151, w0=72.79999999999998, w1=15.443873563297448\n",
      "SubSGD iter. 294/499: loss=5.331249997367386, w0=72.97499999999998, w1=15.493602262039094\n",
      "SubSGD iter. 295/499: loss=5.333833894420877, w0=72.79999999999998, w1=15.509244196643857\n",
      "SubSGD iter. 296/499: loss=5.327677502316749, w0=72.79999999999998, w1=15.749610149066292\n",
      "SubSGD iter. 297/499: loss=5.31553774052073, w0=73.06249999999999, w1=15.700239437090904\n",
      "SubSGD iter. 298/499: loss=5.330271671185427, w0=72.88749999999999, w1=15.645466119320544\n",
      "SubSGD iter. 299/499: loss=5.3236265574671195, w0=72.5375, w1=15.808371664681074\n",
      "SubSGD iter. 300/499: loss=5.3146235876626315, w0=72.5375, w1=15.989024529186507\n",
      "SubSGD iter. 301/499: loss=5.312711660939626, w0=72.44999999999999, w1=15.898353537276309\n",
      "SubSGD iter. 302/499: loss=5.316712064693852, w0=72.36249999999998, w1=15.999656987119394\n",
      "SubSGD iter. 303/499: loss=5.3201093914677555, w0=72.18749999999999, w1=15.980498961355842\n",
      "SubSGD iter. 304/499: loss=5.33137891283927, w0=72.01249999999999, w1=15.912711567616862\n",
      "SubSGD iter. 305/499: loss=5.346838250498318, w0=71.92499999999998, w1=16.080356670265857\n",
      "SubSGD iter. 306/499: loss=5.358040798006532, w0=71.66249999999998, w1=16.08281278090454\n",
      "SubSGD iter. 307/499: loss=5.3922230561419235, w0=71.92499999999998, w1=15.665400297757138\n",
      "SubSGD iter. 308/499: loss=5.362827744629971, w0=72.01249999999999, w1=15.606121100594457\n",
      "SubSGD iter. 309/499: loss=5.356955226024445, w0=72.18749999999999, w1=15.74799799081531\n",
      "SubSGD iter. 310/499: loss=5.336859655775166, w0=72.09999999999998, w1=15.68281118262659\n",
      "SubSGD iter. 311/499: loss=5.345326996789053, w0=72.18749999999999, w1=15.89204950580556\n",
      "SubSGD iter. 312/499: loss=5.333463930925706, w0=72.18749999999999, w1=16.211774670122498\n",
      "SubSGD iter. 313/499: loss=5.3331386424420115, w0=72.18749999999999, w1=16.182322846464654\n",
      "SubSGD iter. 314/499: loss=5.3324449377774785, w0=72.53749999999998, w1=16.267162287079717\n",
      "SubSGD iter. 315/499: loss=5.315121909503923, w0=72.44999999999997, w1=16.37468134714687\n",
      "SubSGD iter. 316/499: loss=5.323920316417405, w0=72.44999999999997, w1=16.196854863015076\n",
      "SubSGD iter. 317/499: loss=5.316812313580319, w0=72.27499999999998, w1=16.006075186115183\n",
      "SubSGD iter. 318/499: loss=5.324897991831279, w0=72.27499999999998, w1=16.13497760634874\n",
      "SubSGD iter. 319/499: loss=5.32449245009325, w0=72.18749999999997, w1=16.074449779384707\n",
      "SubSGD iter. 320/499: loss=5.330192653409234, w0=72.09999999999997, w1=16.090236097411882\n",
      "SubSGD iter. 321/499: loss=5.3386382249005, w0=72.36249999999997, w1=16.358628108681085\n",
      "SubSGD iter. 322/499: loss=5.327549497102938, w0=72.62499999999997, w1=15.849884592752073\n",
      "SubSGD iter. 323/499: loss=5.31152068677987, w0=72.97499999999997, w1=15.927001846463819\n",
      "SubSGD iter. 324/499: loss=5.318674561997411, w0=72.62499999999997, w1=15.938179006944685\n",
      "SubSGD iter. 325/499: loss=5.310819278558634, w0=72.97499999999997, w1=15.96816519206168\n",
      "SubSGD iter. 326/499: loss=5.31785668469795, w0=73.23749999999997, w1=16.372178897964872\n",
      "SubSGD iter. 327/499: loss=5.349180120635279, w0=73.41249999999997, w1=16.448011784523512\n",
      "SubSGD iter. 328/499: loss=5.373897042995681, w0=72.97499999999997, w1=16.004490740377925\n",
      "SubSGD iter. 329/499: loss=5.318413767624375, w0=72.88749999999996, w1=15.853102620720568\n",
      "SubSGD iter. 330/499: loss=5.315957806056783, w0=72.97499999999997, w1=15.913799676239522\n",
      "SubSGD iter. 331/499: loss=5.3190471187522474, w0=73.06249999999997, w1=16.245375920624266\n",
      "SubSGD iter. 332/499: loss=5.328899955547092, w0=73.06249999999997, w1=16.39074334383449\n",
      "SubSGD iter. 333/499: loss=5.337332879602877, w0=73.23749999999997, w1=16.27835221672539\n",
      "SubSGD iter. 334/499: loss=5.341890564243119, w0=73.14999999999996, w1=16.375027172810515\n",
      "SubSGD iter. 335/499: loss=5.342474869013216, w0=73.32499999999996, w1=16.28487612290337\n",
      "SubSGD iter. 336/499: loss=5.349320799874642, w0=73.32499999999996, w1=16.104383004544424\n",
      "SubSGD iter. 337/499: loss=5.343453200102163, w0=73.06249999999996, w1=16.042270414011636\n",
      "SubSGD iter. 338/499: loss=5.32363843903248, w0=73.23749999999995, w1=16.09397393974211\n",
      "SubSGD iter. 339/499: loss=5.336468752217863, w0=73.23749999999995, w1=16.30985610554037\n",
      "SubSGD iter. 340/499: loss=5.344247260822928, w0=73.14999999999995, w1=16.151209451596287\n",
      "SubSGD iter. 341/499: loss=5.330758073548756, w0=73.41249999999995, w1=16.207804679405353\n",
      "SubSGD iter. 342/499: loss=5.354884916097411, w0=73.06249999999996, w1=16.3696285330787\n",
      "SubSGD iter. 343/499: loss=5.335910517958882, w0=72.88749999999996, w1=16.261054529189163\n",
      "SubSGD iter. 344/499: loss=5.320648434959885, w0=72.79999999999995, w1=16.368337348533732\n",
      "SubSGD iter. 345/499: loss=5.319660747067511, w0=72.71249999999995, w1=16.56056212151322\n",
      "SubSGD iter. 346/499: loss=5.334628717199157, w0=72.88749999999995, w1=16.376789504410425\n",
      "SubSGD iter. 347/499: loss=5.3246327533292765, w0=72.71249999999995, w1=16.179196713598973\n",
      "SubSGD iter. 348/499: loss=5.312338115837957, w0=72.36249999999995, w1=16.138647464098185\n",
      "SubSGD iter. 349/499: loss=5.3198575967326205, w0=72.18749999999996, w1=16.33655950270185\n",
      "SubSGD iter. 350/499: loss=5.337942050812083, w0=72.44999999999996, w1=16.06963626005752\n",
      "SubSGD iter. 351/499: loss=5.316134315815954, w0=72.71249999999996, w1=16.196328859277415\n",
      "SubSGD iter. 352/499: loss=5.312644304256974, w0=72.88749999999996, w1=16.1818501234542\n",
      "SubSGD iter. 353/499: loss=5.318745053930892, w0=73.14999999999996, w1=16.24974118237589\n",
      "SubSGD iter. 354/499: loss=5.334122005713416, w0=73.23749999999997, w1=16.34252272158643\n",
      "SubSGD iter. 355/499: loss=5.346832829139597, w0=73.32499999999997, w1=16.04866752685374\n",
      "SubSGD iter. 356/499: loss=5.344076396291383, w0=73.23749999999997, w1=15.555272406067038\n",
      "SubSGD iter. 357/499: loss=5.345000625039572, w0=73.23749999999997, w1=15.93718764298827\n",
      "SubSGD iter. 358/499: loss=5.338181256879996, w0=73.14999999999996, w1=16.03925753053079\n",
      "SubSGD iter. 359/499: loss=5.330135701339293, w0=73.06249999999996, w1=16.20252515544362\n",
      "SubSGD iter. 360/499: loss=5.327774351484314, w0=73.14999999999996, w1=16.118942406406433\n",
      "SubSGD iter. 361/499: loss=5.329998988719963, w0=72.97499999999997, w1=16.16158177976583\n",
      "SubSGD iter. 362/499: loss=5.322367167567938, w0=72.79999999999997, w1=16.28955208292269\n",
      "SubSGD iter. 363/499: loss=5.317585222837732, w0=72.71249999999996, w1=16.28972460824686\n",
      "SubSGD iter. 364/499: loss=5.315353905553327, w0=72.53749999999997, w1=16.14463564679275\n",
      "SubSGD iter. 365/499: loss=5.313564691680489, w0=72.44999999999996, w1=16.291088880395566\n",
      "SubSGD iter. 366/499: loss=5.320262243589294, w0=72.27499999999996, w1=16.194696047204904\n",
      "SubSGD iter. 367/499: loss=5.326256292944649, w0=72.44999999999996, w1=16.17138540083417\n",
      "SubSGD iter. 368/499: loss=5.316514610611727, w0=72.44999999999996, w1=15.985848067485861\n",
      "SubSGD iter. 369/499: loss=5.315867705308214, w0=72.27499999999996, w1=15.516542364945565\n",
      "SubSGD iter. 370/499: loss=5.33733224616852, w0=72.44999999999996, w1=15.693352490594688\n",
      "SubSGD iter. 371/499: loss=5.320172116198909, w0=72.62499999999996, w1=15.90543749026051\n",
      "SubSGD iter. 372/499: loss=5.311079376201142, w0=72.53749999999995, w1=15.802992786554254\n",
      "SubSGD iter. 373/499: loss=5.31470589327644, w0=72.53749999999995, w1=15.787642586864793\n",
      "SubSGD iter. 374/499: loss=5.314960599920015, w0=72.62499999999996, w1=15.931101694761876\n",
      "SubSGD iter. 375/499: loss=5.3108755005178, w0=72.53749999999995, w1=15.959980413112717\n",
      "SubSGD iter. 376/499: loss=5.312858239319743, w0=72.44999999999995, w1=15.852735034910982\n",
      "SubSGD iter. 377/499: loss=5.31741010223061, w0=72.36249999999994, w1=15.742039977283643\n",
      "SubSGD iter. 378/499: loss=5.323919620272111, w0=72.53749999999994, w1=15.882137056188249\n",
      "SubSGD iter. 379/499: loss=5.313494856785562, w0=72.88749999999993, w1=15.870988356051587\n",
      "SubSGD iter. 380/499: loss=5.315319077984112, w0=72.88749999999993, w1=16.23981466672272\n",
      "SubSGD iter. 381/499: loss=5.320090506171814, w0=72.88749999999993, w1=16.41209963450123\n",
      "SubSGD iter. 382/499: loss=5.326642795239969, w0=72.79999999999993, w1=16.585900927181278\n",
      "SubSGD iter. 383/499: loss=5.339435292689927, w0=72.44999999999993, w1=16.38868733874662\n",
      "SubSGD iter. 384/499: loss=5.32453322974911, w0=72.36249999999993, w1=16.37063970274397\n",
      "SubSGD iter. 385/499: loss=5.328075134012237, w0=72.53749999999992, w1=16.146851667530335\n",
      "SubSGD iter. 386/499: loss=5.313592216751607, w0=72.97499999999992, w1=16.357488601501412\n",
      "SubSGD iter. 387/499: loss=5.329028376640578, w0=72.71249999999992, w1=16.253648454660713\n",
      "SubSGD iter. 388/499: loss=5.314208520365828, w0=72.97499999999992, w1=16.542688017219895\n",
      "SubSGD iter. 389/499: loss=5.342071759145302, w0=72.88749999999992, w1=16.580845883356545\n",
      "SubSGD iter. 390/499: loss=5.342096534294224, w0=72.97499999999992, w1=16.434778772457516\n",
      "SubSGD iter. 391/499: loss=5.334234891437097, w0=72.97499999999992, w1=16.43831568956723\n",
      "SubSGD iter. 392/499: loss=5.334473149562781, w0=72.97499999999992, w1=16.472746537080685\n",
      "SubSGD iter. 393/499: loss=5.3367925223215416, w0=72.88749999999992, w1=16.399307622783084\n",
      "SubSGD iter. 394/499: loss=5.325781084139269, w0=72.79999999999991, w1=16.381844689926535\n",
      "SubSGD iter. 395/499: loss=5.320256516362299, w0=72.7124999999999, w1=16.111934939122822\n",
      "SubSGD iter. 396/499: loss=5.3118191977372025, w0=72.7124999999999, w1=16.14204270358275\n",
      "SubSGD iter. 397/499: loss=5.311946901952495, w0=72.97499999999991, w1=16.2026808346115\n",
      "SubSGD iter. 398/499: loss=5.32344675769697, w0=72.97499999999991, w1=16.33857577518564\n",
      "SubSGD iter. 399/499: loss=5.327994562792034, w0=72.97499999999991, w1=16.097678171088667\n",
      "SubSGD iter. 400/499: loss=5.320688547332103, w0=72.97499999999991, w1=15.936942014764202\n",
      "SubSGD iter. 401/499: loss=5.318394056839423, w0=72.4499999999999, w1=16.076993401039356\n",
      "SubSGD iter. 402/499: loss=5.316161813664152, w0=72.3624999999999, w1=16.295566036100364\n",
      "SubSGD iter. 403/499: loss=5.324789850652043, w0=72.3624999999999, w1=16.289801347542838\n",
      "SubSGD iter. 404/499: loss=5.324537583295974, w0=72.6249999999999, w1=16.231434791272573\n",
      "SubSGD iter. 405/499: loss=5.312634509496256, w0=72.4499999999999, w1=16.051445382447575\n",
      "SubSGD iter. 406/499: loss=5.316066326091101, w0=72.2749999999999, w1=16.093429861823328\n",
      "SubSGD iter. 407/499: loss=5.324045578847817, w0=72.2749999999999, w1=15.783014161537187\n",
      "SubSGD iter. 408/499: loss=5.329103526881319, w0=72.09999999999991, w1=15.895927357986633\n",
      "SubSGD iter. 409/499: loss=5.3403032114262965, w0=72.4499999999999, w1=16.096213647112272\n",
      "SubSGD iter. 410/499: loss=5.31623365072832, w0=72.7124999999999, w1=16.052589676991474\n",
      "SubSGD iter. 411/499: loss=5.3116389492384855, w0=72.79999999999991, w1=16.326704912566328\n",
      "SubSGD iter. 412/499: loss=5.318403832132392, w0=72.88749999999992, w1=16.24745013002597\n",
      "SubSGD iter. 413/499: loss=5.32029107455091, w0=72.62499999999991, w1=16.203313243327543\n",
      "SubSGD iter. 414/499: loss=5.312220593256148, w0=72.88749999999992, w1=16.42653973969472\n",
      "SubSGD iter. 415/499: loss=5.327649471378065, w0=72.79999999999991, w1=16.49565363391891\n",
      "SubSGD iter. 416/499: loss=5.329594211129148, w0=72.79999999999991, w1=16.754257188709733\n",
      "SubSGD iter. 417/499: loss=5.3605973389008215, w0=72.97499999999991, w1=16.638427095806446\n",
      "SubSGD iter. 418/499: loss=5.354640887072623, w0=72.8874999999999, w1=16.752629501117948\n",
      "SubSGD iter. 419/499: loss=5.364870701958042, w0=72.8874999999999, w1=16.860941843812924\n",
      "SubSGD iter. 420/499: loss=5.379502836133719, w0=72.8874999999999, w1=16.688795871274245\n",
      "SubSGD iter. 421/499: loss=5.356247287363234, w0=72.7124999999999, w1=16.58987498288167\n",
      "SubSGD iter. 422/499: loss=5.337841384261778, w0=72.7124999999999, w1=16.393387687025267\n",
      "SubSGD iter. 423/499: loss=5.318935058954441, w0=72.53749999999991, w1=16.145403686299066\n",
      "SubSGD iter. 424/499: loss=5.313574231456754, w0=72.36249999999991, w1=16.10261883052937\n",
      "SubSGD iter. 425/499: loss=5.319722937100703, w0=72.62499999999991, w1=15.694945766059547\n",
      "SubSGD iter. 426/499: loss=5.3141473039837885, w0=72.71249999999992, w1=15.749836395986337\n",
      "SubSGD iter. 427/499: loss=5.312240346825095, w0=72.71249999999992, w1=15.676648609555878\n",
      "SubSGD iter. 428/499: loss=5.3159298670167345, w0=72.79999999999993, w1=15.670822615099175\n",
      "SubSGD iter. 429/499: loss=5.319035510224856, w0=72.53749999999992, w1=15.57807793417785\n",
      "SubSGD iter. 430/499: loss=5.320369114261696, w0=72.44999999999992, w1=15.388130750615975\n",
      "SubSGD iter. 431/499: loss=5.335359837291398, w0=72.53749999999992, w1=15.312532561942746\n",
      "SubSGD iter. 432/499: loss=5.338530091409135, w0=72.44999999999992, w1=15.483525086121807\n",
      "SubSGD iter. 433/499: loss=5.328448987333576, w0=72.44999999999992, w1=15.4686521867726\n",
      "SubSGD iter. 434/499: loss=5.329526455668516, w0=72.36249999999991, w1=15.360953700404012\n",
      "SubSGD iter. 435/499: loss=5.342542176243245, w0=72.36249999999991, w1=15.209351719968723\n",
      "SubSGD iter. 436/499: loss=5.3585669437688335, w0=72.44999999999992, w1=15.196987497992636\n",
      "SubSGD iter. 437/499: loss=5.3546985771175075, w0=72.53749999999992, w1=15.59261507489607\n",
      "SubSGD iter. 438/499: loss=5.319592950156502, w0=72.62499999999993, w1=15.651857471264993\n",
      "SubSGD iter. 439/499: loss=5.315964664277616, w0=72.53749999999992, w1=15.752289401426275\n",
      "SubSGD iter. 440/499: loss=5.315603987343388, w0=72.62499999999993, w1=15.622222128132181\n",
      "SubSGD iter. 441/499: loss=5.3176061480687355, w0=72.53749999999992, w1=15.434217939668116\n",
      "SubSGD iter. 442/499: loss=5.329669048987137, w0=72.71249999999992, w1=15.342589894463224\n",
      "SubSGD iter. 443/499: loss=5.3346045537711735, w0=72.62499999999991, w1=15.338932236304949\n",
      "SubSGD iter. 444/499: loss=5.3345350844830755, w0=72.79999999999991, w1=15.49270382016551\n",
      "SubSGD iter. 445/499: loss=5.32858143134542, w0=72.79999999999991, w1=15.7232657752296\n",
      "SubSGD iter. 446/499: loss=5.316707298038781, w0=72.88749999999992, w1=15.885388699267688\n",
      "SubSGD iter. 447/499: loss=5.314910211450646, w0=72.88749999999992, w1=15.631903872183724\n",
      "SubSGD iter. 448/499: loss=5.324228652935352, w0=72.88749999999992, w1=15.793747101288776\n",
      "SubSGD iter. 449/499: loss=5.31807748611619, w0=72.62499999999991, w1=15.625731613140461\n",
      "SubSGD iter. 450/499: loss=5.317411759809742, w0=72.79999999999991, w1=15.633134871106074\n",
      "SubSGD iter. 451/499: loss=5.3209068960264405, w0=72.7124999999999, w1=15.666614970603952\n",
      "SubSGD iter. 452/499: loss=5.316478203841095, w0=72.53749999999991, w1=15.754911076288636\n",
      "SubSGD iter. 453/499: loss=5.315556275868065, w0=72.7124999999999, w1=15.958140800654734\n",
      "SubSGD iter. 454/499: loss=5.311352081044568, w0=72.53749999999991, w1=16.010433861998983\n",
      "SubSGD iter. 455/499: loss=5.312603613398604, w0=72.7124999999999, w1=15.858840630545282\n",
      "SubSGD iter. 456/499: loss=5.311189976264657, w0=72.53749999999991, w1=15.844600492425561\n",
      "SubSGD iter. 457/499: loss=5.314069227471768, w0=72.7124999999999, w1=15.963925743326698\n",
      "SubSGD iter. 458/499: loss=5.311369651566532, w0=72.7124999999999, w1=15.79374576475439\n",
      "SubSGD iter. 459/499: loss=5.311658946978894, w0=72.79999999999991, w1=15.758627268065373\n",
      "SubSGD iter. 460/499: loss=5.3151374258508115, w0=72.79999999999991, w1=15.839125538302872\n",
      "SubSGD iter. 461/499: loss=5.312759907364465, w0=72.88749999999992, w1=15.662220281791313\n",
      "SubSGD iter. 462/499: loss=5.322882757033754, w0=72.62499999999991, w1=15.663452426429902\n",
      "SubSGD iter. 463/499: loss=5.315379463102381, w0=72.71249999999992, w1=15.843623600742111\n",
      "SubSGD iter. 464/499: loss=5.311299606129451, w0=72.62499999999991, w1=15.77770942481148\n",
      "SubSGD iter. 465/499: loss=5.3125423621746375, w0=72.53749999999991, w1=16.028839035205607\n",
      "SubSGD iter. 466/499: loss=5.312516486693492, w0=72.36249999999991, w1=16.072882796686415\n",
      "SubSGD iter. 467/499: loss=5.319632156173393, w0=72.09999999999991, w1=15.918763832077918\n",
      "SubSGD iter. 468/499: loss=5.3397648874624615, w0=71.92499999999991, w1=16.13115873150629\n",
      "SubSGD iter. 469/499: loss=5.359319252929895, w0=71.66249999999991, w1=16.07132491331921\n",
      "SubSGD iter. 470/499: loss=5.391932376315717, w0=71.66249999999991, w1=16.005922802733615\n",
      "SubSGD iter. 471/499: loss=5.390396819728814, w0=71.74999999999991, w1=15.66033015291423\n",
      "SubSGD iter. 472/499: loss=5.383321082357165, w0=71.83749999999992, w1=15.773584805131591\n",
      "SubSGD iter. 473/499: loss=5.365692642621103, w0=71.74999999999991, w1=15.824645059823888\n",
      "SubSGD iter. 474/499: loss=5.375497979336367, w0=71.74999999999991, w1=16.180673349051162\n",
      "SubSGD iter. 475/499: loss=5.383606319496431, w0=71.48749999999991, w1=16.215689254563515\n",
      "SubSGD iter. 476/499: loss=5.421575355315146, w0=71.31249999999991, w1=16.419394440567086\n",
      "SubSGD iter. 477/499: loss=5.457139706946199, w0=71.48749999999991, w1=16.383647863582617\n",
      "SubSGD iter. 478/499: loss=5.427474987434488, w0=71.74999999999991, w1=16.07052323716781\n",
      "SubSGD iter. 479/499: loss=5.380122474678139, w0=71.66249999999991, w1=16.07497692073475\n",
      "SubSGD iter. 480/499: loss=5.392024783794564, w0=71.74999999999991, w1=16.054947933633354\n",
      "SubSGD iter. 481/499: loss=5.379680268938588, w0=72.01249999999992, w1=15.951570429110799\n",
      "SubSGD iter. 482/499: loss=5.34629092647242, w0=72.01249999999992, w1=15.632976271449818\n",
      "SubSGD iter. 483/499: loss=5.355367681571702, w0=72.09999999999992, w1=15.79354965983156\n",
      "SubSGD iter. 484/499: loss=5.342716559829324, w0=72.18749999999993, w1=15.7280443551834\n",
      "SubSGD iter. 485/499: loss=5.337330022618289, w0=72.27499999999993, w1=15.496891783728426\n",
      "SubSGD iter. 486/499: loss=5.338222100367107, w0=72.27499999999993, w1=15.372844467750634\n",
      "SubSGD iter. 487/499: loss=5.347327026975419, w0=72.18749999999993, w1=15.57398230862706\n",
      "SubSGD iter. 488/499: loss=5.341785775548548, w0=72.18749999999993, w1=15.41130154018324\n",
      "SubSGD iter. 489/499: loss=5.35186860480314, w0=72.44999999999993, w1=15.625254930617116\n",
      "SubSGD iter. 490/499: loss=5.3217229584430665, w0=72.53749999999994, w1=15.788936766651787\n",
      "SubSGD iter. 491/499: loss=5.314937047332372, w0=72.44999999999993, w1=15.627878657247946\n",
      "SubSGD iter. 492/499: loss=5.321662994464523, w0=72.62499999999993, w1=15.442635619074927\n",
      "SubSGD iter. 493/499: loss=5.328016459705085, w0=72.88749999999993, w1=15.487298120665228\n",
      "SubSGD iter. 494/499: loss=5.331475861892065, w0=72.97499999999994, w1=15.57472108774851\n",
      "SubSGD iter. 495/499: loss=5.330232627104661, w0=72.97499999999994, w1=15.451812241532831\n",
      "SubSGD iter. 496/499: loss=5.33601416962571, w0=73.06249999999994, w1=15.434241610468924\n",
      "SubSGD iter. 497/499: loss=5.339934554922406, w0=73.06249999999994, w1=15.531122359937218\n",
      "SubSGD iter. 498/499: loss=5.336119599916902, w0=72.88749999999995, w1=15.633363624475697\n",
      "SubSGD iter. 499/499: loss=5.324163847285459, w0=72.88749999999995, w1=15.96663101175193\n",
      "SubSGD: execution time=0.882 seconds\n"
     ]
    }
   ],
   "source": [
    "# Define the parameters of the algorithm.\n",
    "max_iters = 500\n",
    "gamma = 0.7\n",
    "batch_size = 1\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.array([0, 0])\n",
    "\n",
    "# Start SubSGD.\n",
    "start_time = datetime.datetime.now()\n",
    "subsgd_losses, subsgd_ws = stochastic_subgradient_descent(\n",
    "    y, tx, w_initial, batch_size, max_iters, gamma)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"SubSGD: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19de7cec84d34f9f8b54948c3ba63774",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='n_iter', max=501, min=1), Output()), _dom_classes=('widg…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_figure(n_iter)>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ipywidgets import IntSlider, interact\n",
    "def plot_figure(n_iter):\n",
    "    fig = gradient_descent_visualization(\n",
    "        subsgd_losses, subsgd_ws, grid_losses, grid_w0, grid_w1, mean_x, std_x, height, weight, n_iter)\n",
    "    fig.set_size_inches(10.0, 6.0)\n",
    "\n",
    "interact(plot_figure, n_iter=IntSlider(min=1, max=len(subsgd_ws)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "vscode": {
   "interpreter": {
    "hash": "220387e6c3d14f2586cf2004f001028ce90f312409fe8a3fd0eb443ac44e4308"
   }
  },
  "widgets": {
   "state": {
    "d2b2c3aea192430e81437f33ba0b0e69": {
     "views": [
      {
       "cell_index": 22
      }
     ]
    },
    "e4a6a7a70ccd42ddb112989c04f2ed3f": {
     "views": [
      {
       "cell_index": 18
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

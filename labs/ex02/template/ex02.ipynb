{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from helpers import *\n",
    "\n",
    "height, weight, gender = load_data(sub_sample=False, add_outlier=False)\n",
    "x, mean_x, std_x = standardize(height)\n",
    "y, tx = build_model_data(x, weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10000,), (10000, 2))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape, tx.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NB: throughout this laboratory the data has the following format: \n",
    "  * there are **N = 10000** data entries\n",
    "  * **y** represents the column vector containing weight information -- that which we wish to predict/the output (see also the first page of $\\texttt{exercise02.pdf}$). Its **shape** is **(N,)**.\n",
    "  * **tx** represents the matrix $\\tilde{X}$ formed by laterally concatenating a column vector of 1s to the column vector of height information -- the input data (see also the first page of $\\texttt{exercise02.pdf}$). Its **shape** is **(N,2)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Computing the Cost Function\n",
    "Fill in the `compute_cost` function below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(y, tx, w):\n",
    "\n",
    "    \"\"\"Calculate the loss using either MSE\n",
    "\n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        w: numpy array of shape=(2,). The vector of model parameters.\n",
    "\n",
    "    Returns:\n",
    "        the value of the loss (a scalar), corresponding to the input parameters w.\n",
    "    \"\"\"\n",
    "    error = y - np.dot(tx, w)\n",
    "    return np.dot(error.T, error)/len(error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Grid Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill in the function `grid_search()` below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from costs import *\n",
    "\n",
    "def grid_search(y, tx, grid_w0, grid_w1):\n",
    "    \"\"\"Algorithm for grid search.\n",
    "        \n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        grid_w0: numpy array of shape=(num_grid_pts_w0, ). A 1D array containing num_grid_pts_w0 values of parameter w0 to be tested in the grid search.\n",
    "        grid_w1: numpy array of shape=(num_grid_pts_w1, ). A 1D array containing num_grid_pts_w1 values of parameter w1 to be tested in the grid search.\n",
    "        \n",
    "    Returns:\n",
    "        losses: numpy array of shape=(num_grid_pts_w0, num_grid_pts_w1). A 2D array containing the loss value for each combination of w0 and w1\n",
    "    \"\"\"\n",
    "\n",
    "    losses = np.zeros((len(grid_w0), len(grid_w1)))\n",
    "    for i in range(len(grid_w0)):\n",
    "        for j in range(len(grid_w1)):\n",
    "            losses[i,j] = compute_loss_mse(y, tx, np.array([grid_w0[i], grid_w1[j]]).T)\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us play with the grid search demo now!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid Search: loss*=15.558703368609525, w0*=72.72727272727272, w1*=13.636363636363626, execution time=2.125 seconds\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x432 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from grid_search import generate_w, get_best_parameters\n",
    "from plots import grid_visualization\n",
    "\n",
    "# Generate the grid of parameters to be swept\n",
    "grid_w0, grid_w1 = generate_w(num_intervals=100)\n",
    "\n",
    "# Start the grid search\n",
    "start_time = datetime.datetime.now()\n",
    "grid_losses = grid_search(y, tx, grid_w0, grid_w1)\n",
    "\n",
    "# Select the best combinaison\n",
    "loss_star, w0_star, w1_star = get_best_parameters(grid_w0, grid_w1, grid_losses)\n",
    "end_time = datetime.datetime.now()\n",
    "execution_time = (end_time - start_time).total_seconds()\n",
    "\n",
    "# Print the results\n",
    "print(\"Grid Search: loss*={l}, w0*={w0}, w1*={w1}, execution time={t:.3f} seconds\".format(\n",
    "      l=loss_star, w0=w0_star, w1=w1_star, t=execution_time))\n",
    "\n",
    "# Plot the results\n",
    "fig = grid_visualization(grid_losses, grid_w0, grid_w1, mean_x, std_x, height, weight)\n",
    "fig.set_size_inches(10.0,6.0)\n",
    "fig.savefig(\"grid_plot\")  # Optional saving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, please fill in the functions `compute_gradient` below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(y, tx, w):\n",
    "    \"\"\"Computes the gradient at w.\n",
    "        \n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        w: numpy array of shape=(2, ). The vector of model parameters.\n",
    "        \n",
    "    Returns:\n",
    "        An numpy array of shape (2, ) (same shape as w), containing the gradient of the loss at w.\n",
    "    \"\"\"\n",
    "    error = y - np.dot(tx, w)\n",
    "    return -np.dot(tx.T, error)/len(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please fill in the functions `gradient_descent` below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(y, tx, initial_w, max_iters, gamma):\n",
    "    \"\"\"The Gradient Descent (GD) algorithm.\n",
    "        \n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        initial_w: numpy array of shape=(2, ). The initial guess (or the initialization) for the model parameters\n",
    "        max_iters: a scalar denoting the total number of iterations of GD\n",
    "        gamma: a scalar denoting the stepsize\n",
    "        \n",
    "    Returns:\n",
    "        losses: a list of length max_iters containing the loss value (scalar) for each iteration of GD\n",
    "        ws: a list of length max_iters containing the model parameters as numpy arrays of shape (2, ), for each iteration of GD \n",
    "    \"\"\"\n",
    "    # Define parameters to store w and loss\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        loss = compute_loss(y, tx, w)\n",
    "        \n",
    "        # store w and loss\n",
    "        ws.append(w)\n",
    "        losses.append(loss)\n",
    "        \n",
    "        #update w by gradient\n",
    "        w = w - gamma*compute_gradient(y, tx, w)\n",
    "        \n",
    "        print(\"GD iter. {bi}/{ti}: loss={l}, w0={w0}, w1={w1}\".format(\n",
    "              bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]))\n",
    "\n",
    "    return losses, ws"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your gradient descent function through gradient descent demo shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GD iter. 0/49: loss=2792.236712759168, w0=51.3057454014736, w1=9.435798704492266\n",
      "GD iter. 1/49: loss=265.3024621089608, w0=66.69746902191571, w1=12.266538315839991\n",
      "GD iter. 2/49: loss=37.878379550441274, w0=71.31498610804834, w1=13.115760199244328\n",
      "GD iter. 3/49: loss=17.410212120174478, w0=72.70024123388814, w1=13.37052676426563\n",
      "GD iter. 4/49: loss=15.568077051450457, w0=73.11581777164007, w1=13.446956733772023\n",
      "GD iter. 5/49: loss=15.402284895265295, w0=73.24049073296565, w1=13.469885724623941\n",
      "GD iter. 6/49: loss=15.38736360120863, w0=73.27789262136334, w1=13.476764421879516\n",
      "GD iter. 7/49: loss=15.386020684743528, w0=73.28911318788263, w1=13.478828031056189\n",
      "GD iter. 8/49: loss=15.38589982226167, w0=73.29247935783842, w1=13.47944711380919\n",
      "GD iter. 9/49: loss=15.385888944638301, w0=73.29348920882516, w1=13.47963283863509\n",
      "GD iter. 10/49: loss=15.3858879656522, w0=73.29379216412119, w1=13.479688556082861\n",
      "GD iter. 11/49: loss=15.385887877543452, w0=73.29388305071, w1=13.479705271317192\n",
      "GD iter. 12/49: loss=15.385887869613665, w0=73.29391031668663, w1=13.479710285887492\n",
      "GD iter. 13/49: loss=15.385887868899983, w0=73.29391849647962, w1=13.479711790258582\n",
      "GD iter. 14/49: loss=15.385887868835754, w0=73.29392095041752, w1=13.479712241569908\n",
      "GD iter. 15/49: loss=15.385887868829974, w0=73.29392168659889, w1=13.479712376963306\n",
      "GD iter. 16/49: loss=15.385887868829453, w0=73.2939219074533, w1=13.479712417581325\n",
      "GD iter. 17/49: loss=15.385887868829409, w0=73.29392197370962, w1=13.479712429766732\n",
      "GD iter. 18/49: loss=15.385887868829403, w0=73.29392199358652, w1=13.479712433422353\n",
      "GD iter. 19/49: loss=15.385887868829407, w0=73.2939219995496, w1=13.47971243451904\n",
      "GD iter. 20/49: loss=15.385887868829403, w0=73.29392200133852, w1=13.479712434848047\n",
      "GD iter. 21/49: loss=15.385887868829403, w0=73.29392200187519, w1=13.479712434946748\n",
      "GD iter. 22/49: loss=15.385887868829398, w0=73.29392200203618, w1=13.479712434976358\n",
      "GD iter. 23/49: loss=15.385887868829398, w0=73.29392200208449, w1=13.479712434985242\n",
      "GD iter. 24/49: loss=15.385887868829398, w0=73.29392200209898, w1=13.479712434987906\n",
      "GD iter. 25/49: loss=15.385887868829403, w0=73.29392200210333, w1=13.479712434988706\n",
      "GD iter. 26/49: loss=15.3858878688294, w0=73.29392200210464, w1=13.479712434988945\n",
      "GD iter. 27/49: loss=15.3858878688294, w0=73.29392200210502, w1=13.479712434989018\n",
      "GD iter. 28/49: loss=15.385887868829398, w0=73.29392200210513, w1=13.47971243498904\n",
      "GD iter. 29/49: loss=15.385887868829398, w0=73.29392200210518, w1=13.479712434989047\n",
      "GD iter. 30/49: loss=15.385887868829391, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 31/49: loss=15.385887868829395, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 32/49: loss=15.385887868829395, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 33/49: loss=15.385887868829395, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 34/49: loss=15.385887868829395, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 35/49: loss=15.385887868829395, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 36/49: loss=15.385887868829395, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 37/49: loss=15.385887868829395, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 38/49: loss=15.385887868829395, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 39/49: loss=15.385887868829395, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 40/49: loss=15.385887868829395, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 41/49: loss=15.385887868829395, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 42/49: loss=15.385887868829395, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 43/49: loss=15.385887868829395, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 44/49: loss=15.385887868829395, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 45/49: loss=15.385887868829395, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 46/49: loss=15.385887868829395, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 47/49: loss=15.385887868829395, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 48/49: loss=15.385887868829395, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 49/49: loss=15.385887868829395, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD: execution time=0.094 seconds\n"
     ]
    }
   ],
   "source": [
    "# from gradient_descent import *\n",
    "from plots import gradient_descent_visualization\n",
    "\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters = 50\n",
    "gamma = 0.7\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.array([0, 0])\n",
    "\n",
    "# Start gradient descent.\n",
    "start_time = datetime.datetime.now()\n",
    "gd_losses, gd_ws = gradient_descent(y, tx, w_initial, max_iters, gamma)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"GD: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "054fcce3ad66488795f884932e928832",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='n_iter', max=51, min=1), Output()), _dom_classes=('widge"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_figure(n_iter)>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Time Visualization\n",
    "from ipywidgets import IntSlider, interact\n",
    "def plot_figure(n_iter):\n",
    "    fig = gradient_descent_visualization(\n",
    "        gd_losses, gd_ws, grid_losses, grid_w0, grid_w1, mean_x, std_x, height, weight, n_iter)\n",
    "    fig.set_size_inches(10.0, 6.0)\n",
    "\n",
    "interact(plot_figure, n_iter=IntSlider(min=1, max=len(gd_ws)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 4. Stochastic gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def compute_stoch_gradient(y, tx, w, n = random.randint(1, len(y))):\n",
    "    \"\"\"Compute a stochastic gradient at w from just few examples n and their corresponding y_n labels.\n",
    "        \n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        w: numpy array of shape=(2, ). The vector of model parameters.\n",
    "        \n",
    "    Returns:\n",
    "        A numpy array of shape (2, ) (same shape as w), containing the stochastic gradient of the loss at w.\n",
    "    \"\"\"\n",
    "    for i,j in batch_iter(y, tx, n):\n",
    "        y = i\n",
    "        tx = j\n",
    "     \n",
    "    return compute_gradient(y, tx, w)\n",
    "\n",
    "\n",
    "def stochastic_gradient_descent(y, tx, initial_w, batch_size, max_iters, gamma):\n",
    "    \"\"\"The Stochastic Gradient Descent algorithm (SGD).\n",
    "            \n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        initial_w: numpy array of shape=(2, ). The initial guess (or the initialization) for the model parameters\n",
    "        batch_size: a scalar denoting the number of data points in a mini-batch used for computing the stochastic gradient\n",
    "        max_iters: a scalar denoting the total number of iterations of SGD\n",
    "        gamma: a scalar denoting the stepsize\n",
    "        \n",
    "    Returns:\n",
    "        losses: a list of length max_iters containing the loss value (scalar) for each iteration of SGD\n",
    "        ws: a list of length max_iters containing the model parameters as numpy arrays of shape (2, ), for each iteration of SGD \n",
    "    \"\"\"\n",
    "    \n",
    "    # Define parameters to store w and loss\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    \n",
    "    for n_iter in range(max_iters):\n",
    "        loss = compute_loss(y, tx, w)\n",
    "        \n",
    "        # store w and loss\n",
    "        ws.append(w)\n",
    "        losses.append(loss)\n",
    "        \n",
    "        #update w by gradient\n",
    "        w = w - gamma*compute_stoch_gradient(y, tx, w, batch_size)\n",
    "\n",
    "        print(\"SGD iter. {bi}/{ti}: loss={l}, w0={w0}, w1={w1}\".format(\n",
    "              bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]))\n",
    "    return losses, ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD iter. 0/49: loss=2792.236712759168, w0=5.6571520616619155, w1=-6.1461263773865875\n",
      "SGD iter. 1/49: loss=2495.3389864017295, w0=13.806223421249761, w1=1.2537733855133348\n",
      "SGD iter. 2/49: loss=1859.5158219129305, w0=18.357106015870393, w1=-3.9861399417088057\n",
      "SGD iter. 3/49: loss=1676.9407628438412, w0=24.005097909703878, w1=-5.018224190850016\n",
      "SGD iter. 4/49: loss=1401.1668077814472, w0=27.446084287336763, w1=-8.295749464415358\n",
      "SGD iter. 5/49: loss=1303.4833698949053, w0=30.516016488641213, w1=-9.413232731542712\n",
      "SGD iter. 6/49: loss=1192.403957127169, w0=39.63962750345337, w1=15.427570938000533\n",
      "SGD iter. 7/49: loss=583.5887333436997, w0=42.15378820839348, w1=15.624089639350792\n",
      "SGD iter. 8/49: loss=502.5390310112555, w0=44.833387142127734, w1=13.301616680466447\n",
      "SGD iter. 9/49: loss=420.4027691757148, w0=47.109412561350844, w1=14.641643652568037\n",
      "SGD iter. 10/49: loss=358.87519727249844, w0=50.46490867900723, w1=19.143251739830934\n",
      "SGD iter. 11/49: loss=292.00565125066606, w0=53.6122984186927, w1=22.045827395486818\n",
      "SGD iter. 12/49: loss=245.75820406663073, w0=54.65422856290684, w1=23.437197718909736\n",
      "SGD iter. 13/49: loss=238.6807302122252, w0=58.639057635529134, w1=18.370727432924532\n",
      "SGD iter. 14/49: loss=134.7294265252147, w0=59.16593179359544, w1=18.478256258330777\n",
      "SGD iter. 15/49: loss=127.67866171163698, w0=61.0277394652163, w1=18.422407268460866\n",
      "SGD iter. 16/49: loss=102.83062099138299, w0=62.35435074559741, w1=19.107505548532156\n",
      "SGD iter. 17/49: loss=91.05902517135672, w0=63.766268757430716, w1=18.142377540355874\n",
      "SGD iter. 18/49: loss=71.64419898661016, w0=64.30533254204818, w1=18.480002837736777\n",
      "SGD iter. 19/49: loss=68.28471016545899, w0=65.24564239477431, w1=18.521780066268793\n",
      "SGD iter. 20/49: loss=60.484513186917965, w0=66.09312723804278, w1=17.788285076479685\n",
      "SGD iter. 21/49: loss=50.59350958940451, w0=67.00246395070617, w1=18.124083168485758\n",
      "SGD iter. 22/49: loss=45.96219983016656, w0=67.8470678854655, w1=18.683605427754895\n",
      "SGD iter. 23/49: loss=43.760248892885514, w0=68.00433419664652, w1=18.71246849255783\n",
      "SGD iter. 24/49: loss=43.06662542366933, w0=67.59863747606711, w1=18.949147616336766\n",
      "SGD iter. 25/49: loss=46.56138138657592, w0=67.40377125560292, w1=18.821937586858972\n",
      "SGD iter. 26/49: loss=47.00251056372589, w0=68.99249741530443, w1=19.811433778533857\n",
      "SGD iter. 27/49: loss=44.68236219294692, w0=69.69558439291625, w1=18.680516957137566\n",
      "SGD iter. 28/49: loss=35.38408848248146, w0=70.82079596704857, w1=18.243284743771238\n",
      "SGD iter. 29/49: loss=29.78987463196509, w0=71.44324559432721, w1=18.15264442673925\n",
      "SGD iter. 30/49: loss=28.016536151743676, w0=71.7470988790806, w1=18.378343452736512\n",
      "SGD iter. 31/49: loss=28.58051167980997, w0=70.63142428265192, w1=16.685891525121693\n",
      "SGD iter. 32/49: loss=24.070127100878235, w0=70.29712277701698, w1=16.14146755723338\n",
      "SGD iter. 33/49: loss=23.41876083197102, w0=70.12712580963779, w1=16.004706142181185\n",
      "SGD iter. 34/49: loss=23.587983541822364, w0=71.41416407514211, w1=17.014953607821028\n",
      "SGD iter. 35/49: loss=23.40159787586289, w0=72.72251170345416, w1=16.163924617917242\n",
      "SGD iter. 36/49: loss=19.151640255021704, w0=72.1366297181603, w1=15.988805797410464\n",
      "SGD iter. 37/49: loss=19.203325334742207, w0=73.14574663640744, w1=15.08736839579147\n",
      "SGD iter. 38/49: loss=16.68914468248101, w0=72.7185978980042, w1=14.937894590310314\n",
      "SGD iter. 39/49: loss=16.614534380257886, w0=73.26936239369495, w1=13.824805452894239\n",
      "SGD iter. 40/49: loss=15.445734051515494, w0=72.2281861339777, w1=13.868843840688106\n",
      "SGD iter. 41/49: loss=16.02949596458679, w0=71.55264034112636, w1=13.824223066299222\n",
      "SGD iter. 42/49: loss=16.96126256780285, w0=71.29592229096548, w1=14.034532327285115\n",
      "SGD iter. 43/49: loss=17.5358018481303, w0=72.2009025028729, w1=14.095445468677008\n",
      "SGD iter. 44/49: loss=16.172797266067686, w0=71.40106130128228, w1=12.86491984887774\n",
      "SGD iter. 45/49: loss=17.366333647158008, w0=71.11901643116508, w1=13.358638114646945\n",
      "SGD iter. 46/49: loss=17.758324485605716, w0=70.3871127283938, w1=13.373293825167655\n",
      "SGD iter. 47/49: loss=19.61632040595482, w0=71.59388316214294, w1=11.930345421115975\n",
      "SGD iter. 48/49: loss=18.031222969358467, w0=70.88579050662071, w1=10.510839209111694\n",
      "SGD iter. 49/49: loss=22.692540634267264, w0=72.63517720015523, w1=12.754179825649711\n",
      "SGD: execution time=0.151 seconds\n"
     ]
    }
   ],
   "source": [
    "# from stochastic_gradient_descent import *\n",
    "\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters = 50\n",
    "gamma = 0.1\n",
    "batch_size = 1\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.array([0, 0])\n",
    "\n",
    "# Start SGD.\n",
    "start_time = datetime.datetime.now()\n",
    "sgd_losses, sgd_ws = stochastic_gradient_descent(\n",
    "    y, tx, w_initial, batch_size, max_iters, gamma)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"SGD: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "537400942112414eb284e8b623cafd88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='n_iter', max=51, min=1), Output()), _dom_classes=('widge"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_figure(n_iter)>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Time Visualization\n",
    "from ipywidgets import IntSlider, interact\n",
    "def plot_figure(n_iter):\n",
    "    fig = gradient_descent_visualization(\n",
    "        sgd_losses, sgd_ws, grid_losses, grid_w0, grid_w1, mean_x, std_x, height, weight, n_iter)\n",
    "    fig.set_size_inches(10.0, 6.0)\n",
    "\n",
    "interact(plot_figure, n_iter=IntSlider(min=1, max=len(sgd_ws)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Effect of Outliers and MAE Cost Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from helpers import *\n",
    "\n",
    "height, weight, gender = load_data(sub_sample=True, add_outlier=True)\n",
    "x, mean_x, std_x = standardize(height)\n",
    "y, tx = build_model_data(x, weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((202,), (202, 2))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape, tx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD iter. 0/49: loss=2869.8351145358533, w0=49.6344298147335, w1=12.646294072764611\n",
      "SGD iter. 1/49: loss=365.72396616513697, w0=61.69550357304975, w1=5.3538341876094195\n",
      "SGD iter. 2/49: loss=158.604887195384, w0=63.44135488580108, w1=4.367585841432458\n",
      "SGD iter. 3/49: loss=144.6179650166825, w0=78.87748799765126, w1=30.131171206378358\n",
      "SGD iter. 4/49: loss=259.83113629388214, w0=82.94798959474687, w1=25.269603343859806\n",
      "SGD iter. 5/49: loss=206.67302445407185, w0=68.46945576248551, w1=13.018323855749244\n",
      "SGD iter. 6/49: loss=83.5684872600801, w0=71.26283291338689, w1=10.608971935272132\n",
      "SGD iter. 7/49: loss=69.95537187544296, w0=84.66909743179055, w1=33.22351502406822\n",
      "SGD iter. 8/49: loss=368.29185391120683, w0=202.70414001091686, w1=-433.90854594626404\n",
      "SGD iter. 9/49: loss=107326.91672357188, w0=360.00708436905165, w1=-312.34549302289656\n",
      "SGD iter. 10/49: loss=93234.00386427483, w0=396.76726800117933, w1=-274.27138541665306\n",
      "SGD iter. 11/49: loss=92833.23894920178, w0=65.98095059107004, w1=-111.15730298394101\n",
      "SGD iter. 12/49: loss=7564.095951830119, w0=15.28878419842399, w1=-80.32298551801836\n",
      "SGD iter. 13/49: loss=5966.548577679705, w0=-132.66778999364095, w1=505.22566110628287\n",
      "SGD iter. 14/49: loss=143547.99074411788, w0=280.6019875370517, w1=175.71757698775195\n",
      "SGD iter. 15/49: loss=34954.30772706402, w0=-58.93288236288191, w1=-403.7902738988895\n",
      "SGD iter. 16/49: loss=94950.482583713, w0=562.1769327097927, w1=695.854752553964\n",
      "SGD iter. 17/49: loss=353680.3093315021, w0=-346.38755010220075, w1=-385.77514702609153\n",
      "SGD iter. 18/49: loss=167186.38857981743, w0=187.98170339775106, w1=75.71538171968132\n",
      "SGD iter. 19/49: loss=8645.90144661652, w0=109.15062654073476, w1=71.51624039405303\n",
      "SGD iter. 20/49: loss=2510.3294621814275, w0=93.82489567246095, w1=77.64497810877916\n",
      "SGD iter. 21/49: loss=2479.55362393738, w0=46.465490445173444, w1=41.807373708926775\n",
      "SGD iter. 22/49: loss=920.3473651618676, w0=76.64575928308493, w1=31.41760426724619\n",
      "SGD iter. 23/49: loss=276.98107330952774, w0=77.60838057170477, w1=30.71832712264141\n",
      "SGD iter. 24/49: loss=265.917317466259, w0=58.86127382557638, w1=6.476306717804757\n",
      "SGD iter. 25/49: loss=191.9404012348103, w0=77.04333239163543, w1=22.671017510161146\n",
      "SGD iter. 26/49: loss=138.05728428304002, w0=65.86987177861285, w1=5.230450402100857\n",
      "SGD iter. 27/49: loss=116.37957942957607, w0=60.21028169098143, w1=14.803260569362353\n",
      "SGD iter. 28/49: loss=169.04650811694202, w0=65.58229998943011, w1=10.008226967434158\n",
      "SGD iter. 29/49: loss=102.45965848625066, w0=69.97445082648987, w1=3.4983429176919874\n",
      "SGD iter. 30/49: loss=102.70831543170708, w0=60.171834871917326, w1=14.8149552763305\n",
      "SGD iter. 31/49: loss=169.6241632358331, w0=72.23745237337816, w1=25.5617684194863\n",
      "SGD iter. 32/49: loss=173.1208546559593, w0=76.17110677324764, w1=22.358873917156096\n",
      "SGD iter. 33/49: loss=132.25891825474346, w0=82.74036845127785, w1=18.610098722684004\n",
      "SGD iter. 34/49: loss=132.2292578316635, w0=76.03788873928683, w1=12.556089395099365\n",
      "SGD iter. 35/49: loss=69.0283597859259, w0=75.5676847940398, w1=11.745984227267595\n",
      "SGD iter. 36/49: loss=67.30837255846315, w0=74.70818336335495, w1=12.449559921764621\n",
      "SGD iter. 37/49: loss=67.13641038927015, w0=73.21437695303042, w1=13.230438112101336\n",
      "SGD iter. 38/49: loss=68.70510562067356, w0=73.74748799442874, w1=13.572131094100316\n",
      "SGD iter. 39/49: loss=69.20081570710028, w0=79.70685069701214, w1=19.21772753612386\n",
      "SGD iter. 40/49: loss=115.30951872184292, w0=77.01264409752682, w1=20.663307313530865\n",
      "SGD iter. 41/49: loss=116.61992937013306, w0=76.06295958070346, w1=19.831665194239285\n",
      "SGD iter. 42/49: loss=106.61263340133593, w0=82.77356170786263, w1=12.743996256200619\n",
      "SGD iter. 43/49: loss=105.28633636908185, w0=75.2958291102432, w1=20.978294496393524\n",
      "SGD iter. 44/49: loss=116.12034876536629, w0=165.96922879829663, w1=-337.8681221774759\n",
      "SGD iter. 45/49: loss=65155.52415062691, w0=66.02458120615229, w1=-323.5507516844399\n",
      "SGD iter. 46/49: loss=56072.05490026223, w0=57.041137779724465, w1=-323.2260930498889\n",
      "SGD iter. 47/49: loss=56076.08846422354, w0=-8.147280186559911, w1=-300.7845224802189\n",
      "SGD iter. 48/49: loss=52061.26543357147, w0=-92.09591735032434, w1=-248.05417621791833\n",
      "SGD iter. 49/49: loss=47434.69556237925, w0=-225.62876470678088, w1=-68.06973729507632\n",
      "GD: execution time=0.046 seconds\n"
     ]
    }
   ],
   "source": [
    "from plots import gradient_descent_visualization\n",
    "\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters = 50\n",
    "gamma = 0.7\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.array([0, 0])\n",
    "\n",
    "# Start gradient descent.\n",
    "start_time = datetime.datetime.now()\n",
    "sgd_losses, sgd_ws = stochastic_gradient_descent(\n",
    "    y, tx, w_initial, batch_size, max_iters, gamma)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"GD: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c76a723196c840c3a9c4961b549d067e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='n_iter', max=51, min=1), Output()), _dom_classes=('widge"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_figure(n_iter)>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Time Visualization\n",
    "from ipywidgets import IntSlider, interact\n",
    "def plot_figure(n_iter):\n",
    "    fig = gradient_descent_visualization(\n",
    "        gd_losses, gd_ws, grid_losses, grid_w0, grid_w1, mean_x, std_x, height, weight, n_iter)\n",
    "    fig.set_size_inches(10.0, 6.0)\n",
    "\n",
    "interact(plot_figure, n_iter=IntSlider(min=1, max=len(gd_ws)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 6. Subgradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss_mae(y, tx, w):\n",
    "\n",
    "    \"\"\"Calculate the loss using MAE.\n",
    "\n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        w: numpy array of shape=(2,). The vector of model parameters.\n",
    "\n",
    "    Returns:\n",
    "        the value of the loss (a scalar), corresponding to the input parameters w.\n",
    "    \"\"\"\n",
    "    error = y - np.dot(tx, w)\n",
    "    return np.sum(np.abs(error))/len(error)\n",
    "\n",
    "def compute_subgradient_mae(y, tx, w):\n",
    "    \"\"\"Compute a subgradient of the MAE at w.\n",
    "        \n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        w: numpy array of shape=(2, ). The vector of model parameters.\n",
    "        \n",
    "    Returns:\n",
    "        A numpy array of shape (2, ) (same shape as w), containing the subgradient of the MAE at w.\n",
    "    \"\"\"\n",
    "    error = y - np.dot(tx, w)\n",
    "    return -np.dot(tx.T, np.sign(error))/len(error)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subgradient_descent(y, tx, initial_w, max_iters, gamma):\n",
    "    \"\"\"The SubGradient Descent (SubGD) algorithm.\n",
    "        \n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        initial_w: numpy array of shape=(2, ). The initial guess (or the initialization) for the model parameters\n",
    "        max_iters: a scalar denoting the total number of iterations of GD\n",
    "        gamma: a scalar denoting the stepsize\n",
    "        \n",
    "    Returns:\n",
    "        losses: a list of length max_iters containing the loss value (scalar) for each iteration of SubGD\n",
    "        ws: a list of length max_iters containing the model parameters as numpy arrays of shape (2, ), for each iteration of SubGD \n",
    "    \"\"\"\n",
    "    # Define parameters to store w and loss\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        loss = compute_loss(y, tx, w)\n",
    "        \n",
    "        # store w and loss\n",
    "        ws.append(w)\n",
    "        losses.append(loss)\n",
    "        \n",
    "        #update w by gradient\n",
    "        w = w - gamma*compute_subgradient_mae(y, tx, w)\n",
    "        \n",
    "        print(\"SubGD iter. {bi}/{ti}: loss={l}, w0={w0}, w1={w1}\".format(\n",
    "              bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]))\n",
    "\n",
    "    return losses, ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SubGD iter. 0/499: loss=2869.8351145358533, w0=0.7, w1=8.756471895211877e-16\n",
      "SubGD iter. 1/499: loss=2818.2326504374037, w0=1.4, w1=1.7512943790423754e-15\n",
      "SubGD iter. 2/499: loss=2767.1201863389556, w0=2.0999999999999996, w1=2.626941568563563e-15\n",
      "SubGD iter. 3/499: loss=2716.4977222405073, w0=2.8, w1=3.502588758084751e-15\n",
      "SubGD iter. 4/499: loss=2666.365258142059, w0=3.5, w1=4.378235947605939e-15\n",
      "SubGD iter. 5/499: loss=2616.722794043611, w0=4.2, w1=5.253883137127127e-15\n",
      "SubGD iter. 6/499: loss=2567.5703299451625, w0=4.9, w1=6.1295303266483146e-15\n",
      "SubGD iter. 7/499: loss=2518.9078658467133, w0=5.6000000000000005, w1=7.0051775161695025e-15\n",
      "SubGD iter. 8/499: loss=2470.7354017482653, w0=6.300000000000001, w1=7.88082470569069e-15\n",
      "SubGD iter. 9/499: loss=2423.0529376498166, w0=7.000000000000001, w1=8.756471895211878e-15\n",
      "SubGD iter. 10/499: loss=2375.8604735513677, w0=7.700000000000001, w1=9.632119084733065e-15\n",
      "SubGD iter. 11/499: loss=2329.1580094529195, w0=8.4, w1=1.0507766274254253e-14\n",
      "SubGD iter. 12/499: loss=2282.945545354471, w0=9.1, w1=1.1383413463775441e-14\n",
      "SubGD iter. 13/499: loss=2237.223081256023, w0=9.799999999999999, w1=1.2259060653296629e-14\n",
      "SubGD iter. 14/499: loss=2191.990617157574, w0=10.499999999999998, w1=1.3134707842817817e-14\n",
      "SubGD iter. 15/499: loss=2147.248153059126, w0=11.199999999999998, w1=1.4010355032339005e-14\n",
      "SubGD iter. 16/499: loss=2102.995688960677, w0=11.899999999999997, w1=1.488600222186019e-14\n",
      "SubGD iter. 17/499: loss=2059.233224862229, w0=12.599999999999996, w1=1.576164941138138e-14\n",
      "SubGD iter. 18/499: loss=2015.9607607637802, w0=13.299999999999995, w1=1.6637296600902567e-14\n",
      "SubGD iter. 19/499: loss=1973.178296665332, w0=13.999999999999995, w1=1.7512943790423755e-14\n",
      "SubGD iter. 20/499: loss=1930.8858325668837, w0=14.699999999999994, w1=1.8388590979944943e-14\n",
      "SubGD iter. 21/499: loss=1889.0833684684346, w0=15.399999999999993, w1=1.926423816946613e-14\n",
      "SubGD iter. 22/499: loss=1847.7709043699872, w0=16.099999999999994, w1=2.013988535898732e-14\n",
      "SubGD iter. 23/499: loss=1806.9484402715382, w0=16.799999999999994, w1=2.1015532548508507e-14\n",
      "SubGD iter. 24/499: loss=1766.6159761730898, w0=17.499999999999993, w1=2.1891179738029695e-14\n",
      "SubGD iter. 25/499: loss=1726.773512074641, w0=18.199999999999992, w1=2.2766826927550882e-14\n",
      "SubGD iter. 26/499: loss=1687.4210479761928, w0=18.89999999999999, w1=2.364247411707207e-14\n",
      "SubGD iter. 27/499: loss=1648.5585838777447, w0=19.59999999999999, w1=2.4518121306593258e-14\n",
      "SubGD iter. 28/499: loss=1610.1861197792962, w0=20.29999999999999, w1=2.5393768496114446e-14\n",
      "SubGD iter. 29/499: loss=1572.3036556808477, w0=20.99999999999999, w1=2.6269415685635634e-14\n",
      "SubGD iter. 30/499: loss=1534.9111915823992, w0=21.69999999999999, w1=2.7145062875156822e-14\n",
      "SubGD iter. 31/499: loss=1498.0087274839505, w0=22.399999999999988, w1=2.802071006467801e-14\n",
      "SubGD iter. 32/499: loss=1461.5962633855022, w0=23.099999999999987, w1=2.8896357254199195e-14\n",
      "SubGD iter. 33/499: loss=1425.6737992870535, w0=23.799999999999986, w1=2.977200444372038e-14\n",
      "SubGD iter. 34/499: loss=1390.2413351886055, w0=24.499999999999986, w1=3.064765163324157e-14\n",
      "SubGD iter. 35/499: loss=1355.298871090157, w0=25.199999999999985, w1=3.152329882276276e-14\n",
      "SubGD iter. 36/499: loss=1320.8464069917088, w0=25.899999999999984, w1=3.2398946012283946e-14\n",
      "SubGD iter. 37/499: loss=1286.8839428932602, w0=26.599999999999984, w1=3.3274593201805134e-14\n",
      "SubGD iter. 38/499: loss=1253.4114787948117, w0=27.299999999999983, w1=3.415024039132632e-14\n",
      "SubGD iter. 39/499: loss=1220.429014696363, w0=27.999999999999982, w1=3.502588758084751e-14\n",
      "SubGD iter. 40/499: loss=1187.9365505979144, w0=28.69999999999998, w1=3.59015347703687e-14\n",
      "SubGD iter. 41/499: loss=1155.934086499466, w0=29.39999999999998, w1=3.6777181959889886e-14\n",
      "SubGD iter. 42/499: loss=1124.4216224010179, w0=30.09999999999998, w1=3.7652829149411074e-14\n",
      "SubGD iter. 43/499: loss=1093.3991583025693, w0=30.79999999999998, w1=3.852847633893226e-14\n",
      "SubGD iter. 44/499: loss=1062.866694204121, w0=31.49999999999998, w1=3.940412352845345e-14\n",
      "SubGD iter. 45/499: loss=1032.8242301056723, w0=32.19999999999998, w1=4.027977071797464e-14\n",
      "SubGD iter. 46/499: loss=1003.2717660072236, w0=32.899999999999984, w1=4.1155417907495825e-14\n",
      "SubGD iter. 47/499: loss=974.2093019087752, w0=33.59999999999999, w1=4.2031065097017013e-14\n",
      "SubGD iter. 48/499: loss=945.6368378103265, w0=34.29999999999999, w1=4.29067122865382e-14\n",
      "SubGD iter. 49/499: loss=917.554373711878, w0=34.99999999999999, w1=4.378235947605939e-14\n",
      "SubGD iter. 50/499: loss=889.9619096134296, w0=35.699999999999996, w1=4.465800666558058e-14\n",
      "SubGD iter. 51/499: loss=862.8594455149808, w0=36.4, w1=4.5533653855101765e-14\n",
      "SubGD iter. 52/499: loss=836.2469814165323, w0=37.1, w1=4.640930104462295e-14\n",
      "SubGD iter. 53/499: loss=810.1245173180836, w0=37.800000000000004, w1=4.728494823414414e-14\n",
      "SubGD iter. 54/499: loss=784.492053219635, w0=38.50000000000001, w1=4.816059542366533e-14\n",
      "SubGD iter. 55/499: loss=759.3495891211867, w0=39.20000000000001, w1=4.9036242613186517e-14\n",
      "SubGD iter. 56/499: loss=734.6971250227377, w0=39.90000000000001, w1=4.9911889802707705e-14\n",
      "SubGD iter. 57/499: loss=710.5346609242893, w0=40.600000000000016, w1=5.078753699222889e-14\n",
      "SubGD iter. 58/499: loss=686.8621968258408, w0=41.30000000000002, w1=5.166318418175008e-14\n",
      "SubGD iter. 59/499: loss=663.6797327273923, w0=42.00000000000002, w1=5.253883137127127e-14\n",
      "SubGD iter. 60/499: loss=640.9872686289436, w0=42.700000000000024, w1=5.3414478560792456e-14\n",
      "SubGD iter. 61/499: loss=618.7848045304952, w0=43.40000000000003, w1=5.4290125750313644e-14\n",
      "SubGD iter. 62/499: loss=597.0723404320465, w0=44.10000000000003, w1=5.516577293983483e-14\n",
      "SubGD iter. 63/499: loss=575.849876333598, w0=44.80000000000003, w1=5.604142012935602e-14\n",
      "SubGD iter. 64/499: loss=555.1174122351495, w0=45.500000000000036, w1=5.691706731887721e-14\n",
      "SubGD iter. 65/499: loss=534.8749481367009, w0=46.20000000000004, w1=5.779271450839839e-14\n",
      "SubGD iter. 66/499: loss=515.1224840382524, w0=46.90000000000004, w1=5.866836169791957e-14\n",
      "SubGD iter. 67/499: loss=495.8600199398039, w0=47.59306930693074, w1=0.01114784567828894\n",
      "SubGD iter. 68/499: loss=477.1480669293974, w0=48.279207920792125, w1=0.03308574108991741\n",
      "SubGD iter. 69/499: loss=458.9765238171777, w0=48.96534653465351, w1=0.055023636501545875\n",
      "SubGD iter. 70/499: loss=441.2762481736448, w0=49.63069306930698, w1=0.1053832638830964\n",
      "SubGD iter. 71/499: loss=424.2440826814299, w0=50.28910891089114, w1=0.16746568532795278\n",
      "SubGD iter. 72/499: loss=407.6944527790815, w0=50.947524752475296, w1=0.22954810677280915\n",
      "SubGD iter. 73/499: loss=391.5821885242349, w0=51.59207920792084, w1=0.312425129327494\n",
      "SubGD iter. 74/499: loss=375.9955528848729, w0=52.22277227722777, w1=0.41195013288401805\n",
      "SubGD iter. 75/499: loss=360.95695350929566, w0=52.84653465346539, w1=0.5208167847923948\n",
      "SubGD iter. 76/499: loss=346.3748247543327, w0=53.4564356435644, w1=0.6457900912636185\n",
      "SubGD iter. 77/499: loss=332.3117701076253, w0=54.0594059405941, w1=0.7796904498577408\n",
      "SubGD iter. 78/499: loss=318.6833724768495, w0=54.655445544554496, w1=0.9197570104995888\n",
      "SubGD iter. 79/499: loss=305.5086034302326, w0=55.24455445544559, w1=1.067092029785011\n",
      "SubGD iter. 80/499: loss=292.7666734173504, w0=55.819801980198065, w1=1.2261255948210965\n",
      "SubGD iter. 81/499: loss=280.5315301161579, w0=56.36732673267331, w1=1.410709342622233\n",
      "SubGD iter. 82/499: loss=268.89668417535387, w0=56.900990099009945, w1=1.605853732220289\n",
      "SubGD iter. 83/499: loss=257.7339200525459, w0=57.42772277227727, w1=1.808762802293982\n",
      "SubGD iter. 84/499: loss=246.93766902970745, w0=57.933663366336674, w1=2.0285064197514897\n",
      "SubGD iter. 85/499: loss=236.64352344592228, w0=58.43267326732677, w1=2.2494370848672975\n",
      "SubGD iter. 86/499: loss=226.75154983044976, w0=58.91089108910895, w1=2.4837982986028537\n",
      "SubGD iter. 87/499: loss=217.35738896411289, w0=59.382178217821824, w1=2.7260245553531703\n",
      "SubGD iter. 88/499: loss=208.28322256993152, w0=59.83960396039608, w1=2.978742333469156\n",
      "SubGD iter. 89/499: loss=199.60239149197486, w0=60.262376237623805, w1=3.251528669355458\n",
      "SubGD iter. 90/499: loss=191.51606823720044, w0=60.67821782178222, w1=3.5270865794243\n",
      "SubGD iter. 91/499: loss=183.75485658516752, w0=61.087128712871326, w1=3.806459183951836\n",
      "SubGD iter. 92/499: loss=176.30486084041334, w0=61.49603960396043, w1=4.085831788479371\n",
      "SubGD iter. 93/499: loss=169.10012226467097, w0=61.891089108910926, w1=4.373839384328629\n",
      "SubGD iter. 94/499: loss=162.25177552382945, w0=62.27920792079211, w1=4.666037469532069\n",
      "SubGD iter. 95/499: loss=155.69742299714338, w0=62.65346534653469, w1=4.959829093241791\n",
      "SubGD iter. 96/499: loss=149.52752679496197, w0=63.02079207920796, w1=5.257057192056662\n",
      "SubGD iter. 97/499: loss=143.6406908762161, w0=63.38118811881192, w1=5.5604343163524295\n",
      "SubGD iter. 98/499: loss=138.01748857628547, w0=63.74158415841588, w1=5.863811440648197\n",
      "SubGD iter. 99/499: loss=132.61620926126304, w0=64.08811881188123, w1=6.172402175278572\n",
      "SubGD iter. 100/499: loss=127.54972442477155, w0=64.42772277227726, w1=6.4863693105165225\n",
      "SubGD iter. 101/499: loss=122.7408733871857, w0=64.7673267326733, w1=6.800336445754473\n",
      "SubGD iter. 102/499: loss=118.145928561526, w0=65.10693069306933, w1=7.1143035809924235\n",
      "SubGD iter. 103/499: loss=113.76488994779255, w0=65.44653465346536, w1=7.428270716230374\n",
      "SubGD iter. 104/499: loss=109.59775754598526, w0=65.76534653465349, w1=7.747893210218651\n",
      "SubGD iter. 105/499: loss=105.79833542751523, w0=66.070297029703, w1=8.073669686866932\n",
      "SubGD iter. 106/499: loss=102.29523108809978, w0=66.37524752475251, w1=8.399446163515213\n",
      "SubGD iter. 107/499: loss=98.99125186585265, w0=66.6663366336634, w1=8.73297028041742\n",
      "SubGD iter. 108/499: loss=95.97103181808454, w0=66.9574257425743, w1=9.066494397319628\n",
      "SubGD iter. 109/499: loss=93.14678297619835, w0=67.23465346534658, w1=9.398630319470323\n",
      "SubGD iter. 110/499: loss=90.61539672531049, w0=67.51188118811886, w1=9.730766241621017\n",
      "SubGD iter. 111/499: loss=88.27117995547901, w0=67.78910891089114, w1=10.062902163771712\n",
      "SubGD iter. 112/499: loss=86.11413266670394, w0=68.06633663366343, w1=10.363999289979459\n",
      "SubGD iter. 113/499: loss=84.16459694644125, w0=68.32970297029709, w1=10.66046690927365\n",
      "SubGD iter. 114/499: loss=82.46374060728381, w0=68.59306930693076, w1=10.943174379960851\n",
      "SubGD iter. 115/499: loss=80.92130656136143, w0=68.85643564356442, w1=11.225881850648053\n",
      "SubGD iter. 116/499: loss=79.52815785669323, w0=69.11287128712878, w1=11.504395843582245\n",
      "SubGD iter. 117/499: loss=78.31663397216155, w0=69.35544554455453, w1=11.78820189306779\n",
      "SubGD iter. 118/499: loss=77.31763568851034, w0=69.58415841584166, w1=12.06091146519101\n",
      "SubGD iter. 119/499: loss=76.50863231252772, w0=69.80594059405948, w1=12.324245668386087\n",
      "SubGD iter. 120/499: loss=75.84369059931622, w0=70.0277227722773, w1=12.587579871581164\n",
      "SubGD iter. 121/499: loss=75.29728112325216, w0=70.25643564356443, w1=12.824765405096523\n",
      "SubGD iter. 122/499: loss=74.79581982001427, w0=70.47821782178225, w1=13.065616959310187\n",
      "SubGD iter. 123/499: loss=74.43521733660026, w0=70.69306930693077, w1=13.302953389983951\n",
      "SubGD iter. 124/499: loss=74.19719822092485, w0=70.89405940594067, w1=13.525403099312957\n",
      "SubGD iter. 125/499: loss=74.06837899395498, w0=71.08811881188126, w1=13.742945617944251\n",
      "SubGD iter. 126/499: loss=74.03676697743124, w0=71.27524752475254, w1=13.953548196006883\n",
      "SubGD iter. 127/499: loss=74.08918974672692, w0=71.46237623762383, w1=14.164150774069515\n",
      "SubGD iter. 128/499: loss=74.22098311709009, w0=71.62178217821788, w1=14.349779559473214\n",
      "SubGD iter. 129/499: loss=74.41647628166025, w0=71.75346534653471, w1=14.516890107612351\n",
      "SubGD iter. 130/499: loss=74.67096152833813, w0=71.87128712871292, w1=14.670791185324227\n",
      "SubGD iter. 131/499: loss=74.95294838238388, w0=71.95445544554461, w1=14.780276456654562\n",
      "SubGD iter. 132/499: loss=75.17779670886826, w0=72.0376237623763, w1=14.889761727984897\n",
      "SubGD iter. 133/499: loss=75.4215490289155, w0=72.10693069306937, w1=14.985916181776767\n",
      "SubGD iter. 134/499: loss=75.65853052170146, w0=72.17623762376245, w1=15.082070635568638\n",
      "SubGD iter. 135/499: loss=75.90956114411352, w0=72.24554455445552, w1=15.178225089360508\n",
      "SubGD iter. 136/499: loss=76.17464089615169, w0=72.30099009900998, w1=15.25972348971595\n",
      "SubGD iter. 137/499: loss=76.41613751021141, w0=72.34950495049513, w1=15.335091856448178\n",
      "SubGD iter. 138/499: loss=76.65285618006462, w0=72.39801980198028, w1=15.410460223180406\n",
      "SubGD iter. 139/499: loss=76.89760893143634, w0=72.43267326732682, w1=15.469961786755766\n",
      "SubGD iter. 140/499: loss=77.10246868795771, w0=72.46039603960405, w1=15.51864528583285\n",
      "SubGD iter. 141/499: loss=77.27462217352513, w0=72.48811881188128, w1=15.561592159086528\n",
      "SubGD iter. 142/499: loss=77.42392987125342, w0=72.5019801980199, w1=15.597828332032567\n",
      "SubGD iter. 143/499: loss=77.56681600428637, w0=72.52277227722782, w1=15.624722856626754\n",
      "SubGD iter. 144/499: loss=77.6575549725318, w0=72.55049504950505, w1=15.642690329098041\n",
      "SubGD iter. 145/499: loss=77.69773565765118, w0=72.56435643564366, w1=15.664356578291132\n",
      "SubGD iter. 146/499: loss=77.77686805360935, w0=72.58514851485158, w1=15.677095775361325\n",
      "SubGD iter. 147/499: loss=77.80488113813033, w0=72.6059405940595, w1=15.689834972431518\n",
      "SubGD iter. 148/499: loss=77.8334888203511, w0=72.62673267326743, w1=15.70257416950171\n",
      "SubGD iter. 149/499: loss=77.86269110027165, w0=72.64059405940604, w1=15.724240418694801\n",
      "SubGD iter. 150/499: loss=77.9441777135799, w0=72.66138613861396, w1=15.736979615764994\n",
      "SubGD iter. 151/499: loss=77.97453880885702, w0=72.66831683168327, w1=15.74811029423132\n",
      "SubGD iter. 152/499: loss=78.01721470220257, w0=72.67524752475258, w1=15.759240972697647\n",
      "SubGD iter. 153/499: loss=78.06006252205766, w0=72.68217821782189, w1=15.770371651163973\n",
      "SubGD iter. 154/499: loss=78.1030822684223, w0=72.68217821782189, w1=15.774323911906727\n",
      "SubGD iter. 155/499: loss=78.12180591760107, w0=72.68217821782189, w1=15.77827617264948\n",
      "SubGD iter. 156/499: loss=78.14054518714481, w0=72.68217821782189, w1=15.782228433392234\n",
      "SubGD iter. 157/499: loss=78.15930007705352, w0=72.68217821782189, w1=15.786180694134988\n",
      "SubGD iter. 158/499: loss=78.17807058732721, w0=72.68217821782189, w1=15.790132954877741\n",
      "SubGD iter. 159/499: loss=78.19685671796589, w0=72.68217821782189, w1=15.794085215620495\n",
      "SubGD iter. 160/499: loss=78.21565846896952, w0=72.68217821782189, w1=15.798037476363248\n",
      "SubGD iter. 161/499: loss=78.23447584033816, w0=72.68217821782189, w1=15.801989737106002\n",
      "SubGD iter. 162/499: loss=78.25330883207177, w0=72.68217821782189, w1=15.805941997848755\n",
      "SubGD iter. 163/499: loss=78.27215744417036, w0=72.68217821782189, w1=15.809894258591509\n",
      "SubGD iter. 164/499: loss=78.29102167663393, w0=72.68217821782189, w1=15.813846519334263\n",
      "SubGD iter. 165/499: loss=78.30990152946246, w0=72.68217821782189, w1=15.817798780077016\n",
      "SubGD iter. 166/499: loss=78.32879700265599, w0=72.68217821782189, w1=15.82175104081977\n",
      "SubGD iter. 167/499: loss=78.34770809621449, w0=72.68217821782189, w1=15.825703301562523\n",
      "SubGD iter. 168/499: loss=78.36663481013797, w0=72.68217821782189, w1=15.829655562305277\n",
      "SubGD iter. 169/499: loss=78.38557714442643, w0=72.68217821782189, w1=15.83360782304803\n",
      "SubGD iter. 170/499: loss=78.40453509907987, w0=72.68217821782189, w1=15.837560083790784\n",
      "SubGD iter. 171/499: loss=78.42350867409829, w0=72.68217821782189, w1=15.841512344533538\n",
      "SubGD iter. 172/499: loss=78.44249786948167, w0=72.68217821782189, w1=15.845464605276291\n",
      "SubGD iter. 173/499: loss=78.46150268523004, w0=72.68217821782189, w1=15.849416866019045\n",
      "SubGD iter. 174/499: loss=78.48052312134341, w0=72.68217821782189, w1=15.853369126761798\n",
      "SubGD iter. 175/499: loss=78.49955917782175, w0=72.68217821782189, w1=15.857321387504552\n",
      "SubGD iter. 176/499: loss=78.51861085466504, w0=72.68217821782189, w1=15.861273648247305\n",
      "SubGD iter. 177/499: loss=78.53767815187334, w0=72.68217821782189, w1=15.865225908990059\n",
      "SubGD iter. 178/499: loss=78.55676106944661, w0=72.68217821782189, w1=15.869178169732812\n",
      "SubGD iter. 179/499: loss=78.57585960738484, w0=72.68217821782189, w1=15.873130430475566\n",
      "SubGD iter. 180/499: loss=78.59497376568808, w0=72.68217821782189, w1=15.87708269121832\n",
      "SubGD iter. 181/499: loss=78.6141035443563, w0=72.68217821782189, w1=15.881034951961073\n",
      "SubGD iter. 182/499: loss=78.63324894338946, w0=72.68217821782189, w1=15.884987212703827\n",
      "SubGD iter. 183/499: loss=78.65240996278762, w0=72.68217821782189, w1=15.88893947344658\n",
      "SubGD iter. 184/499: loss=78.67158660255075, w0=72.68217821782189, w1=15.892891734189334\n",
      "SubGD iter. 185/499: loss=78.69077886267888, w0=72.68217821782189, w1=15.896843994932087\n",
      "SubGD iter. 186/499: loss=78.70998674317197, w0=72.68217821782189, w1=15.900796255674841\n",
      "SubGD iter. 187/499: loss=78.72921024403006, w0=72.68217821782189, w1=15.904748516417595\n",
      "SubGD iter. 188/499: loss=78.7484493652531, w0=72.68217821782189, w1=15.908700777160348\n",
      "SubGD iter. 189/499: loss=78.76770410684115, w0=72.68217821782189, w1=15.912653037903102\n",
      "SubGD iter. 190/499: loss=78.78697446879414, w0=72.67524752475258, w1=15.910526938117364\n",
      "SubGD iter. 191/499: loss=78.78623350545445, w0=72.67524752475258, w1=15.914479198860118\n",
      "SubGD iter. 192/499: loss=78.80551108487171, w0=72.67524752475258, w1=15.918431459602871\n",
      "SubGD iter. 193/499: loss=78.82480428465396, w0=72.66831683168327, w1=15.916305359817134\n",
      "SubGD iter. 194/499: loss=78.82409907031953, w0=72.66831683168327, w1=15.920257620559887\n",
      "SubGD iter. 195/499: loss=78.84339948756606, w0=72.66831683168327, w1=15.924209881302641\n",
      "SubGD iter. 196/499: loss=78.86271552517753, w0=72.66831683168327, w1=15.928162142045394\n",
      "SubGD iter. 197/499: loss=78.88204718315401, w0=72.66138613861396, w1=15.926036042259657\n",
      "SubGD iter. 198/499: loss=78.88136931492416, w0=72.66138613861396, w1=15.92998830300241\n",
      "SubGD iter. 199/499: loss=78.90070819036488, w0=72.66138613861396, w1=15.933940563745164\n",
      "SubGD iter. 200/499: loss=78.9200626861706, w0=72.65445544554466, w1=15.931814463959427\n",
      "SubGD iter. 201/499: loss=78.91942056694602, w0=72.65445544554466, w1=15.93576672470218\n",
      "SubGD iter. 202/499: loss=78.93878228021597, w0=72.65445544554466, w1=15.939718985444934\n",
      "SubGD iter. 203/499: loss=78.95815961385092, w0=72.65445544554466, w1=15.943671246187687\n",
      "SubGD iter. 204/499: loss=78.97755256785085, w0=72.64752475247535, w1=15.94154514640195\n",
      "SubGD iter. 205/499: loss=78.97693779473084, w0=72.64752475247535, w1=15.945497407144703\n",
      "SubGD iter. 206/499: loss=78.99633796619504, w0=72.64752475247535, w1=15.949449667887457\n",
      "SubGD iter. 207/499: loss=79.0157537580242, w0=72.64059405940604, w1=15.94732356810172\n",
      "SubGD iter. 208/499: loss=79.01517473390946, w0=72.64059405940604, w1=15.951275828844473\n",
      "SubGD iter. 209/499: loss=79.0345977432029, w0=72.64059405940604, w1=15.955228089587226\n",
      "SubGD iter. 210/499: loss=79.0540363728613, w0=72.64059405940604, w1=15.95918035032998\n",
      "SubGD iter. 211/499: loss=79.07349062288469, w0=72.63366336633673, w1=15.957054250544243\n",
      "SubGD iter. 212/499: loss=79.07293894487452, w0=72.63366336633673, w1=15.961006511286996\n",
      "SubGD iter. 213/499: loss=79.09240041236217, w0=72.63366336633673, w1=15.96495877202975\n",
      "SubGD iter. 214/499: loss=79.11187750021479, w0=72.62673267326743, w1=15.962832672244012\n",
      "SubGD iter. 215/499: loss=79.1113615712099, w0=72.63366336633673, w1=15.967301372051416\n",
      "SubGD iter. 216/499: loss=79.12342941191532, w0=72.62673267326743, w1=15.965175272265679\n",
      "SubGD iter. 217/499: loss=79.12290850230904, w0=72.63366336633673, w1=15.969643972073083\n",
      "SubGD iter. 218/499: loss=79.13498681139073, w0=72.62673267326743, w1=15.967517872287345\n",
      "SubGD iter. 219/499: loss=79.13446092118305, w0=72.63366336633673, w1=15.97198657209475\n",
      "SubGD iter. 220/499: loss=79.14654969864098, w0=72.62673267326743, w1=15.969860472309012\n",
      "SubGD iter. 221/499: loss=79.1460188278319, w0=72.63366336633673, w1=15.974329172116416\n",
      "SubGD iter. 222/499: loss=79.15811807366613, w0=72.62673267326743, w1=15.972203072330679\n",
      "SubGD iter. 223/499: loss=79.15758222225561, w0=72.62673267326743, w1=15.970593411609592\n",
      "SubGD iter. 224/499: loss=79.14963612667178, w0=72.63366336633673, w1=15.975062111416996\n",
      "SubGD iter. 225/499: loss=79.16173864779171, w0=72.62673267326743, w1=15.972936011631258\n",
      "SubGD iter. 226/499: loss=79.16120123807913, w0=72.62673267326743, w1=15.971326350910171\n",
      "SubGD iter. 227/499: loss=79.1532539627117, w0=72.63366336633673, w1=15.975795050717576\n",
      "SubGD iter. 228/499: loss=79.16535975911734, w0=72.62673267326743, w1=15.973668950931838\n",
      "SubGD iter. 229/499: loss=79.16482079110266, w0=72.62673267326743, w1=15.972059290210751\n",
      "SubGD iter. 230/499: loss=79.15687233595162, w0=72.62673267326743, w1=15.970449629489664\n",
      "SubGD iter. 231/499: loss=79.14892647180822, w0=72.63366336633673, w1=15.974918329297068\n",
      "SubGD iter. 232/499: loss=79.16102835040903, w0=72.62673267326743, w1=15.972792229511331\n",
      "SubGD iter. 233/499: loss=79.16049124639157, w0=72.62673267326743, w1=15.971182568790244\n",
      "SubGD iter. 234/499: loss=79.15254420246457, w0=72.63366336633673, w1=15.975651268597648\n",
      "SubGD iter. 235/499: loss=79.16464935635108, w0=72.62673267326743, w1=15.97352516881191\n",
      "SubGD iter. 236/499: loss=79.16411069403154, w0=72.62673267326743, w1=15.971915508090824\n",
      "SubGD iter. 237/499: loss=79.15616247032092, w0=72.63366336633673, w1=15.976384207898228\n",
      "SubGD iter. 238/499: loss=79.16827089949315, w0=72.62673267326743, w1=15.97425810811249\n",
      "SubGD iter. 239/499: loss=79.16773067887151, w0=72.62673267326743, w1=15.972648447391403\n",
      "SubGD iter. 240/499: loss=79.15978127537731, w0=72.62673267326743, w1=15.971038786670317\n",
      "SubGD iter. 241/499: loss=79.15183446289073, w0=72.63366336633673, w1=15.97550748647772\n",
      "SubGD iter. 242/499: loss=79.1639389742581, w0=72.62673267326743, w1=15.973381386691983\n",
      "SubGD iter. 243/499: loss=79.16340061763371, w0=72.62673267326743, w1=15.971771725970896\n",
      "SubGD iter. 244/499: loss=79.15545262536354, w0=72.63366336633673, w1=15.9762404257783\n",
      "SubGD iter. 245/499: loss=79.16756041201661, w0=72.62673267326743, w1=15.974114325992563\n",
      "SubGD iter. 246/499: loss=79.16702049709012, w0=72.62673267326743, w1=15.972504665271476\n",
      "SubGD iter. 247/499: loss=79.15907132503634, w0=72.62673267326743, w1=15.970895004550389\n",
      "SubGD iter. 248/499: loss=79.1511247439902, w0=72.63366336633673, w1=15.975363704357793\n",
      "SubGD iter. 249/499: loss=79.16322861283845, w0=72.62673267326743, w1=15.973237604572056\n",
      "SubGD iter. 250/499: loss=79.16269056190916, w0=72.62673267326743, w1=15.971627943850969\n",
      "SubGD iter. 251/499: loss=79.15474280107942, w0=72.63366336633673, w1=15.976096643658373\n",
      "SubGD iter. 252/499: loss=79.1668499452134, w0=72.62673267326743, w1=15.973970543872635\n",
      "SubGD iter. 253/499: loss=79.16631033598202, w0=72.62673267326743, w1=15.972360883151548\n",
      "SubGD iter. 254/499: loss=79.15836139536867, w0=72.62673267326743, w1=15.970751222430462\n",
      "SubGD iter. 255/499: loss=79.15041504576297, w0=72.63366336633673, w1=15.975219922237866\n",
      "SubGD iter. 256/499: loss=79.16251827209209, w0=72.62673267326743, w1=15.973093822452128\n",
      "SubGD iter. 257/499: loss=79.16198052685793, w0=72.62673267326743, w1=15.971484161731041\n",
      "SubGD iter. 258/499: loss=79.15403299746863, w0=72.63366336633673, w1=15.975952861538445\n",
      "SubGD iter. 259/499: loss=79.16613949908347, w0=72.62673267326743, w1=15.973826761752708\n",
      "SubGD iter. 260/499: loss=79.16560019554723, w0=72.62673267326743, w1=15.972217101031621\n",
      "SubGD iter. 261/499: loss=79.1576514863743, w0=72.62673267326743, w1=15.970607440310534\n",
      "SubGD iter. 262/499: loss=79.14970536820901, w0=72.63366336633673, w1=15.975076140117938\n",
      "SubGD iter. 263/499: loss=79.16180795201902, w0=72.62673267326743, w1=15.9729500403322\n",
      "SubGD iter. 264/499: loss=79.16127051248002, w0=72.62673267326743, w1=15.971340379611114\n",
      "SubGD iter. 265/499: loss=79.15332321453113, w0=72.63366336633673, w1=15.975809079418518\n",
      "SubGD iter. 266/499: loss=79.16542907362681, w0=72.62673267326743, w1=15.97368297963278\n",
      "SubGD iter. 267/499: loss=79.16489007578572, w0=72.62673267326743, w1=15.972073318911693\n",
      "SubGD iter. 268/499: loss=79.15694159805324, w0=72.62673267326743, w1=15.970463658190607\n",
      "SubGD iter. 269/499: loss=79.1489957113284, w0=72.63366336633673, w1=15.97493235799801\n",
      "SubGD iter. 270/499: loss=79.16109765261925, w0=72.62673267326743, w1=15.972806258212273\n",
      "SubGD iter. 271/499: loss=79.16056051877537, w0=72.62673267326743, w1=15.971196597491186\n",
      "SubGD iter. 272/499: loss=79.15261345226692, w0=72.63366336633673, w1=15.97566529729859\n",
      "SubGD iter. 273/499: loss=79.16471866884348, w0=72.62673267326743, w1=15.973539197512853\n",
      "SubGD iter. 274/499: loss=79.16417997669751, w0=72.62673267326743, w1=15.971929536791766\n",
      "SubGD iter. 275/499: loss=79.15623173040547, w0=72.63366336633673, w1=15.97639823659917\n",
      "SubGD iter. 276/499: loss=79.16834022226774, w0=72.62673267326743, w1=15.974272136813433\n",
      "SubGD iter. 277/499: loss=79.16779997181969, w0=72.62673267326743, w1=15.972662476092346\n",
      "SubGD iter. 278/499: loss=79.15985054574404, w0=72.62673267326743, w1=15.971052815371259\n",
      "SubGD iter. 279/499: loss=79.15190371067601, w0=72.63366336633673, w1=15.975521515178663\n",
      "SubGD iter. 280/499: loss=79.16400828473344, w0=72.62673267326743, w1=15.973395415392925\n",
      "SubGD iter. 281/499: loss=79.16346989828261, w0=72.62673267326743, w1=15.971785754671838\n",
      "SubGD iter. 282/499: loss=79.15552188343099, w0=72.63366336633673, w1=15.976254454479243\n",
      "SubGD iter. 283/499: loss=79.16762973277415, w0=72.62673267326743, w1=15.974128354693505\n",
      "SubGD iter. 284/499: loss=79.16708978802122, w0=72.62673267326743, w1=15.972518693972418\n",
      "SubGD iter. 285/499: loss=79.15914059338598, w0=72.62673267326743, w1=15.970909033251331\n",
      "SubGD iter. 286/499: loss=79.1511939897584, w0=72.63366336633673, w1=15.975377733058735\n",
      "SubGD iter. 287/499: loss=79.16329792129669, w0=72.62673267326743, w1=15.973251633272998\n",
      "SubGD iter. 288/499: loss=79.162759840541, w0=72.62673267326743, w1=15.971641972551911\n",
      "SubGD iter. 289/499: loss=79.15481205712982, w0=72.63366336633673, w1=15.976110672359315\n",
      "SubGD iter. 290/499: loss=79.16691926395383, w0=72.62673267326743, w1=15.973984572573578\n",
      "SubGD iter. 291/499: loss=79.16637962489605, w0=72.62673267326743, w1=15.97237491185249\n",
      "SubGD iter. 292/499: loss=79.15843066170125, w0=72.62673267326743, w1=15.970765251131404\n",
      "SubGD iter. 293/499: loss=79.1504842895141, w0=72.63366336633673, w1=15.975233950938808\n",
      "SubGD iter. 294/499: loss=79.16258757853325, w0=72.62673267326743, w1=15.97310785115307\n",
      "SubGD iter. 295/499: loss=79.1620498034727, w0=72.62673267326743, w1=15.971498190431983\n",
      "SubGD iter. 296/499: loss=79.15410225150194, w0=72.63366336633673, w1=15.975966890239388\n",
      "SubGD iter. 297/499: loss=79.16620881580681, w0=72.62673267326743, w1=15.97384079045365\n",
      "SubGD iter. 298/499: loss=79.16566948244416, w0=72.62673267326743, w1=15.972231129732563\n",
      "SubGD iter. 299/499: loss=79.1577207506898, w0=72.62673267326743, w1=15.970621469011476\n",
      "SubGD iter. 300/499: loss=79.1497746099431, w0=72.63366336633673, w1=15.97509016881888\n",
      "SubGD iter. 301/499: loss=79.16187725644312, w0=72.62673267326743, w1=15.972964069033143\n",
      "SubGD iter. 302/499: loss=79.16133978707768, w0=72.62673267326743, w1=15.971354408312056\n",
      "SubGD iter. 303/499: loss=79.15339246654736, w0=72.63366336633673, w1=15.97582310811946\n",
      "SubGD iter. 304/499: loss=79.1654983883331, w0=72.62673267326743, w1=15.973697008333723\n",
      "SubGD iter. 305/499: loss=79.1649593606656, w0=72.62673267326743, w1=15.972087347612636\n",
      "SubGD iter. 306/499: loss=79.15701086035165, w0=72.62673267326743, w1=15.970477686891549\n",
      "SubGD iter. 307/499: loss=79.14906495104536, w0=72.63366336633673, w1=15.974946386698953\n",
      "SubGD iter. 308/499: loss=79.16116695502627, w0=72.62673267326743, w1=15.972820286913215\n",
      "SubGD iter. 309/499: loss=79.16062979135597, w0=72.62673267326743, w1=15.971210626192129\n",
      "SubGD iter. 310/499: loss=79.15268270226606, w0=72.63366336633673, w1=15.975679325999533\n",
      "SubGD iter. 311/499: loss=79.16478798153268, w0=72.62673267326743, w1=15.973553226213795\n",
      "SubGD iter. 312/499: loss=79.16424925956031, w0=72.62673267326743, w1=15.971943565492708\n",
      "SubGD iter. 313/499: loss=79.15630099068682, w0=72.63366336633673, w1=15.976412265300112\n",
      "SubGD iter. 314/499: loss=79.16840954523913, w0=72.62673267326743, w1=15.974286165514375\n",
      "SubGD iter. 315/499: loss=79.16786926496465, w0=72.62673267326743, w1=15.972676504793288\n",
      "SubGD iter. 316/499: loss=79.15991981630755, w0=72.62673267326743, w1=15.971066844072201\n",
      "SubGD iter. 317/499: loss=79.15197295865809, w0=72.63366336633673, w1=15.975535543879605\n",
      "SubGD iter. 318/499: loss=79.16407759540556, w0=72.62673267326743, w1=15.973409444093868\n",
      "SubGD iter. 319/499: loss=79.16353917912832, w0=72.62673267326743, w1=15.97179978337278\n",
      "SubGD iter. 320/499: loss=79.15559114169525, w0=72.63366336633673, w1=15.976268483180185\n",
      "SubGD iter. 321/499: loss=79.16769905372846, w0=72.62673267326743, w1=15.974142383394447\n",
      "SubGD iter. 322/499: loss=79.16715907914913, w0=72.62673267326743, w1=15.97253272267336\n",
      "SubGD iter. 323/499: loss=79.15920986193244, w0=72.62673267326743, w1=15.970923061952274\n",
      "SubGD iter. 324/499: loss=79.1512632357234, w0=72.63366336633673, w1=15.975391761759678\n",
      "SubGD iter. 325/499: loss=79.16336722995173, w0=72.62673267326743, w1=15.97326566197394\n",
      "SubGD iter. 326/499: loss=79.16282911936965, w0=72.62673267326743, w1=15.971656001252853\n",
      "SubGD iter. 327/499: loss=79.154881313377, w0=72.63366336633673, w1=15.976124701060257\n",
      "SubGD iter. 328/499: loss=79.16698858289105, w0=72.62673267326743, w1=15.97399860127452\n",
      "SubGD iter. 329/499: loss=79.16644891400686, w0=72.62673267326743, w1=15.972388940553433\n",
      "SubGD iter. 330/499: loss=79.15849992823063, w0=72.62673267326743, w1=15.970779279832346\n",
      "SubGD iter. 331/499: loss=79.15055353346202, w0=72.63366336633673, w1=15.97524797963975\n",
      "SubGD iter. 332/499: loss=79.16265688517124, w0=72.62673267326743, w1=15.973121879854013\n",
      "SubGD iter. 333/499: loss=79.16211908028427, w0=72.62673267326743, w1=15.971512219132926\n",
      "SubGD iter. 334/499: loss=79.15417150573205, w0=72.63366336633673, w1=15.97598091894033\n",
      "SubGD iter. 335/499: loss=79.16627813272696, w0=72.62673267326743, w1=15.973854819154592\n",
      "SubGD iter. 336/499: loss=79.16573876953791, w0=72.62673267326743, w1=15.972245158433505\n",
      "SubGD iter. 337/499: loss=79.1577900152021, w0=72.62673267326743, w1=15.970635497712419\n",
      "SubGD iter. 338/499: loss=79.14984385187392, w0=72.63366336633673, w1=15.975104197519823\n",
      "SubGD iter. 339/499: loss=79.161946561064, w0=72.62673267326743, w1=15.972978097734085\n",
      "SubGD iter. 340/499: loss=79.16140906187218, w0=72.62673267326743, w1=15.971368437012998\n",
      "SubGD iter. 341/499: loss=79.1534617187604, w0=72.63366336633673, w1=15.975837136820402\n",
      "SubGD iter. 342/499: loss=79.16556770323618, w0=72.62673267326743, w1=15.973711037034665\n",
      "SubGD iter. 343/499: loss=79.16502864574225, w0=72.62673267326743, w1=15.972101376313578\n",
      "SubGD iter. 344/499: loss=79.15708012284689, w0=72.62673267326743, w1=15.970491715592491\n",
      "SubGD iter. 345/499: loss=79.14913419095913, w0=72.63366336633673, w1=15.974960415399895\n",
      "SubGD iter. 346/499: loss=79.16123625763008, w0=72.62673267326743, w1=15.972834315614158\n",
      "SubGD iter. 347/499: loss=79.16069906413337, w0=72.62673267326743, w1=15.97122465489307\n",
      "SubGD iter. 348/499: loss=79.15275195246204, w0=72.63366336633673, w1=15.975693354700475\n",
      "SubGD iter. 349/499: loss=79.1648572944187, w0=72.62673267326743, w1=15.973567254914737\n",
      "SubGD iter. 350/499: loss=79.1643185426199, w0=72.62673267326743, w1=15.97195759419365\n",
      "SubGD iter. 351/499: loss=79.15637025116494, w0=72.63366336633673, w1=15.976426294001055\n",
      "SubGD iter. 352/499: loss=79.16847886840732, w0=72.62673267326743, w1=15.974300194215317\n",
      "SubGD iter. 353/499: loss=79.16793855830643, w0=72.62673267326743, w1=15.97269053349423\n",
      "SubGD iter. 354/499: loss=79.1599890870679, w0=72.62673267326743, w1=15.971080872773143\n",
      "SubGD iter. 355/499: loss=79.15204220683698, w0=72.63366336633673, w1=15.975549572580547\n",
      "SubGD iter. 356/499: loss=79.16414690627451, w0=72.62673267326743, w1=15.97342347279481\n",
      "SubGD iter. 357/499: loss=79.16360846017085, w0=72.62673267326743, w1=15.971813812073723\n",
      "SubGD iter. 358/499: loss=79.15566040015631, w0=72.63366336633673, w1=15.976282511881127\n",
      "SubGD iter. 359/499: loss=79.16776837487957, w0=72.62673267326743, w1=15.97415641209539\n",
      "SubGD iter. 360/499: loss=79.16722837047382, w0=72.62673267326743, w1=15.972546751374303\n",
      "SubGD iter. 361/499: loss=79.1592791306757, w0=72.62673267326743, w1=15.970937090653216\n",
      "SubGD iter. 362/499: loss=79.1513324818852, w0=72.63366336633673, w1=15.97540579046062\n",
      "SubGD iter. 363/499: loss=79.16343653880362, w0=72.62673267326743, w1=15.973279690674882\n",
      "SubGD iter. 364/499: loss=79.16289839839509, w0=72.62673267326743, w1=15.971670029953795\n",
      "SubGD iter. 365/499: loss=79.154950569821, w0=72.63366336633673, w1=15.9761387297612\n",
      "SubGD iter. 366/499: loss=79.1670579020251, w0=72.62673267326743, w1=15.974012629975462\n",
      "SubGD iter. 367/499: loss=79.16651820331448, w0=72.62673267326743, w1=15.972402969254375\n",
      "SubGD iter. 368/499: loss=79.1585691949568, w0=72.62673267326743, w1=15.970793308533288\n",
      "SubGD iter. 369/499: loss=79.15062277760674, w0=72.63366336633673, w1=15.975262008340692\n",
      "SubGD iter. 370/499: loss=79.16272619200602, w0=72.62673267326743, w1=15.973135908554955\n",
      "SubGD iter. 371/499: loss=79.16218835729262, w0=72.62673267326743, w1=15.971526247833868\n",
      "SubGD iter. 372/499: loss=79.15424076015896, w0=72.63366336633673, w1=15.975994947641272\n",
      "SubGD iter. 373/499: loss=79.16634744984394, w0=72.62673267326743, w1=15.973868847855535\n",
      "SubGD iter. 374/499: loss=79.16580805682847, w0=72.62673267326743, w1=15.972259187134448\n",
      "SubGD iter. 375/499: loss=79.1578592799112, w0=72.62673267326743, w1=15.97064952641336\n",
      "SubGD iter. 376/499: loss=79.14991309400158, w0=72.63366336633673, w1=15.975118226220765\n",
      "SubGD iter. 377/499: loss=79.16201586588173, w0=72.62673267326743, w1=15.972992126435027\n",
      "SubGD iter. 378/499: loss=79.16147833686347, w0=72.62673267326743, w1=15.97138246571394\n",
      "SubGD iter. 379/499: loss=79.15353097117023, w0=72.63366336633673, w1=15.975851165521345\n",
      "SubGD iter. 380/499: loss=79.16563701833608, w0=72.62673267326743, w1=15.973725065735607\n",
      "SubGD iter. 381/499: loss=79.16509793101574, w0=72.62673267326743, w1=15.97211540501452\n",
      "SubGD iter. 382/499: loss=79.1571493855389, w0=72.62673267326743, w1=15.970505744293433\n",
      "SubGD iter. 383/499: loss=79.1492034310697, w0=72.63366336633673, w1=15.974974444100837\n",
      "SubGD iter. 384/499: loss=79.16130556043072, w0=72.62673267326743, w1=15.9728483443151\n",
      "SubGD iter. 385/499: loss=79.1607683371076, w0=72.62673267326743, w1=15.971238683594013\n",
      "SubGD iter. 386/499: loss=79.1528212028548, w0=72.63366336633673, w1=15.975707383401417\n",
      "SubGD iter. 387/499: loss=79.16492660750151, w0=72.62673267326743, w1=15.97358128361568\n",
      "SubGD iter. 388/499: loss=79.1643878258763, w0=72.62673267326743, w1=15.971971622894593\n",
      "SubGD iter. 389/499: loss=79.1564395118399, w0=72.63366336633673, w1=15.976440322701997\n",
      "SubGD iter. 390/499: loss=79.16854819177233, w0=72.62673267326743, w1=15.97431422291626\n",
      "SubGD iter. 391/499: loss=79.16800785184503, w0=72.62673267326743, w1=15.972704562195172\n",
      "SubGD iter. 392/499: loss=79.16005835802503, w0=72.62673267326743, w1=15.971094901474086\n",
      "SubGD iter. 393/499: loss=79.15211145521265, w0=72.63366336633673, w1=15.97556360128149\n",
      "SubGD iter. 394/499: loss=79.16421621734024, w0=72.62673267326743, w1=15.973437501495752\n",
      "SubGD iter. 395/499: loss=79.16367774141015, w0=72.62673267326743, w1=15.971827840774665\n",
      "SubGD iter. 396/499: loss=79.1557296588142, w0=72.63366336633673, w1=15.97629654058207\n",
      "SubGD iter. 397/499: loss=79.1678376962275, w0=72.62673267326743, w1=15.974170440796332\n",
      "SubGD iter. 398/499: loss=79.16729766199533, w0=72.62673267326743, w1=15.972560780075245\n",
      "SubGD iter. 399/499: loss=79.15934839961577, w0=72.62673267326743, w1=15.970951119354158\n",
      "SubGD iter. 400/499: loss=79.15140172824383, w0=72.63366336633673, w1=15.975419819161562\n",
      "SubGD iter. 401/499: loss=79.16350584785228, w0=72.62673267326743, w1=15.973293719375825\n",
      "SubGD iter. 402/499: loss=79.16296767761733, w0=72.62673267326743, w1=15.971684058654738\n",
      "SubGD iter. 403/499: loss=79.15501982646178, w0=72.63366336633673, w1=15.976152758462142\n",
      "SubGD iter. 404/499: loss=79.16712722135595, w0=72.62673267326743, w1=15.974026658676404\n",
      "SubGD iter. 405/499: loss=79.16658749281892, w0=72.62673267326743, w1=15.972416997955317\n",
      "SubGD iter. 406/499: loss=79.15863846187979, w0=72.62673267326743, w1=15.97080733723423\n",
      "SubGD iter. 407/499: loss=79.15069202194827, w0=72.63366336633673, w1=15.975276037041635\n",
      "SubGD iter. 408/499: loss=79.16279549903761, w0=72.62673267326743, w1=15.973149937255897\n",
      "SubGD iter. 409/499: loss=79.1622576344978, w0=72.62673267326743, w1=15.97154027653481\n",
      "SubGD iter. 410/499: loss=79.1543100147827, w0=72.63366336633673, w1=15.976008976342214\n",
      "SubGD iter. 411/499: loss=79.16641676715773, w0=72.62673267326743, w1=15.973882876556477\n",
      "SubGD iter. 412/499: loss=79.16587734431582, w0=72.62673267326743, w1=15.97227321583539\n",
      "SubGD iter. 413/499: loss=79.15792854481711, w0=72.62673267326743, w1=15.970663555114303\n",
      "SubGD iter. 414/499: loss=79.14998233632603, w0=72.63366336633673, w1=15.975132254921707\n",
      "SubGD iter. 415/499: loss=79.16208517089622, w0=72.62673267326743, w1=15.97300615513597\n",
      "SubGD iter. 416/499: loss=79.16154761205156, w0=72.62673267326743, w1=15.971396494414883\n",
      "SubGD iter. 417/499: loss=79.15360022377686, w0=72.63366336633673, w1=15.975865194222287\n",
      "SubGD iter. 418/499: loss=79.16570633363277, w0=72.62673267326743, w1=15.97373909443655\n",
      "SubGD iter. 419/499: loss=79.16516721648603, w0=72.62673267326743, w1=15.972129433715462\n",
      "SubGD iter. 420/499: loss=79.15721864842774, w0=72.62673267326743, w1=15.970519772994376\n",
      "SubGD iter. 421/499: loss=79.14927267137709, w0=72.63366336633673, w1=15.97498847280178\n",
      "SubGD iter. 422/499: loss=79.16137486342815, w0=72.62673267326743, w1=15.972862373016042\n",
      "SubGD iter. 423/499: loss=79.16083761027862, w0=72.62673267326743, w1=15.971252712294955\n",
      "SubGD iter. 424/499: loss=79.15289045344437, w0=72.63366336633673, w1=15.97572141210236\n",
      "SubGD iter. 425/499: loss=79.16499592078114, w0=72.62673267326743, w1=15.973595312316622\n",
      "SubGD iter. 426/499: loss=79.1644571093295, w0=72.62673267326743, w1=15.971985651595535\n",
      "SubGD iter. 427/499: loss=79.15650877271166, w0=72.63366336633673, w1=15.97645435140294\n",
      "SubGD iter. 428/499: loss=79.16861751533413, w0=72.62673267326743, w1=15.974328251617202\n",
      "SubGD iter. 429/499: loss=79.16807714558041, w0=72.62673267326743, w1=15.972718590896115\n",
      "SubGD iter. 430/499: loss=79.16012762917896, w0=72.62673267326743, w1=15.971108930175028\n",
      "SubGD iter. 431/499: loss=79.15218070378513, w0=72.63366336633673, w1=15.975577629982432\n",
      "SubGD iter. 432/499: loss=79.16428552860279, w0=72.62673267326743, w1=15.973451530196694\n",
      "SubGD iter. 433/499: loss=79.1637470228463, w0=72.62673267326743, w1=15.971841869475607\n",
      "SubGD iter. 434/499: loss=79.15579891766887, w0=72.63366336633673, w1=15.976310569283012\n",
      "SubGD iter. 435/499: loss=79.16790701777224, w0=72.62673267326743, w1=15.974184469497274\n",
      "SubGD iter. 436/499: loss=79.16736695371365, w0=72.62673267326743, w1=15.972574808776187\n",
      "SubGD iter. 437/499: loss=79.15941766875261, w0=72.62673267326743, w1=15.9709651480551\n",
      "SubGD iter. 438/499: loss=79.15147097479922, w0=72.63366336633673, w1=15.975433847862504\n",
      "SubGD iter. 439/499: loss=79.16357515709774, w0=72.62673267326743, w1=15.973307748076767\n",
      "SubGD iter. 440/499: loss=79.16303695703638, w0=72.62673267326743, w1=15.97169808735568\n",
      "SubGD iter. 441/499: loss=79.1550890832994, w0=72.63366336633673, w1=15.976166787163084\n",
      "SubGD iter. 442/499: loss=79.16719654088362, w0=72.62673267326743, w1=15.974040687377347\n",
      "SubGD iter. 443/499: loss=79.16665678252016, w0=72.62673267326743, w1=15.97243102665626\n",
      "SubGD iter. 444/499: loss=79.15870772899957, w0=72.62673267326743, w1=15.970821365935173\n",
      "SubGD iter. 445/499: loss=79.15076126648661, w0=72.63366336633673, w1=15.975290065742577\n",
      "SubGD iter. 446/499: loss=79.16286480626599, w0=72.62673267326743, w1=15.97316396595684\n",
      "SubGD iter. 447/499: loss=79.16232691189977, w0=72.62673267326743, w1=15.971554305235752\n",
      "SubGD iter. 448/499: loss=79.15437926960321, w0=72.63366336633673, w1=15.976023005043157\n",
      "SubGD iter. 449/499: loss=79.16648608466829, w0=72.62673267326743, w1=15.97389690525742\n",
      "SubGD iter. 450/499: loss=79.16594663199997, w0=72.62673267326743, w1=15.972287244536332\n",
      "SubGD iter. 451/499: loss=79.15799780991983, w0=72.62673267326743, w1=15.970677583815245\n",
      "SubGD iter. 452/499: loss=79.1500515788473, w0=72.63366336633673, w1=15.97514628362265\n",
      "SubGD iter. 453/499: loss=79.16215447610753, w0=72.62673267326743, w1=15.973020183836912\n",
      "SubGD iter. 454/499: loss=79.16161688743645, w0=72.62673267326743, w1=15.971410523115825\n",
      "SubGD iter. 455/499: loss=79.15366947658032, w0=72.63366336633673, w1=15.97587922292323\n",
      "SubGD iter. 456/499: loss=79.16577564912629, w0=72.62673267326743, w1=15.973753123137492\n",
      "SubGD iter. 457/499: loss=79.16523650215309, w0=72.62673267326743, w1=15.972143462416405\n",
      "SubGD iter. 458/499: loss=79.15728791151336, w0=72.62673267326743, w1=15.970533801695318\n",
      "SubGD iter. 459/499: loss=79.14934191188128, w0=72.63366336633673, w1=15.975002501502722\n",
      "SubGD iter. 460/499: loss=79.16144416662239, w0=72.62673267326743, w1=15.972876401716984\n",
      "SubGD iter. 461/499: loss=79.16090688364643, w0=72.62673267326743, w1=15.971266740995897\n",
      "SubGD iter. 462/499: loss=79.15295970423074, w0=72.63366336633673, w1=15.975735440803302\n",
      "SubGD iter. 463/499: loss=79.16506523425757, w0=72.62673267326743, w1=15.973609341017564\n",
      "SubGD iter. 464/499: loss=79.16452639297951, w0=72.62673267326743, w1=15.971999680296477\n",
      "SubGD iter. 465/499: loss=79.1565780337802, w0=72.63366336633673, w1=15.976468380103881\n",
      "SubGD iter. 466/499: loss=79.16868683909274, w0=72.62673267326743, w1=15.974342280318144\n",
      "SubGD iter. 467/499: loss=79.16814643951261, w0=72.62673267326743, w1=15.972732619597057\n",
      "SubGD iter. 468/499: loss=79.16019690052971, w0=72.62673267326743, w1=15.97112295887597\n",
      "SubGD iter. 469/499: loss=79.15224995255444, w0=72.63366336633673, w1=15.975591658683374\n",
      "SubGD iter. 470/499: loss=79.16435484006213, w0=72.62673267326743, w1=15.973465558897637\n",
      "SubGD iter. 471/499: loss=79.16381630447923, w0=72.62673267326743, w1=15.97185589817655\n",
      "SubGD iter. 472/499: loss=79.15586817672035, w0=72.63366336633673, w1=15.976324597983954\n",
      "SubGD iter. 473/499: loss=79.16797633951374, w0=72.62673267326743, w1=15.974198498198216\n",
      "SubGD iter. 474/499: loss=79.16743624562874, w0=72.62673267326743, w1=15.97258883747713\n",
      "SubGD iter. 475/499: loss=79.15948693808629, w0=72.62673267326743, w1=15.970979176756043\n",
      "SubGD iter. 476/499: loss=79.15154022155146, w0=72.63366336633673, w1=15.975447876563447\n",
      "SubGD iter. 477/499: loss=79.16364446654, w0=72.62673267326743, w1=15.97332177677771\n",
      "SubGD iter. 478/499: loss=79.16310623665223, w0=72.62673267326743, w1=15.971712116056622\n",
      "SubGD iter. 479/499: loss=79.1551583403338, w0=72.63366336633673, w1=15.976180815864026\n",
      "SubGD iter. 480/499: loss=79.16726586060807, w0=72.62673267326743, w1=15.974054716078289\n",
      "SubGD iter. 481/499: loss=79.1667260724182, w0=72.62673267326743, w1=15.972445055357202\n",
      "SubGD iter. 482/499: loss=79.15877699631615, w0=72.62673267326743, w1=15.970835394636115\n",
      "SubGD iter. 483/499: loss=79.15083051122176, w0=72.63366336633673, w1=15.97530409444352\n",
      "SubGD iter. 484/499: loss=79.16293411369118, w0=72.62673267326743, w1=15.973177994657782\n",
      "SubGD iter. 485/499: loss=79.16239618949854, w0=72.62673267326743, w1=15.971568333936695\n",
      "SubGD iter. 486/499: loss=79.15444852462053, w0=72.63366336633673, w1=15.976037033744099\n",
      "SubGD iter. 487/499: loss=79.16655540237568, w0=72.62673267326743, w1=15.973910933958361\n",
      "SubGD iter. 488/499: loss=79.16601591988095, w0=72.62673267326743, w1=15.972301273237274\n",
      "SubGD iter. 489/499: loss=79.15806707521934, w0=72.62673267326743, w1=15.970691612516188\n",
      "SubGD iter. 490/499: loss=79.15012082156535, w0=72.63366336633673, w1=15.975160312323592\n",
      "SubGD iter. 491/499: loss=79.16222378151568, w0=72.62673267326743, w1=15.973034212537854\n",
      "SubGD iter. 492/499: loss=79.16168616301816, w0=72.62673267326743, w1=15.971424551816767\n",
      "SubGD iter. 493/499: loss=79.15373872958057, w0=72.63366336633673, w1=15.975893251624171\n",
      "SubGD iter. 494/499: loss=79.16584496481659, w0=72.62673267326743, w1=15.973767151838434\n",
      "SubGD iter. 495/499: loss=79.165305788017, w0=72.62673267326743, w1=15.972157491117347\n",
      "SubGD iter. 496/499: loss=79.15735717479579, w0=72.62673267326743, w1=15.97054783039626\n",
      "SubGD iter. 497/499: loss=79.14941115258226, w0=72.63366336633673, w1=15.975016530203664\n",
      "SubGD iter. 498/499: loss=79.16151347001343, w0=72.62673267326743, w1=15.972890430417927\n",
      "SubGD iter. 499/499: loss=79.16097615721105, w0=72.62673267326743, w1=15.97128076969684\n",
      "SubGD: execution time=0.341 seconds\n"
     ]
    }
   ],
   "source": [
    "# Define the parameters of the algorithm.\n",
    "max_iters = 500\n",
    "gamma = 0.7\n",
    "batch_size = 1\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.array([0, 0])\n",
    "\n",
    "# Start SubSGD.\n",
    "start_time = datetime.datetime.now()\n",
    "subgd_losses, subgd_ws = subgradient_descent(\n",
    "    y, tx, w_initial, max_iters, gamma)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"SubGD: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8047c26cde44441ead48a5e3acee1fef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='n_iter', max=501, min=1), Output()), _dom_classes=('widg"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_figure(n_iter)>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ipywidgets import IntSlider, interact\n",
    "def plot_figure(n_iter):\n",
    "    fig = gradient_descent_visualization(\n",
    "        subgd_losses, subgd_ws, grid_losses, grid_w0, grid_w1, mean_x, std_x, height, weight, n_iter)\n",
    "    fig.set_size_inches(10.0, 6.0)\n",
    "\n",
    "interact(plot_figure, n_iter=IntSlider(min=1, max=len(subgd_ws)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic Subgradient Descent\n",
    "\n",
    "**NB** for the computation of the subgradient you can reuse the `compute_subgradient` method that you implemented above, just making sure that you pass in a minibatch as opposed to the full data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_stoch_subgradient_mae(y, tx, w, n = random.randint(1, len(y))):\n",
    "    \"\"\"Compute a stochastic gradient at w from just few examples n and their corresponding y_n labels.\n",
    "        \n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        w: numpy array of shape=(2, ). The vector of model parameters.\n",
    "        \n",
    "    Returns:\n",
    "        A numpy array of shape (2, ) (same shape as w), containing the stochastic gradient of the loss at w.\n",
    "    \"\"\"\n",
    "    for i,j in batch_iter(y, tx, n):\n",
    "        y = i\n",
    "        tx = j\n",
    "     \n",
    "    return compute_subgradient_mae(y, tx, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stochastic_subgradient_descent(y, tx, initial_w, batch_size, max_iters, gamma):\n",
    "    \"\"\"The Stochastic SubGradient Descent algorithm (SubSGD).\n",
    "            \n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        initial_w: numpy array of shape=(2, ). The initial guess (or the initialization) for the model parameters\n",
    "        batch_size: a scalar denoting the number of data points in a mini-batch used for computing the stochastic subgradient\n",
    "        max_iters: a scalar denoting the total number of iterations of SubSGD\n",
    "        gamma: a scalar denoting the stepsize\n",
    "        \n",
    "    Returns:\n",
    "        losses: a list of length max_iters containing the loss value (scalar) for each iteration of SubSGD\n",
    "        ws: a list of length max_iters containing the model parameters as numpy arrays of shape (2, ), for each iteration of SubSGD \n",
    "    \"\"\"\n",
    "    \n",
    "    # Define parameters to store w and loss\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    \n",
    "    for n_iter in range(max_iters):\n",
    "        loss = compute_loss_mae(y, tx, w)\n",
    "        \n",
    "        # store w and loss\n",
    "        ws.append(w)\n",
    "        losses.append(loss)\n",
    "        \n",
    "        #update w by gradient\n",
    "        w = w - gamma*compute_stoch_subgradient_mae(y, tx, w)\n",
    "        \n",
    "        print(\"SubSGD iter. {bi}/{ti}: loss={l}, w0={w0}, w1={w1}\".format(\n",
    "              bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]))\n",
    "    return losses, ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SubSGD iter. 0/499: loss=74.06780585492638, w0=0.7, w1=-0.00059174628149748\n",
      "SubSGD iter. 1/499: loss=73.36780585492637, w0=1.4, w1=0.009882160628060771\n",
      "SubSGD iter. 2/499: loss=72.66780585492639, w0=2.0999999999999996, w1=0.012786726223826133\n",
      "SubSGD iter. 3/499: loss=71.96780585492638, w0=2.8, w1=0.006734344735497178\n",
      "SubSGD iter. 4/499: loss=71.26780585492638, w0=3.5, w1=0.004122321453722827\n",
      "SubSGD iter. 5/499: loss=70.56780585492638, w0=4.2, w1=-0.003130452084377155\n",
      "SubSGD iter. 6/499: loss=69.86780585492637, w0=4.9, w1=-0.0058745054660320544\n",
      "SubSGD iter. 7/499: loss=69.16780585492639, w0=5.6000000000000005, w1=0.02019676039950603\n",
      "SubSGD iter. 8/499: loss=68.46780585492638, w0=6.300000000000001, w1=0.02955114814322502\n",
      "SubSGD iter. 9/499: loss=67.76780585492638, w0=7.000000000000001, w1=0.06641999794963846\n",
      "SubSGD iter. 10/499: loss=67.06780585492638, w0=7.700000000000001, w1=0.05190698963549237\n",
      "SubSGD iter. 11/499: loss=66.36780585492637, w0=8.4, w1=0.03485577190264758\n",
      "SubSGD iter. 12/499: loss=65.66780585492639, w0=9.1, w1=0.03137376196911982\n",
      "SubSGD iter. 13/499: loss=64.96780585492638, w0=9.799999999999999, w1=0.030580003522850525\n",
      "SubSGD iter. 14/499: loss=64.26780585492638, w0=10.499999999999998, w1=0.029698017488141402\n",
      "SubSGD iter. 15/499: loss=63.567805854926384, w0=11.199999999999998, w1=0.033896754213182154\n",
      "SubSGD iter. 16/499: loss=62.867805854926374, w0=11.899999999999997, w1=0.02136700820198263\n",
      "SubSGD iter. 17/499: loss=62.16780585492638, w0=12.599999999999996, w1=0.03188302690585425\n",
      "SubSGD iter. 18/499: loss=61.46780585492638, w0=13.299999999999995, w1=0.026081370889511915\n",
      "SubSGD iter. 19/499: loss=60.767805854926394, w0=13.999999999999995, w1=0.022371061918243246\n",
      "SubSGD iter. 20/499: loss=60.067805854926384, w0=14.699999999999994, w1=0.008568277174328037\n",
      "SubSGD iter. 21/499: loss=59.36780585492639, w0=15.399999999999993, w1=-0.0035927994159410306\n",
      "SubSGD iter. 22/499: loss=58.667805854926385, w0=16.099999999999994, w1=-0.00783741366694974\n",
      "SubSGD iter. 23/499: loss=57.96780585492639, w0=16.799999999999994, w1=-0.02726518857816375\n",
      "SubSGD iter. 24/499: loss=57.26780585492638, w0=17.499999999999993, w1=-0.028187682740024006\n",
      "SubSGD iter. 25/499: loss=56.567805854926384, w0=18.199999999999992, w1=-0.014061630095471045\n",
      "SubSGD iter. 26/499: loss=55.86780585492639, w0=18.89999999999999, w1=-0.026771611707933726\n",
      "SubSGD iter. 27/499: loss=55.167805854926385, w0=19.59999999999999, w1=-0.03678489314078764\n",
      "SubSGD iter. 28/499: loss=54.46780585492639, w0=20.29999999999999, w1=-0.04129415506314223\n",
      "SubSGD iter. 29/499: loss=53.767805854926394, w0=20.99999999999999, w1=-0.042475160264835866\n",
      "SubSGD iter. 30/499: loss=53.067805854926384, w0=21.69999999999999, w1=-0.042113979085586895\n",
      "SubSGD iter. 31/499: loss=52.36780585492639, w0=22.399999999999988, w1=-0.08367179036845827\n",
      "SubSGD iter. 32/499: loss=51.667805854926385, w0=23.099999999999987, w1=-0.088016271701797\n",
      "SubSGD iter. 33/499: loss=50.9678058549264, w0=23.799999999999986, w1=-0.08507664253671753\n",
      "SubSGD iter. 34/499: loss=50.267805854926394, w0=24.499999999999986, w1=-0.0781691029885246\n",
      "SubSGD iter. 35/499: loss=49.567805854926405, w0=25.199999999999985, w1=-0.07286642166222146\n",
      "SubSGD iter. 36/499: loss=48.867805854926395, w0=25.899999999999984, w1=-0.08991035711169794\n",
      "SubSGD iter. 37/499: loss=48.1678058549264, w0=26.599999999999984, w1=-0.10264270017989721\n",
      "SubSGD iter. 38/499: loss=47.4678058549264, w0=27.299999999999983, w1=-0.09753144820301178\n",
      "SubSGD iter. 39/499: loss=46.767805854926394, w0=27.999999999999982, w1=-0.11404748594493137\n",
      "SubSGD iter. 40/499: loss=46.06780585492639, w0=28.69999999999998, w1=-0.12756920546364683\n",
      "SubSGD iter. 41/499: loss=45.367805854926395, w0=29.39999999999998, w1=-0.13099808402555674\n",
      "SubSGD iter. 42/499: loss=44.6678058549264, w0=30.09999999999998, w1=-0.10483927846196972\n",
      "SubSGD iter. 43/499: loss=43.9678058549264, w0=30.79999999999998, w1=-0.10580416659248291\n",
      "SubSGD iter. 44/499: loss=43.2678058549264, w0=31.49999999999998, w1=-0.08448334916406873\n",
      "SubSGD iter. 45/499: loss=42.567805854926405, w0=32.19999999999998, w1=-0.08906161235945895\n",
      "SubSGD iter. 46/499: loss=41.867805854926395, w0=32.899999999999984, w1=-0.0829208281812787\n",
      "SubSGD iter. 47/499: loss=41.167805854926385, w0=33.59999999999999, w1=-0.08512612156666383\n",
      "SubSGD iter. 48/499: loss=40.4678058549264, w0=34.29999999999999, w1=-0.0902006256338997\n",
      "SubSGD iter. 49/499: loss=39.767805854926394, w0=34.99999999999999, w1=-0.09451556995582001\n",
      "SubSGD iter. 50/499: loss=39.06780585492639, w0=35.699999999999996, w1=-0.07101564498242827\n",
      "SubSGD iter. 51/499: loss=38.36780585492639, w0=36.4, w1=-0.04194350723810261\n",
      "SubSGD iter. 52/499: loss=37.66780585492638, w0=37.1, w1=-0.04950373072860524\n",
      "SubSGD iter. 53/499: loss=36.967805854926375, w0=37.800000000000004, w1=-0.03798996230843894\n",
      "SubSGD iter. 54/499: loss=36.26780585492637, w0=38.50000000000001, w1=-0.0036080459798540354\n",
      "SubSGD iter. 55/499: loss=35.56780585492636, w0=39.20000000000001, w1=-0.01607317496084852\n",
      "SubSGD iter. 56/499: loss=34.86780585492637, w0=39.90000000000001, w1=-0.01642249319228101\n",
      "SubSGD iter. 57/499: loss=34.16780585492637, w0=40.600000000000016, w1=-0.04503376295074721\n",
      "SubSGD iter. 58/499: loss=33.46780585492636, w0=41.30000000000002, w1=-0.016601568194615935\n",
      "SubSGD iter. 59/499: loss=32.76780585492636, w0=42.00000000000002, w1=-0.010492845518222986\n",
      "SubSGD iter. 60/499: loss=32.06780585492636, w0=42.700000000000024, w1=-0.006118648478783427\n",
      "SubSGD iter. 61/499: loss=31.36780585492636, w0=43.40000000000003, w1=-0.019610000828715845\n",
      "SubSGD iter. 62/499: loss=30.66780585492635, w0=44.10000000000003, w1=-0.01723758081767005\n",
      "SubSGD iter. 63/499: loss=29.967805854926347, w0=44.80000000000003, w1=-0.018978138376874027\n",
      "SubSGD iter. 64/499: loss=29.267805854926348, w0=45.500000000000036, w1=-0.03519378713453255\n",
      "SubSGD iter. 65/499: loss=28.567805854926345, w0=46.20000000000004, w1=-0.027039187427983447\n",
      "SubSGD iter. 66/499: loss=27.867805854926342, w0=46.90000000000004, w1=-0.023533756660400296\n",
      "SubSGD iter. 67/499: loss=27.17364499636531, w0=47.59259259259263, w1=-0.019552959866733017\n",
      "SubSGD iter. 68/499: loss=26.491880996860132, w0=48.277777777777814, w1=0.0021819690264912282\n",
      "SubSGD iter. 69/499: loss=25.81958266571991, w0=48.962962962963, w1=0.022828263916201914\n",
      "SubSGD iter. 70/499: loss=25.15962121770101, w0=49.63333333333337, w1=0.06729953130017281\n",
      "SubSGD iter. 71/499: loss=24.52499761200259, w0=50.28888888888893, w1=0.14301685372636758\n",
      "SubSGD iter. 72/499: loss=23.901990128103137, w0=50.944444444444485, w1=0.2034509514504806\n",
      "SubSGD iter. 73/499: loss=23.29031904107952, w0=51.59259259259263, w1=0.271030123272232\n",
      "SubSGD iter. 74/499: loss=22.692299372711105, w0=52.225925925925964, w1=0.38474079322401533\n",
      "SubSGD iter. 75/499: loss=22.107689087217146, w0=52.844444444444484, w1=0.5028551283490156\n",
      "SubSGD iter. 76/499: loss=21.54284675393242, w0=53.448148148148185, w1=0.6342308270588377\n",
      "SubSGD iter. 77/499: loss=20.99568973633539, w0=54.051851851851886, w1=0.7621400177228487\n",
      "SubSGD iter. 78/499: loss=20.455504886233445, w0=54.640740740740775, w1=0.908770632265004\n",
      "SubSGD iter. 79/499: loss=19.926597886398756, w0=55.244444444444476, w1=1.0389156003185385\n",
      "SubSGD iter. 80/499: loss=19.3962330868591, w0=55.80370370370373, w1=1.2172407781716241\n",
      "SubSGD iter. 81/499: loss=18.902923632408058, w0=56.34074074074077, w1=1.4262749611753076\n",
      "SubSGD iter. 82/499: loss=18.431889683026892, w0=56.8703703703704, w1=1.6151148365349064\n",
      "SubSGD iter. 83/499: loss=17.975468292643185, w0=57.400000000000034, w1=1.814028340747403\n",
      "SubSGD iter. 84/499: loss=17.524141952875965, w0=57.91481481481485, w1=2.025875144758865\n",
      "SubSGD iter. 85/499: loss=17.089224488592947, w0=58.42962962962966, w1=2.238555991072653\n",
      "SubSGD iter. 86/499: loss=16.658689623421736, w0=58.91481481481485, w1=2.4666801528373727\n",
      "SubSGD iter. 87/499: loss=16.251822535710275, w0=59.37777777777781, w1=2.7227955405277466\n",
      "SubSGD iter. 88/499: loss=15.853146504092685, w0=59.833333333333364, w1=2.964331398746456\n",
      "SubSGD iter. 89/499: loss=15.476322867463681, w0=60.25925925925929, w1=3.246355683345371\n",
      "SubSGD iter. 90/499: loss=15.112182659275986, w0=60.677777777777806, w1=3.505093437911256\n",
      "SubSGD iter. 91/499: loss=14.76393094594026, w0=61.08888888888892, w1=3.7820887993174144\n",
      "SubSGD iter. 92/499: loss=14.41322705117003, w0=61.49259259259262, w1=4.052635017106174\n",
      "SubSGD iter. 93/499: loss=14.071390831623791, w0=61.88148148148151, w1=4.34142401930714\n",
      "SubSGD iter. 94/499: loss=13.733478923796133, w0=62.270370370370394, w1=4.647600364924192\n",
      "SubSGD iter. 95/499: loss=13.393699429812655, w0=62.629629629629655, w1=4.960795462242414\n",
      "SubSGD iter. 96/499: loss=13.070919135153147, w0=63.00370370370373, w1=5.254689924580702\n",
      "SubSGD iter. 97/499: loss=12.750075663845353, w0=63.370370370370395, w1=5.561903148733525\n",
      "SubSGD iter. 98/499: loss=12.428151836687398, w0=63.74444444444447, w1=5.8495690934637015\n",
      "SubSGD iter. 99/499: loss=12.112424400690871, w0=64.08888888888892, w1=6.172289836259726\n",
      "SubSGD iter. 100/499: loss=11.800298882481416, w0=64.44814814814818, w1=6.488993514262167\n",
      "SubSGD iter. 101/499: loss=11.483955434259787, w0=64.76296296296299, w1=6.800359672010532\n",
      "SubSGD iter. 102/499: loss=11.19156815074442, w0=65.10000000000002, w1=7.116308968205975\n",
      "SubSGD iter. 103/499: loss=10.886344139337485, w0=65.44444444444447, w1=7.413375445529459\n",
      "SubSGD iter. 104/499: loss=10.592346618370453, w0=65.77407407407411, w1=7.730260781262841\n",
      "SubSGD iter. 105/499: loss=10.300220481623349, w0=66.08888888888892, w1=8.051827408925101\n",
      "SubSGD iter. 106/499: loss=10.013561180537772, w0=66.38148148148152, w1=8.366971299822675\n",
      "SubSGD iter. 107/499: loss=9.740965059990028, w0=66.67407407407411, w1=8.706822218285543\n",
      "SubSGD iter. 108/499: loss=9.45744051861874, w0=66.96666666666671, w1=9.041878177464618\n",
      "SubSGD iter. 109/499: loss=9.179061233438023, w0=67.2518518518519, w1=9.368170000135033\n",
      "SubSGD iter. 110/499: loss=8.911297697909243, w0=67.51481481481486, w1=9.71119272605861\n",
      "SubSGD iter. 111/499: loss=8.644396565353606, w0=67.79259259259263, w1=10.029523306515609\n",
      "SubSGD iter. 112/499: loss=8.389275689918739, w0=68.062962962963, w1=10.346388626290937\n",
      "SubSGD iter. 113/499: loss=8.149268699939062, w0=68.31851851851856, w1=10.64275819916962\n",
      "SubSGD iter. 114/499: loss=7.9299044912184184, w0=68.58888888888893, w1=10.915968334017387\n",
      "SubSGD iter. 115/499: loss=7.7178402052646256, w0=68.8666666666667, w1=11.196722780468097\n",
      "SubSGD iter. 116/499: loss=7.501549559853646, w0=69.11481481481485, w1=11.489118601290258\n",
      "SubSGD iter. 117/499: loss=7.295512868595813, w0=69.37037037037041, w1=11.774184326100864\n",
      "SubSGD iter. 118/499: loss=7.0978186428157395, w0=69.57407407407412, w1=12.09416034182165\n",
      "SubSGD iter. 119/499: loss=6.9105923815952455, w0=69.80000000000004, w1=12.354241988859004\n",
      "SubSGD iter. 120/499: loss=6.741637340041222, w0=70.04074074074079, w1=12.608151810332089\n",
      "SubSGD iter. 121/499: loss=6.573520891928948, w0=70.2518518518519, w1=12.83687828623127\n",
      "SubSGD iter. 122/499: loss=6.427627843580924, w0=70.48518518518523, w1=13.090993342822493\n",
      "SubSGD iter. 123/499: loss=6.267329069517599, w0=70.71111111111115, w1=13.323053428681456\n",
      "SubSGD iter. 124/499: loss=6.122095512442891, w0=70.92222222222226, w1=13.549341710515924\n",
      "SubSGD iter. 125/499: loss=5.990593762384662, w0=71.12592592592597, w1=13.75314530797037\n",
      "SubSGD iter. 126/499: loss=5.871846285789628, w0=71.31481481481485, w1=13.982544881831256\n",
      "SubSGD iter. 127/499: loss=5.752886634748983, w0=71.49629629629634, w1=14.192795063074065\n",
      "SubSGD iter. 128/499: loss=5.651841685455442, w0=71.66296296296301, w1=14.358287605447249\n",
      "SubSGD iter. 129/499: loss=5.576948765371243, w0=71.80000000000004, w1=14.534700970483108\n",
      "SubSGD iter. 130/499: loss=5.51268752471172, w0=71.90000000000003, w1=14.673480114194264\n",
      "SubSGD iter. 131/499: loss=5.476261709615753, w0=71.98518518518522, w1=14.76872024659593\n",
      "SubSGD iter. 132/499: loss=5.451459759927209, w0=72.04814814814817, w1=14.880911783415923\n",
      "SubSGD iter. 133/499: loss=5.427566271878385, w0=72.11111111111113, w1=15.005474321712168\n",
      "SubSGD iter. 134/499: loss=5.404221968245374, w0=72.19629629629631, w1=15.099349626366084\n",
      "SubSGD iter. 135/499: loss=5.3835448097006005, w0=72.25185185185187, w1=15.180960143620176\n",
      "SubSGD iter. 136/499: loss=5.369642760216041, w0=72.30740740740742, w1=15.258668492367596\n",
      "SubSGD iter. 137/499: loss=5.357075349996473, w0=72.36296296296298, w1=15.329625483262596\n",
      "SubSGD iter. 138/499: loss=5.34558508994084, w0=72.42592592592594, w1=15.410251103017007\n",
      "SubSGD iter. 139/499: loss=5.3344723981709805, w0=72.43703703703704, w1=15.483399743221163\n",
      "SubSGD iter. 140/499: loss=5.328936506843748, w0=72.46296296296298, w1=15.538945917748748\n",
      "SubSGD iter. 141/499: loss=5.324329269848089, w0=72.49629629629631, w1=15.562161734306134\n",
      "SubSGD iter. 142/499: loss=5.321985311533328, w0=72.4851851851852, w1=15.60775965781486\n",
      "SubSGD iter. 143/499: loss=5.320402290899778, w0=72.51111111111113, w1=15.619636888785333\n",
      "SubSGD iter. 144/499: loss=5.319070658955466, w0=72.52962962962965, w1=15.660803898528638\n",
      "SubSGD iter. 145/499: loss=5.317502691935693, w0=72.54074074074076, w1=15.666288767275569\n",
      "SubSGD iter. 146/499: loss=5.317072840612575, w0=72.56666666666669, w1=15.693962023476821\n",
      "SubSGD iter. 147/499: loss=5.3157991420844155, w0=72.58518518518521, w1=15.747387004729099\n",
      "SubSGD iter. 148/499: loss=5.314276813700851, w0=72.59629629629632, w1=15.756576099700153\n",
      "SubSGD iter. 149/499: loss=5.313779549709361, w0=72.63703703703706, w1=15.759798377832578\n",
      "SubSGD iter. 150/499: loss=5.31251078693134, w0=72.662962962963, w1=15.765656018178396\n",
      "SubSGD iter. 151/499: loss=5.311906244309498, w0=72.69629629629632, w1=15.772074301329074\n",
      "SubSGD iter. 152/499: loss=5.311673952787854, w0=72.67037037037039, w1=15.76559101188306\n",
      "SubSGD iter. 153/499: loss=5.311833937305201, w0=72.66666666666669, w1=15.781162106024329\n",
      "SubSGD iter. 154/499: loss=5.311623012149965, w0=72.66296296296298, w1=15.792270008203895\n",
      "SubSGD iter. 155/499: loss=5.311559926074349, w0=72.65185185185187, w1=15.78916680596746\n",
      "SubSGD iter. 156/499: loss=5.311642410997213, w0=72.69259259259262, w1=15.798955241359373\n",
      "SubSGD iter. 157/499: loss=5.311522180667839, w0=72.68888888888891, w1=15.798616991513013\n",
      "SubSGD iter. 158/499: loss=5.3115240904558245, w0=72.6851851851852, w1=15.810957897986109\n",
      "SubSGD iter. 159/499: loss=5.31145441262699, w0=72.68888888888891, w1=15.813953722252789\n",
      "SubSGD iter. 160/499: loss=5.311437497943217, w0=72.69259259259262, w1=15.827836334268067\n",
      "SubSGD iter. 161/499: loss=5.311359115511109, w0=72.71111111111114, w1=15.813385824418967\n",
      "SubSGD iter. 162/499: loss=5.311503700376805, w0=72.72222222222224, w1=15.831005071661968\n",
      "SubSGD iter. 163/499: loss=5.311486774931175, w0=72.73333333333335, w1=15.827832967993208\n",
      "SubSGD iter. 164/499: loss=5.3116196390975, w0=72.73703703703706, w1=15.832200265288918\n",
      "SubSGD iter. 165/499: loss=5.311624845589817, w0=72.72592592592595, w1=15.836968816064976\n",
      "SubSGD iter. 166/499: loss=5.311480479948974, w0=72.72962962962966, w1=15.811592076810358\n",
      "SubSGD iter. 167/499: loss=5.311699974955385, w0=72.72592592592595, w1=15.818420347695445\n",
      "SubSGD iter. 168/499: loss=5.31161411089442, w0=72.71481481481484, w1=15.812386755939581\n",
      "SubSGD iter. 169/499: loss=5.311547568418845, w0=72.71851851851855, w1=15.80835172586244\n",
      "SubSGD iter. 170/499: loss=5.3116133088015305, w0=72.70000000000003, w1=15.832269406606843\n",
      "SubSGD iter. 171/499: loss=5.311334086000004, w0=72.68888888888893, w1=15.813177708292221\n",
      "SubSGD iter. 172/499: loss=5.3114418793853755, w0=72.70000000000003, w1=15.831120827042048\n",
      "SubSGD iter. 173/499: loss=5.311340570979895, w0=72.6740740740741, w1=15.834219278317473\n",
      "SubSGD iter. 174/499: loss=5.311323076855124, w0=72.6777777777778, w1=15.828125601103215\n",
      "SubSGD iter. 175/499: loss=5.311357482285458, w0=72.6666666666667, w1=15.821387278725702\n",
      "SubSGD iter. 176/499: loss=5.311395527438321, w0=72.6555555555556, w1=15.830545077979325\n",
      "SubSGD iter. 177/499: loss=5.3113438217090625, w0=72.65185185185189, w1=15.84974409504572\n",
      "SubSGD iter. 178/499: loss=5.311235422392706, w0=72.64814814814818, w1=15.84016610228822\n",
      "SubSGD iter. 179/499: loss=5.311289500570948, w0=72.65185185185189, w1=15.840750881612989\n",
      "SubSGD iter. 180/499: loss=5.311286198856135, w0=72.6703703703704, w1=15.853179763438051\n",
      "SubSGD iter. 181/499: loss=5.311216024310831, w0=72.65185185185189, w1=15.847216103568668\n",
      "SubSGD iter. 182/499: loss=5.311249695651951, w0=72.64814814814818, w1=15.855786520149463\n",
      "SubSGD iter. 183/499: loss=5.311201306336236, w0=72.65185185185189, w1=15.877230077112108\n",
      "SubSGD iter. 184/499: loss=5.311080234152852, w0=72.64814814814818, w1=15.875803394789411\n",
      "SubSGD iter. 185/499: loss=5.311088289325047, w0=72.65185185185189, w1=15.889419744407746\n",
      "SubSGD iter. 186/499: loss=5.3110114102335375, w0=72.6555555555556, w1=15.887229580941696\n",
      "SubSGD iter. 187/499: loss=5.311023776086519, w0=72.6592592592593, w1=15.878984194965245\n",
      "SubSGD iter. 188/499: loss=5.311070330251238, w0=72.6481481481482, w1=15.8871487321661\n",
      "SubSGD iter. 189/499: loss=5.311024232565723, w0=72.63703703703709, w1=15.909269158464475\n",
      "SubSGD iter. 190/499: loss=5.310899338719334, w0=72.6481481481482, w1=15.908133702832263\n",
      "SubSGD iter. 191/499: loss=5.310905749600363, w0=72.64444444444449, w1=15.916592408062398\n",
      "SubSGD iter. 192/499: loss=5.310857991016626, w0=72.6481481481482, w1=15.906944877889995\n",
      "SubSGD iter. 193/499: loss=5.3109124618091474, w0=72.63703703703709, w1=15.92350478493847\n",
      "SubSGD iter. 194/499: loss=5.31081896313696, w0=72.6407407407408, w1=15.949290891298691\n",
      "SubSGD iter. 195/499: loss=5.310673372542851, w0=72.6444444444445, w1=15.980639647127797\n",
      "SubSGD iter. 196/499: loss=5.310746599080924, w0=72.6333333333334, w1=15.983595038487108\n",
      "SubSGD iter. 197/499: loss=5.310645564446876, w0=72.62962962962969, w1=15.972017363865712\n",
      "SubSGD iter. 198/499: loss=5.310574539110427, w0=72.64814814814821, w1=15.990965363813974\n",
      "SubSGD iter. 199/499: loss=5.310814631563244, w0=72.6370370370371, w1=16.004386808041982\n",
      "SubSGD iter. 200/499: loss=5.310745385318856, w0=72.61851851851858, w1=16.00258020951764\n",
      "SubSGD iter. 201/499: loss=5.3106448188421, w0=72.61481481481488, w1=16.005994305364013\n",
      "SubSGD iter. 202/499: loss=5.310659268262455, w0=72.61851851851858, w1=16.009591831309045\n",
      "SubSGD iter. 203/499: loss=5.310660942173795, w0=72.62222222222229, w1=16.008370432298623\n",
      "SubSGD iter. 204/499: loss=5.310658133548065, w0=72.64074074074081, w1=16.001111758898837\n",
      "SubSGD iter. 205/499: loss=5.310772108393582, w0=72.65925925925933, w1=15.99925382864929\n",
      "SubSGD iter. 206/499: loss=5.3109498169976455, w0=72.6333333333334, w1=15.994402436389954\n",
      "SubSGD iter. 207/499: loss=5.310678389598827, w0=72.6370370370371, w1=15.978423091206274\n",
      "SubSGD iter. 208/499: loss=5.310666526100568, w0=72.625925925926, w1=15.950984856639705\n",
      "SubSGD iter. 209/499: loss=5.310716805455479, w0=72.64444444444452, w1=15.949572631732268\n",
      "SubSGD iter. 210/499: loss=5.310671781811915, w0=72.64814814814822, w1=15.919638988900555\n",
      "SubSGD iter. 211/499: loss=5.310840789756846, w0=72.65925925925933, w1=15.914800013486206\n",
      "SubSGD iter. 212/499: loss=5.31086811103194, w0=72.64814814814822, w1=15.90696150653444\n",
      "SubSGD iter. 213/499: loss=5.310912367922378, w0=72.64444444444452, w1=15.914162549935165\n",
      "SubSGD iter. 214/499: loss=5.310871710206465, w0=72.61111111111119, w1=15.93468771222879\n",
      "SubSGD iter. 215/499: loss=5.311122040803596, w0=72.63703703703712, w1=15.938047579451492\n",
      "SubSGD iter. 216/499: loss=5.310736853257183, w0=72.6185185185186, w1=15.953978958861178\n",
      "SubSGD iter. 217/499: loss=5.310822110374732, w0=72.6296296296297, w1=15.952764672154592\n",
      "SubSGD iter. 218/499: loss=5.310668773034282, w0=72.64814814814822, w1=15.957826315158213\n",
      "SubSGD iter. 219/499: loss=5.31071397881432, w0=72.64444444444452, w1=15.932061867948427\n",
      "SubSGD iter. 220/499: loss=5.310770649103742, w0=72.63333333333341, w1=15.940203635940621\n",
      "SubSGD iter. 221/499: loss=5.310724679975152, w0=72.65185185185193, w1=15.957559140416048\n",
      "SubSGD iter. 222/499: loss=5.310749837662074, w0=72.64074074074082, w1=15.965276531758201\n",
      "SubSGD iter. 223/499: loss=5.310663266581089, w0=72.62962962962972, w1=15.972421284537266\n",
      "SubSGD iter. 224/499: loss=5.310575467932196, w0=72.62592592592601, w1=15.974800610856983\n",
      "SubSGD iter. 225/499: loss=5.310580939229511, w0=72.6148148148149, w1=15.950311744615213\n",
      "SubSGD iter. 226/499: loss=5.310924583283901, w0=72.62592592592601, w1=15.956165875279337\n",
      "SubSGD iter. 227/499: loss=5.3106837305740555, w0=72.6222222222223, w1=15.99575488741744\n",
      "SubSGD iter. 228/499: loss=5.3106291239093935, w0=72.62592592592601, w1=15.989698829809267\n",
      "SubSGD iter. 229/499: loss=5.31061519791217, w0=72.60740740740749, w1=15.991325943570866\n",
      "SubSGD iter. 230/499: loss=5.310745449032315, w0=72.59629629629639, w1=15.998916472699568\n",
      "SubSGD iter. 231/499: loss=5.310915314135553, w0=72.6148148148149, w1=15.99900171571757\n",
      "SubSGD iter. 232/499: loss=5.310654097929863, w0=72.63333333333343, w1=16.00160825817302\n",
      "SubSGD iter. 233/499: loss=5.310700275736184, w0=72.6148148148149, w1=15.992712528723308\n",
      "SubSGD iter. 234/499: loss=5.310649447694382, w0=72.62592592592601, w1=15.988789969821783\n",
      "SubSGD iter. 235/499: loss=5.310613107974708, w0=72.59259259259268, w1=16.000344587542408\n",
      "SubSGD iter. 236/499: loss=5.311018117798694, w0=72.59629629629639, w1=16.031513568707158\n",
      "SubSGD iter. 237/499: loss=5.310966071946703, w0=72.60740740740749, w1=16.034625568433615\n",
      "SubSGD iter. 238/499: loss=5.310757367113102, w0=72.6185185185186, w1=16.05739181930522\n",
      "SubSGD iter. 239/499: loss=5.310770858978291, w0=72.60740740740749, w1=16.052914555512643\n",
      "SubSGD iter. 240/499: loss=5.310823877257213, w0=72.64074074074082, w1=16.055231369203373\n",
      "SubSGD iter. 241/499: loss=5.310936485096257, w0=72.61481481481489, w1=16.068425836606032\n",
      "SubSGD iter. 242/499: loss=5.310824406226111, w0=72.63333333333341, w1=16.063385841176036\n",
      "SubSGD iter. 243/499: loss=5.310887911887592, w0=72.6296296296297, w1=16.063122805244763\n",
      "SubSGD iter. 244/499: loss=5.310850442638695, w0=72.6185185185186, w1=16.05912926717022\n",
      "SubSGD iter. 245/499: loss=5.310774854266266, w0=72.63703703703712, w1=16.029847267785915\n",
      "SubSGD iter. 246/499: loss=5.310822716001722, w0=72.63333333333341, w1=16.02745443820554\n",
      "SubSGD iter. 247/499: loss=5.310778777961652, w0=72.6222222222223, w1=16.007797954896976\n",
      "SubSGD iter. 248/499: loss=5.310656817127512, w0=72.6185185185186, w1=15.990548693812075\n",
      "SubSGD iter. 249/499: loss=5.310617152187461, w0=72.61481481481489, w1=15.97285514776312\n",
      "SubSGD iter. 250/499: loss=5.310745499154436, w0=72.6185185185186, w1=15.987409091946073\n",
      "SubSGD iter. 251/499: loss=5.3106099326248835, w0=72.61481481481489, w1=16.00867625841657\n",
      "SubSGD iter. 252/499: loss=5.3106612513030695, w0=72.61111111111119, w1=16.009219663449507\n",
      "SubSGD iter. 253/499: loss=5.310698323431368, w0=72.63703703703712, w1=16.025499694154682\n",
      "SubSGD iter. 254/499: loss=5.31080951117977, w0=72.61111111111119, w1=16.030746796424086\n",
      "SubSGD iter. 255/499: loss=5.310714240629835, w0=72.60740740740748, w1=16.052863209264743\n",
      "SubSGD iter. 256/499: loss=5.310823690530287, w0=72.61851851851858, w1=16.05621707185158\n",
      "SubSGD iter. 257/499: loss=5.310768157628528, w0=72.60740740740748, w1=16.071427964394374\n",
      "SubSGD iter. 258/499: loss=5.310913348159809, w0=72.60370370370377, w1=16.058002111754682\n",
      "SubSGD iter. 259/499: loss=5.310915719446724, w0=72.60000000000007, w1=16.063891475168887\n",
      "SubSGD iter. 260/499: loss=5.311010477506336, w0=72.60370370370377, w1=16.067083776911517\n",
      "SubSGD iter. 261/499: loss=5.310948746035662, w0=72.6296296296297, w1=16.069707952084695\n",
      "SubSGD iter. 262/499: loss=5.310870443609103, w0=72.6185185185186, w1=16.056061160060874\n",
      "SubSGD iter. 263/499: loss=5.310767799106978, w0=72.60740740740749, w1=16.03743910492743\n",
      "SubSGD iter. 264/499: loss=5.310767598883558, w0=72.62592592592601, w1=16.038241437876174\n",
      "SubSGD iter. 265/499: loss=5.310738200490951, w0=72.60740740740749, w1=16.02512885939418\n",
      "SubSGD iter. 266/499: loss=5.31074675705141, w0=72.60370370370379, w1=16.018498854652005\n",
      "SubSGD iter. 267/499: loss=5.31077852514841, w0=72.61481481481489, w1=16.01926508564465\n",
      "SubSGD iter. 268/499: loss=5.310683185970296, w0=72.625925925926, w1=16.021866091140037\n",
      "SubSGD iter. 269/499: loss=5.310689167022268, w0=72.65925925925933, w1=16.02589316181047\n",
      "SubSGD iter. 270/499: loss=5.31103072825554, w0=72.6333333333334, w1=16.00658653532015\n",
      "SubSGD iter. 271/499: loss=5.310715396184719, w0=72.60000000000007, w1=16.00345176160615\n",
      "SubSGD iter. 272/499: loss=5.310804069636107, w0=72.58888888888896, w1=15.9994231731057\n",
      "SubSGD iter. 273/499: loss=5.311132778947702, w0=72.61481481481489, w1=16.00813672965053\n",
      "SubSGD iter. 274/499: loss=5.31066085237459, w0=72.625925925926, w1=16.00440062652359\n",
      "SubSGD iter. 275/499: loss=5.310649004918884, w0=72.62222222222229, w1=15.997418469965714\n",
      "SubSGD iter. 276/499: loss=5.310632949342942, w0=72.61111111111119, w1=15.999430976185055\n",
      "SubSGD iter. 277/499: loss=5.310691085659477, w0=72.61481481481489, w1=15.980507410226307\n",
      "SubSGD iter. 278/499: loss=5.310684709807206, w0=72.625925925926, w1=15.991194057961502\n",
      "SubSGD iter. 279/499: loss=5.310618636212207, w0=72.6370370370371, w1=15.99645257672346\n",
      "SubSGD iter. 280/499: loss=5.3107212867938465, w0=72.64074074074081, w1=15.973364018621997\n",
      "SubSGD iter. 281/499: loss=5.310687830586929, w0=72.62222222222229, w1=15.97189985198582\n",
      "SubSGD iter. 282/499: loss=5.3106199574525, w0=72.6333333333334, w1=15.987677814168656\n",
      "SubSGD iter. 283/499: loss=5.310657965001878, w0=72.6370370370371, w1=15.98153997935398\n",
      "SubSGD iter. 284/499: loss=5.310675992979458, w0=72.64814814814821, w1=15.992872040790436\n",
      "SubSGD iter. 285/499: loss=5.310820422685403, w0=72.6444444444445, w1=15.99292979886065\n",
      "SubSGD iter. 286/499: loss=5.310783927779447, w0=72.6333333333334, w1=15.965807501219812\n",
      "SubSGD iter. 287/499: loss=5.3105915386194855, w0=72.60740740740746, w1=15.969537060958546\n",
      "SubSGD iter. 288/499: loss=5.310918539272912, w0=72.61111111111117, w1=15.951971348136148\n",
      "SubSGD iter. 289/499: loss=5.310984740109898, w0=72.61481481481488, w1=15.955906785865265\n",
      "SubSGD iter. 290/499: loss=5.310880136441293, w0=72.64814814814821, w1=15.972820805945386\n",
      "SubSGD iter. 291/499: loss=5.310759521362392, w0=72.65185185185192, w1=15.973591507575518\n",
      "SubSGD iter. 292/499: loss=5.310798532536907, w0=72.64814814814821, w1=15.960808588615716\n",
      "SubSGD iter. 293/499: loss=5.310723036829977, w0=72.62962962962969, w1=15.952506632712728\n",
      "SubSGD iter. 294/499: loss=5.310670420321145, w0=72.6333333333334, w1=15.941215973526694\n",
      "SubSGD iter. 295/499: loss=5.310718964229296, w0=72.61481481481488, w1=15.934731424207364\n",
      "SubSGD iter. 296/499: loss=5.311048352889529, w0=72.64074074074081, w1=15.95196100600095\n",
      "SubSGD iter. 297/499: loss=5.310658296843543, w0=72.6296296296297, w1=15.946104047480976\n",
      "SubSGD iter. 298/499: loss=5.310711293508848, w0=72.63333333333341, w1=15.934014670932426\n",
      "SubSGD iter. 299/499: loss=5.3107596234086385, w0=72.6296296296297, w1=15.933777211391586\n",
      "SubSGD iter. 300/499: loss=5.310789986266073, w0=72.6185185185186, w1=15.931293839472474\n",
      "SubSGD iter. 301/499: loss=5.311002320293836, w0=72.63703703703712, w1=15.919937926799697\n",
      "SubSGD iter. 302/499: loss=5.3108391019275265, w0=72.63333333333341, w1=15.938657597950737\n",
      "SubSGD iter. 303/499: loss=5.310733409039801, w0=72.60740740740748, w1=15.965816973091123\n",
      "SubSGD iter. 304/499: loss=5.31094809154117, w0=72.61851851851858, w1=15.973386793929084\n",
      "SubSGD iter. 305/499: loss=5.310667935105579, w0=72.62962962962969, w1=15.970968075908083\n",
      "SubSGD iter. 306/499: loss=5.310572126256696, w0=72.65555555555562, w1=15.97092312027516\n",
      "SubSGD iter. 307/499: loss=5.310827098216796, w0=72.65925925925933, w1=15.96037286917976\n",
      "SubSGD iter. 308/499: loss=5.310831724426792, w0=72.65555555555562, w1=15.95736560891163\n",
      "SubSGD iter. 309/499: loss=5.310785920185362, w0=72.64444444444452, w1=15.969790162732764\n",
      "SubSGD iter. 310/499: loss=5.310713646100285, w0=72.64074074074081, w1=15.958439237569662\n",
      "SubSGD iter. 311/499: loss=5.310642499767218, w0=72.6370370370371, w1=15.962243242929288\n",
      "SubSGD iter. 312/499: loss=5.310617383283489, w0=72.64074074074081, w1=15.968852363052168\n",
      "SubSGD iter. 313/499: loss=5.310674127401299, w0=72.64444444444452, w1=15.97683276566208\n",
      "SubSGD iter. 314/499: loss=5.310735036495399, w0=72.64814814814822, w1=15.9673999077653\n",
      "SubSGD iter. 315/499: loss=5.310743056547452, w0=72.64444444444452, w1=15.952443437658363\n",
      "SubSGD iter. 316/499: loss=5.310660959145336, w0=72.64074074074081, w1=15.944395467136548\n",
      "SubSGD iter. 317/499: loss=5.310701012532473, w0=72.6370370370371, w1=15.928735361574358\n",
      "SubSGD iter. 318/499: loss=5.310789430847389, w0=72.61851851851858, w1=15.92004833567976\n",
      "SubSGD iter. 319/499: loss=5.311091654244168, w0=72.62222222222229, w1=15.914032028762087\n",
      "SubSGD iter. 320/499: loss=5.311066106941413, w0=72.64074074074081, w1=15.934968115985164\n",
      "SubSGD iter. 321/499: loss=5.310754240175136, w0=72.62222222222229, w1=15.94480802999554\n",
      "SubSGD iter. 322/499: loss=5.310821623295655, w0=72.64074074074081, w1=15.94730591946522\n",
      "SubSGD iter. 323/499: loss=5.31068457986607, w0=72.64444444444452, w1=15.934821920440331\n",
      "SubSGD iter. 324/499: loss=5.310755065607868, w0=72.64074074074081, w1=15.930660655609309\n",
      "SubSGD iter. 325/499: loss=5.310778560470199, w0=72.6370370370371, w1=15.92478207388353\n",
      "SubSGD iter. 326/499: loss=5.310811751452738, w0=72.64814814814821, w1=15.931046895916923\n",
      "SubSGD iter. 327/499: loss=5.310776379723906, w0=72.65925925925931, w1=15.910447681764781\n",
      "SubSGD iter. 328/499: loss=5.3108926846745135, w0=72.65555555555561, w1=15.912613392835476\n",
      "SubSGD iter. 329/499: loss=5.310880456881878, w0=72.6518518518519, w1=15.898801616843501\n",
      "SubSGD iter. 330/499: loss=5.31095843936765, w0=72.65555555555561, w1=15.882190256228045\n",
      "SubSGD iter. 331/499: loss=5.311052228551143, w0=72.6444444444445, w1=15.87191541806296\n",
      "SubSGD iter. 332/499: loss=5.3111102411790245, w0=72.6333333333334, w1=15.859644202709132\n",
      "SubSGD iter. 333/499: loss=5.311278140224674, w0=72.65185185185192, w1=15.862557770272076\n",
      "SubSGD iter. 334/499: loss=5.311163075270466, w0=72.64814814814821, w1=15.84402661984126\n",
      "SubSGD iter. 335/499: loss=5.3112677037538445, w0=72.65185185185192, w1=15.811484495361412\n",
      "SubSGD iter. 336/499: loss=5.3114514394125125, w0=72.65555555555562, w1=15.797311910771924\n",
      "SubSGD iter. 337/499: loss=5.311531459054937, w0=72.65185185185192, w1=15.805775383693613\n",
      "SubSGD iter. 338/499: loss=5.3114836735524, w0=72.66296296296302, w1=15.8130499432151\n",
      "SubSGD iter. 339/499: loss=5.311442600758088, w0=72.65925925925931, w1=15.81379998990113\n",
      "SubSGD iter. 340/499: loss=5.311438365929415, w0=72.64814814814821, w1=15.829440306195558\n",
      "SubSGD iter. 341/499: loss=5.311350059346422, w0=72.65185185185192, w1=15.824439154983367\n",
      "SubSGD iter. 342/499: loss=5.311378296280142, w0=72.64074074074081, w1=15.841589466850964\n",
      "SubSGD iter. 343/499: loss=5.311287860678863, w0=72.65185185185192, w1=15.845557983647018\n",
      "SubSGD iter. 344/499: loss=5.311259057540913, w0=72.64814814814821, w1=15.865740593060753\n",
      "SubSGD iter. 345/499: loss=5.31114510477681, w0=72.65185185185192, w1=15.862428590162999\n",
      "SubSGD iter. 346/499: loss=5.311163804632571, w0=72.66296296296302, w1=15.862548171119464\n",
      "SubSGD iter. 347/499: loss=5.311163129468115, w0=72.67407407407413, w1=15.858279314225918\n",
      "SubSGD iter. 348/499: loss=5.311187231804567, w0=72.6481481481482, w1=15.855575857741316\n",
      "SubSGD iter. 349/499: loss=5.311202495754472, w0=72.62222222222226, w1=15.853316672062126\n",
      "SubSGD iter. 350/499: loss=5.311548427943642, w0=72.64074074074078, w1=15.865308814044388\n",
      "SubSGD iter. 351/499: loss=5.311147542638604, w0=72.64444444444449, w1=15.876357753724193\n",
      "SubSGD iter. 352/499: loss=5.311085159366396, w0=72.6481481481482, w1=15.880349265526812\n",
      "SubSGD iter. 353/499: loss=5.311062622944394, w0=72.66666666666671, w1=15.903528344984325\n",
      "SubSGD iter. 354/499: loss=5.3109317518504024, w0=72.65555555555561, w1=15.896120486063381\n",
      "SubSGD iter. 355/499: loss=5.310973577264691, w0=72.65925925925931, w1=15.883732264044928\n",
      "SubSGD iter. 356/499: loss=5.311043522241199, w0=72.67777777777783, w1=15.898115956652953\n",
      "SubSGD iter. 357/499: loss=5.310962310664583, w0=72.70370370370377, w1=15.902711524866906\n",
      "SubSGD iter. 358/499: loss=5.311096634471494, w0=72.70740740740747, w1=15.929889050695044\n",
      "SubSGD iter. 359/499: loss=5.311215850707822, w0=72.69629629629637, w1=15.929708363882723\n",
      "SubSGD iter. 360/499: loss=5.311105290909305, w0=72.67037037037043, w1=15.924863364964775\n",
      "SubSGD iter. 361/499: loss=5.310833882928889, w0=72.65185185185192, w1=15.931190574760748\n",
      "SubSGD iter. 362/499: loss=5.310775568500688, w0=72.62592592592598, w1=15.963421408200835\n",
      "SubSGD iter. 363/499: loss=5.310637412290386, w0=72.60740740740746, w1=15.984832644557159\n",
      "SubSGD iter. 364/499: loss=5.310797031608098, w0=72.61851851851857, w1=15.990736710737723\n",
      "SubSGD iter. 365/499: loss=5.310617584535263, w0=72.60740740740746, w1=15.983565935598033\n",
      "SubSGD iter. 366/499: loss=5.3108070943066945, w0=72.61851851851857, w1=15.986984232763806\n",
      "SubSGD iter. 367/499: loss=5.310608955654687, w0=72.6444444444445, w1=16.004772632985976\n",
      "SubSGD iter. 368/499: loss=5.310819897846727, w0=72.6407407407408, w1=16.002135406378176\n",
      "SubSGD iter. 369/499: loss=5.310775217503133, w0=72.6444444444445, w1=15.988567009225433\n",
      "SubSGD iter. 370/499: loss=5.310770676742149, w0=72.66296296296302, w1=15.999343980882314\n",
      "SubSGD iter. 371/499: loss=5.310986761149407, w0=72.6444444444445, w1=15.972914855142717\n",
      "SubSGD iter. 372/499: loss=5.310723136682946, w0=72.6407407407408, w1=15.969881703715176\n",
      "SubSGD iter. 373/499: loss=5.310677253802675, w0=72.62962962962969, w1=15.976921546956351\n",
      "SubSGD iter. 374/499: loss=5.310588624814727, w0=72.62592592592598, w1=15.966023016999301\n",
      "SubSGD iter. 375/499: loss=5.310620803992189, w0=72.62962962962969, w1=15.958068568603274\n",
      "SubSGD iter. 376/499: loss=5.310634913718512, w0=72.64814814814821, w1=15.955565909116913\n",
      "SubSGD iter. 377/499: loss=5.310707113316035, w0=72.6370370370371, w1=15.943466400394094\n",
      "SubSGD iter. 378/499: loss=5.310706258123921, w0=72.64814814814821, w1=15.964235583831103\n",
      "SubSGD iter. 379/499: loss=5.310733445592538, w0=72.6444444444445, w1=15.950592112260551\n",
      "SubSGD iter. 380/499: loss=5.310666025736388, w0=72.6407407407408, w1=15.95760752968758\n",
      "SubSGD iter. 381/499: loss=5.310639973633004, w0=72.6444444444445, w1=15.954455769169316\n",
      "SubSGD iter. 382/499: loss=5.310667071170471, w0=72.6333333333334, w1=15.961178354411357\n",
      "SubSGD iter. 383/499: loss=5.310606254894578, w0=72.61481481481488, w1=15.948158241355932\n",
      "SubSGD iter. 384/499: loss=5.310941690650256, w0=72.62592592592598, w1=15.93769279719612\n",
      "SubSGD iter. 385/499: loss=5.310804805827681, w0=72.6444444444445, w1=15.937589563309752\n",
      "SubSGD iter. 386/499: loss=5.310739439256063, w0=72.64814814814821, w1=15.970809208514183\n",
      "SubSGD iter. 387/499: loss=5.310753411566868, w0=72.6370370370371, w1=15.99418669261788\n",
      "SubSGD iter. 388/499: loss=5.310714404657116, w0=72.6333333333334, w1=15.98438533443046\n",
      "SubSGD iter. 389/499: loss=5.310647964801213, w0=72.6370370370371, w1=15.997940162706875\n",
      "SubSGD iter. 390/499: loss=5.310725805017047, w0=72.61111111111117, w1=15.998464611202056\n",
      "SubSGD iter. 391/499: loss=5.310690371127577, w0=72.61481481481488, w1=15.99742732727743\n",
      "SubSGD iter. 392/499: loss=5.310652933824393, w0=72.61111111111117, w1=15.99344664919221\n",
      "SubSGD iter. 393/499: loss=5.3106866608379955, w0=72.61481481481488, w1=15.98503172203889\n",
      "SubSGD iter. 394/499: loss=5.310648768807656, w0=72.61851851851858, w1=16.004951480940537\n",
      "SubSGD iter. 395/499: loss=5.310650271617053, w0=72.61481481481488, w1=15.99422468981101\n",
      "SubSGD iter. 396/499: loss=5.310650565788843, w0=72.62592592592598, w1=15.998461561567785\n",
      "SubSGD iter. 397/499: loss=5.310635347948058, w0=72.61481481481488, w1=15.97619764557855\n",
      "SubSGD iter. 398/499: loss=5.310718946450408, w0=72.61111111111117, w1=15.977398190733469\n",
      "SubSGD iter. 399/499: loss=5.31078275002252, w0=72.6444444444445, w1=15.965061250949367\n",
      "SubSGD iter. 400/499: loss=5.310699283045529, w0=72.62592592592598, w1=15.957547741270998\n",
      "SubSGD iter. 401/499: loss=5.310674908939354, w0=72.62222222222228, w1=15.944507778699192\n",
      "SubSGD iter. 402/499: loss=5.310824008483095, w0=72.65555555555561, w1=15.948653548725781\n",
      "SubSGD iter. 403/499: loss=5.3107594591720835, w0=72.6518518518519, w1=15.938629946466603\n",
      "SubSGD iter. 404/499: loss=5.310733565162481, w0=72.6407407407408, w1=15.950353555594987\n",
      "SubSGD iter. 405/499: loss=5.310667372648022, w0=72.6444444444445, w1=15.936144238250304\n",
      "SubSGD iter. 406/499: loss=5.310747599686768, w0=72.6407407407408, w1=15.933378904549988\n",
      "SubSGD iter. 407/499: loss=5.3107632130008025, w0=72.63703703703709, w1=15.952820103607191\n",
      "SubSGD iter. 408/499: loss=5.310653446303908, w0=72.6481481481482, w1=15.94115886160311\n",
      "SubSGD iter. 409/499: loss=5.310719286688173, w0=72.6592592592593, w1=15.937558604444645\n",
      "SubSGD iter. 410/499: loss=5.310762430993413, w0=72.6703703703704, w1=15.94118483280231\n",
      "SubSGD iter. 411/499: loss=5.3108834558849916, w0=72.6592592592593, w1=15.949771084793788\n",
      "SubSGD iter. 412/499: loss=5.3107995237817756, w0=72.62592592592597, w1=15.97115797812425\n",
      "SubSGD iter. 413/499: loss=5.310588023135346, w0=72.60740740740745, w1=15.964358336225661\n",
      "SubSGD iter. 414/499: loss=5.310959678909393, w0=72.63333333333338, w1=15.974043995120107\n",
      "SubSGD iter. 415/499: loss=5.310616555202223, w0=72.64444444444449, w1=15.974489827110391\n",
      "SubSGD iter. 416/499: loss=5.310727920322324, w0=72.6481481481482, w1=15.982887184087643\n",
      "SubSGD iter. 417/499: loss=5.310790095825837, w0=72.61481481481486, w1=16.00349742188083\n",
      "SubSGD iter. 418/499: loss=5.310657422062591, w0=72.61111111111116, w1=16.007596299979735\n",
      "SubSGD iter. 419/499: loss=5.310697123113678, w0=72.60740740740745, w1=15.999062385148514\n",
      "SubSGD iter. 420/499: loss=5.310727483456343, w0=72.61111111111116, w1=15.985655698088628\n",
      "SubSGD iter. 421/499: loss=5.310717152627727, w0=72.62222222222226, w1=15.98952049940053\n",
      "SubSGD iter. 422/499: loss=5.310614787838664, w0=72.61851851851856, w1=15.988857466321742\n",
      "SubSGD iter. 423/499: loss=5.310613263183944, w0=72.62222222222226, w1=16.00332291462359\n",
      "SubSGD iter. 424/499: loss=5.310646526703864, w0=72.62592592592597, w1=16.0256680948427\n",
      "SubSGD iter. 425/499: loss=5.310700011659622, w0=72.63703703703708, w1=16.027280127509073\n",
      "SubSGD iter. 426/499: loss=5.310814918864017, w0=72.61851851851856, w1=16.015041400872704\n",
      "SubSGD iter. 427/499: loss=5.310673473542473, w0=72.60000000000004, w1=16.009479188315936\n",
      "SubSGD iter. 428/499: loss=5.3108126005531515, w0=72.58888888888893, w1=15.996564512009286\n",
      "SubSGD iter. 429/499: loss=5.311147205894342, w0=72.60000000000004, w1=15.994586479894243\n",
      "SubSGD iter. 430/499: loss=5.310866228762765, w0=72.59629629629633, w1=16.031495752712885\n",
      "SubSGD iter. 431/499: loss=5.310966007156657, w0=72.60740740740744, w1=16.009723596026095\n",
      "SubSGD iter. 432/499: loss=5.310735366373667, w0=72.62592592592596, w1=16.02805685947861\n",
      "SubSGD iter. 433/499: loss=5.310707267019595, w0=72.60740740740744, w1=16.04275491288751\n",
      "SubSGD iter. 434/499: loss=5.310786930471031, w0=72.63333333333337, w1=16.040785766247083\n",
      "SubSGD iter. 435/499: loss=5.310819269009784, w0=72.62962962962966, w1=16.039821922053445\n",
      "SubSGD iter. 436/499: loss=5.31077967120618, w0=72.63333333333337, w1=16.047914981971477\n",
      "SubSGD iter. 437/499: loss=5.310840922472676, w0=72.63703703703708, w1=16.040378200431594\n",
      "SubSGD iter. 438/499: loss=5.310854701449781, w0=72.62592592592597, w1=16.062227558591594\n",
      "SubSGD iter. 439/499: loss=5.310811053185399, w0=72.62222222222226, w1=16.05695602959403\n",
      "SubSGD iter. 440/499: loss=5.310769856873175, w0=72.63333333333337, w1=16.056518581594446\n",
      "SubSGD iter. 441/499: loss=5.310867054060269, w0=72.62222222222226, w1=16.053547387768432\n",
      "SubSGD iter. 442/499: loss=5.310762018649092, w0=72.60370370370374, w1=16.059812720279563\n",
      "SubSGD iter. 443/499: loss=5.310922303946643, w0=72.62222222222226, w1=16.05446745544415\n",
      "SubSGD iter. 444/499: loss=5.310764134358803, w0=72.64074074074078, w1=16.057525383484858\n",
      "SubSGD iter. 445/499: loss=5.310943452672361, w0=72.65185185185189, w1=16.05774439450586\n",
      "SubSGD iter. 446/499: loss=5.311054128872438, w0=72.61851851851856, w1=16.04856674761215\n",
      "SubSGD iter. 447/499: loss=5.310750565590769, w0=72.62962962962966, w1=16.043889429924164\n",
      "SubSGD iter. 448/499: loss=5.310792025388483, w0=72.62592592592596, w1=16.036247528296332\n",
      "SubSGD iter. 449/499: loss=5.310732144418479, w0=72.62222222222225, w1=16.021051763032567\n",
      "SubSGD iter. 450/499: loss=5.310687294462313, w0=72.64814814814818, w1=16.011483636322627\n",
      "SubSGD iter. 451/499: loss=5.310876951412937, w0=72.63703703703708, w1=16.01911641549366\n",
      "SubSGD iter. 452/499: loss=5.310790123340636, w0=72.63333333333337, w1=16.025504180403143\n",
      "SubSGD iter. 453/499: loss=5.310772854472087, w0=72.61481481481485, w1=16.032584504418825\n",
      "SubSGD iter. 454/499: loss=5.310713814177764, w0=72.59629629629633, w1=16.043979636729237\n",
      "SubSGD iter. 455/499: loss=5.311011406331658, w0=72.60740740740744, w1=16.080850788938836\n",
      "SubSGD iter. 456/499: loss=5.310962316093675, w0=72.62592592592596, w1=16.074037440870462\n",
      "SubSGD iter. 457/499: loss=5.310846923168516, w0=72.61481481481485, w1=16.10037626201398\n",
      "SubSGD iter. 458/499: loss=5.3109904441599545, w0=72.63333333333337, w1=16.10193238700506\n",
      "SubSGD iter. 459/499: loss=5.311004988748773, w0=72.65185185185189, w1=16.09363193162077\n",
      "SubSGD iter. 460/499: loss=5.311163129565253, w0=72.63333333333337, w1=16.122990987875273\n",
      "SubSGD iter. 461/499: loss=5.311071859715857, w0=72.62222222222226, w1=16.1373099354637\n",
      "SubSGD iter. 462/499: loss=5.311276604640478, w0=72.60370370370374, w1=16.119671394520342\n",
      "SubSGD iter. 463/499: loss=5.311348904958133, w0=72.60000000000004, w1=16.11741303164306\n",
      "SubSGD iter. 464/499: loss=5.311390671322341, w0=72.60370370370374, w1=16.112140046514234\n",
      "SubSGD iter. 465/499: loss=5.311243608742106, w0=72.61481481481485, w1=16.121147547218026\n",
      "SubSGD iter. 466/499: loss=5.311149521132951, w0=72.64814814814818, w1=16.126456623726064\n",
      "SubSGD iter. 467/499: loss=5.311226157189914, w0=72.62962962962966, w1=16.11549673299443\n",
      "SubSGD iter. 468/499: loss=5.3110273850127525, w0=72.64074074074077, w1=16.12820549252441\n",
      "SubSGD iter. 469/499: loss=5.311158128336195, w0=72.62222222222225, w1=16.15027288088177\n",
      "SubSGD iter. 470/499: loss=5.311467404010365, w0=72.61851851851854, w1=16.14507182381993\n",
      "SubSGD iter. 471/499: loss=5.311427520877383, w0=72.63703703703706, w1=16.122373439127202\n",
      "SubSGD iter. 472/499: loss=5.311103744391812, w0=72.62592592592596, w1=16.112348965460733\n",
      "SubSGD iter. 473/499: loss=5.311008704566368, w0=72.62962962962966, w1=16.092136813320483\n",
      "SubSGD iter. 474/499: loss=5.310938566462057, w0=72.61851851851856, w1=16.08491400142621\n",
      "SubSGD iter. 475/499: loss=5.310873420536188, w0=72.61481481481485, w1=16.060056908620638\n",
      "SubSGD iter. 476/499: loss=5.310780915114697, w0=72.62592592592596, w1=16.054278397918775\n",
      "SubSGD iter. 477/499: loss=5.310786909315679, w0=72.62962962962966, w1=16.05811439964355\n",
      "SubSGD iter. 478/499: loss=5.310835230681445, w0=72.61851851851856, w1=16.05822282786491\n",
      "SubSGD iter. 479/499: loss=5.310772769895202, w0=72.59259259259262, w1=16.04960646846288\n",
      "SubSGD iter. 480/499: loss=5.31110520966262, w0=72.63333333333337, w1=16.06717137959935\n",
      "SubSGD iter. 481/499: loss=5.310899409648206, w0=72.64444444444447, w1=16.07072858119997\n",
      "SubSGD iter. 482/499: loss=5.311020224885821, w0=72.63333333333337, w1=16.061133512480758\n",
      "SubSGD iter. 483/499: loss=5.31088107092251, w0=72.62962962962966, w1=16.052459897211335\n",
      "SubSGD iter. 484/499: loss=5.310818056343716, w0=72.64074074074077, w1=16.064913947039752\n",
      "SubSGD iter. 485/499: loss=5.310965893848633, w0=72.65185185185187, w1=16.050248719792297\n",
      "SubSGD iter. 486/499: loss=5.311031362369006, w0=72.64074074074077, w1=16.0460070914377\n",
      "SubSGD iter. 487/499: loss=5.310908468331997, w0=72.64444444444447, w1=16.055857650446203\n",
      "SubSGD iter. 488/499: loss=5.3109750576248365, w0=72.64814814814818, w1=16.04273024508725\n",
      "SubSGD iter. 489/499: loss=5.3109718562817925, w0=72.62962962962966, w1=16.022306017176184\n",
      "SubSGD iter. 490/499: loss=5.310726470403885, w0=72.64814814814818, w1=16.039152775602027\n",
      "SubSGD iter. 491/499: loss=5.310960990485927, w0=72.65185185185189, w1=16.04549175754134\n",
      "SubSGD iter. 492/499: loss=5.311016914116975, w0=72.64814814814818, w1=16.03218076770207\n",
      "SubSGD iter. 493/499: loss=5.3109398145080675, w0=72.62962962962966, w1=16.026582587454367\n",
      "SubSGD iter. 494/499: loss=5.3107394595683886, w0=72.62592592592596, w1=16.044746595524774\n",
      "SubSGD iter. 495/499: loss=5.3107579585113545, w0=72.62222222222225, w1=16.0438103759553\n",
      "SubSGD iter. 496/499: loss=5.310739628241296, w0=72.62592592592596, w1=16.03122217995136\n",
      "SubSGD iter. 497/499: loss=5.310716881001279, w0=72.62222222222225, w1=16.023517692471092\n",
      "SubSGD iter. 498/499: loss=5.310692964904824, w0=72.61111111111114, w1=16.01982738306915\n",
      "SubSGD iter. 499/499: loss=5.310706166797166, w0=72.60740740740744, w1=15.986277491347003\n",
      "SubSGD: execution time=0.602 seconds\n"
     ]
    }
   ],
   "source": [
    "# Define the parameters of the algorithm.\n",
    "max_iters = 500\n",
    "gamma = 0.7\n",
    "batch_size = 1\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.array([0, 0])\n",
    "\n",
    "# Start SubSGD.\n",
    "start_time = datetime.datetime.now()\n",
    "subsgd_losses, subsgd_ws = stochastic_subgradient_descent(\n",
    "    y, tx, w_initial, batch_size, max_iters, gamma)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"SubSGD: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19de7cec84d34f9f8b54948c3ba63774",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='n_iter', max=501, min=1), Output()), _dom_classes=('widg"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_figure(n_iter)>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ipywidgets import IntSlider, interact\n",
    "def plot_figure(n_iter):\n",
    "    fig = gradient_descent_visualization(\n",
    "        subsgd_losses, subsgd_ws, grid_losses, grid_w0, grid_w1, mean_x, std_x, height, weight, n_iter)\n",
    "    fig.set_size_inches(10.0, 6.0)\n",
    "\n",
    "interact(plot_figure, n_iter=IntSlider(min=1, max=len(subsgd_ws)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "vscode": {
   "interpreter": {
    "hash": "220387e6c3d14f2586cf2004f001028ce90f312409fe8a3fd0eb443ac44e4308"
   }
  },
  "widgets": {
   "state": {
    "d2b2c3aea192430e81437f33ba0b0e69": {
     "views": [
      {
       "cell_index": 22
      }
     ]
    },
    "e4a6a7a70ccd42ddb112989c04f2ed3f": {
     "views": [
      {
       "cell_index": 18
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

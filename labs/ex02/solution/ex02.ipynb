{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from helpers import *\n",
    "\n",
    "height, weight, gender = load_data(sub_sample=False, add_outlier=False)\n",
    "x, mean_x, std_x = standardize(height)\n",
    "y, tx = build_model_data(x, weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10000,), (10000, 2))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape, tx.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NB: throughout this laboratory the data has the following format: \n",
    "  * there are **N = 10000** data entries\n",
    "  * **y** represents the column vector containing weight information -- that which we wish to predict/the output (see also the first page of $\\texttt{exercise02.pdf}$). Its **shape** is **(N,)**.\n",
    "  * **tx** represents the matrix $\\tilde{X}$ formed by laterally concatenating a column vector of 1s to the column vector of height information -- the input data (see also the first page of $\\texttt{exercise02.pdf}$). Its **shape** is **(N,2)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Computing the Cost Function\n",
    "Fill in the `compute_cost` function below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SOLUTION\n",
    "def calculate_mse(e):\n",
    "    \"\"\"Calculate the mse for vector e.\"\"\"\n",
    "    return 1/2*np.mean(e**2)\n",
    "\n",
    "\n",
    "def calculate_mae(e):\n",
    "    \"\"\"Calculate the mae for vector e.\"\"\"\n",
    "    return np.mean(np.abs(e))\n",
    "### TEMPLATE\n",
    "### END SOLUTION\n",
    "\n",
    "def compute_loss(y, tx, w):\n",
    "\n",
    "    \"\"\"Calculate the loss using either MSE or MAE.\n",
    "\n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        w: numpy array of shape=(2,). The vector of model parameters.\n",
    "\n",
    "    Returns:\n",
    "        the value of the loss (a scalar), corresponding to the input parameters w.\n",
    "    \"\"\"\n",
    "    ### SOLUTION\n",
    "    e = y - tx.dot(w)\n",
    "    return calculate_mse(e)\n",
    "    ### TEMPLATE\n",
    "    # # ***************************************************\n",
    "    # # INSERT YOUR CODE HERE\n",
    "    # # TODO: compute loss by MSE\n",
    "    # # ***************************************************\n",
    "    # raise NotImplementedError\n",
    "    ### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Grid Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill in the function `grid_search()` below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from costs import *\n",
    "\n",
    "def grid_search(y, tx, grid_w0, grid_w1):\n",
    "    \"\"\"Algorithm for grid search.\n",
    "        \n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        grid_w0: numpy array of shape=(num_grid_pts_w0, ). A 1D array containing num_grid_pts_w0 values of parameter w0 to be tested in the grid search.\n",
    "        grid_w1: numpy array of shape=(num_grid_pts_w1, ). A 1D array containing num_grid_pts_w1 values of parameter w1 to be tested in the grid search.\n",
    "        \n",
    "    Returns:\n",
    "        losses: numpy array of shape=(num_grid_pts_w0, num_grid_pts_w1). A 2D array containing the loss value for each combination of w0 and w1\n",
    "    \"\"\"\n",
    "\n",
    "    losses = np.zeros((len(grid_w0), len(grid_w1)))\n",
    "    ### SOLUTION\n",
    "    # compute loss for each combinationof w0 and w1.\n",
    "    for ind_row, row in enumerate(grid_w0):\n",
    "        for ind_col, col in enumerate(grid_w1):\n",
    "            w = np.array([row, col])\n",
    "            losses[ind_row, ind_col] = compute_loss(y, tx, w)\n",
    "    ### TEMPLATE\n",
    "    # # ***************************************************\n",
    "    # # INSERT YOUR CODE HERE\n",
    "    # # TODO: compute loss for each combination of w0 and w1.\n",
    "    # # ***************************************************\n",
    "    # raise NotImplementedError\n",
    "    ### END SOLUTION\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us play with the grid search demo now!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid Search: loss*=42.42448314678248, w0*=66.66666666666669, w1*=16.666666666666686, execution time=0.032 seconds\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x432 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from grid_search import generate_w, get_best_parameters\n",
    "from plots import grid_visualization\n",
    "\n",
    "# Generate the grid of parameters to be swept\n",
    "grid_w0, grid_w1 = generate_w(num_intervals=10)\n",
    "\n",
    "# Start the grid search\n",
    "start_time = datetime.datetime.now()\n",
    "grid_losses = grid_search(y, tx, grid_w0, grid_w1)\n",
    "\n",
    "# Select the best combinaison\n",
    "loss_star, w0_star, w1_star = get_best_parameters(grid_w0, grid_w1, grid_losses)\n",
    "end_time = datetime.datetime.now()\n",
    "execution_time = (end_time - start_time).total_seconds()\n",
    "\n",
    "# Print the results\n",
    "print(\"Grid Search: loss*={l}, w0*={w0}, w1*={w1}, execution time={t:.3f} seconds\".format(\n",
    "      l=loss_star, w0=w0_star, w1=w1_star, t=execution_time))\n",
    "\n",
    "# Plot the results\n",
    "fig = grid_visualization(grid_losses, grid_w0, grid_w1, mean_x, std_x, height, weight)\n",
    "fig.set_size_inches(10.0,6.0)\n",
    "fig.savefig(\"grid_plot\")  # Optional saving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, please fill in the functions `compute_gradient` below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(y, tx, w):\n",
    "    \"\"\"Computes the gradient at w.\n",
    "        \n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        w: numpy array of shape=(2, ). The vector of model parameters.\n",
    "        \n",
    "    Returns:\n",
    "        An numpy array of shape (2, ) (same shape as w), containing the gradient of the loss at w.\n",
    "    \"\"\"\n",
    "    ### SOLUTION\n",
    "    err = y - tx.dot(w)\n",
    "    grad = -tx.T.dot(err) / len(err)\n",
    "    return grad, err\n",
    "    \n",
    "    ### TEMPLATE\n",
    "    # # ***************************************************\n",
    "    # # INSERT YOUR CODE HERE\n",
    "    # # TODO: compute gradient vector\n",
    "    # # ***************************************************\n",
    "    # raise NotImplementedError\n",
    "    ### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please fill in the functions `gradient_descent` below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(y, tx, initial_w, max_iters, gamma):\n",
    "    \"\"\"The Gradient Descent (GD) algorithm.\n",
    "        \n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        initial_w: numpy array of shape=(2, ). The initial guess (or the initialization) for the model parameters\n",
    "        max_iters: a scalar denoting the total number of iterations of GD\n",
    "        gamma: a scalar denoting the stepsize\n",
    "        \n",
    "    Returns:\n",
    "        losses: a list of length max_iters containing the loss value (scalar) for each iteration of GD\n",
    "        ws: a list of length max_iters containing the model parameters as numpy arrays of shape (2, ), for each iteration of GD \n",
    "    \"\"\"\n",
    "    # Define parameters to store w and loss\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        ### SOLUTION\n",
    "        # compute loss, gradient\n",
    "        grad, err = compute_gradient(y, tx, w)\n",
    "        loss = calculate_mse(err)\n",
    "        # update w by gradient descent\n",
    "        w = w - gamma * grad\n",
    "        \n",
    "        ### TEMPLATE\n",
    "        # # ***************************************************\n",
    "        # # INSERT YOUR CODE HERE\n",
    "        # # TODO: compute gradient and loss\n",
    "        # # ***************************************************\n",
    "        # raise NotImplementedError\n",
    "        # # ***************************************************\n",
    "        # # INSERT YOUR CODE HERE\n",
    "        # # TODO: update w by gradient\n",
    "        # # ***************************************************\n",
    "        # raise NotImplementedError\n",
    "        ### END SOLUTION\n",
    "        \n",
    "        # store w and loss\n",
    "        ws.append(w)\n",
    "        losses.append(loss)\n",
    "        print(\"GD iter. {bi}/{ti}: loss={l}, w0={w0}, w1={w1}\".format(\n",
    "              bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]))\n",
    "\n",
    "    return losses, ws"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your gradient descent function through gradient descent demo shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GD iter. 0/49: loss=2792.2367127591674, w0=51.30574540147368, w1=9.435798704492347\n",
      "GD iter. 1/49: loss=265.3024621089587, w0=66.69746902191575, w1=12.266538315840018\n",
      "GD iter. 2/49: loss=37.87837955044096, w0=71.31498610804836, w1=13.115760199244335\n",
      "GD iter. 3/49: loss=17.41021212017444, w0=72.70024123388815, w1=13.370526764265632\n",
      "GD iter. 4/49: loss=15.568077051450448, w0=73.11581777164008, w1=13.446956733772025\n",
      "GD iter. 5/49: loss=15.402284895265295, w0=73.24049073296565, w1=13.469885724623941\n",
      "GD iter. 6/49: loss=15.38736360120863, w0=73.27789262136334, w1=13.476764421879516\n",
      "GD iter. 7/49: loss=15.38602068474353, w0=73.28911318788263, w1=13.478828031056189\n",
      "GD iter. 8/49: loss=15.385899822261674, w0=73.29247935783842, w1=13.47944711380919\n",
      "GD iter. 9/49: loss=15.385888944638305, w0=73.29348920882515, w1=13.47963283863509\n",
      "GD iter. 10/49: loss=15.3858879656522, w0=73.29379216412117, w1=13.479688556082861\n",
      "GD iter. 11/49: loss=15.385887877543452, w0=73.29388305071, w1=13.479705271317192\n",
      "GD iter. 12/49: loss=15.385887869613667, w0=73.29391031668663, w1=13.479710285887492\n",
      "GD iter. 13/49: loss=15.385887868899983, w0=73.29391849647962, w1=13.479711790258582\n",
      "GD iter. 14/49: loss=15.38588786883575, w0=73.29392095041752, w1=13.479712241569908\n",
      "GD iter. 15/49: loss=15.385887868829974, w0=73.29392168659889, w1=13.479712376963306\n",
      "GD iter. 16/49: loss=15.38588786882945, w0=73.2939219074533, w1=13.479712417581325\n",
      "GD iter. 17/49: loss=15.385887868829403, w0=73.29392197370963, w1=13.479712429766732\n",
      "GD iter. 18/49: loss=15.3858878688294, w0=73.29392199358652, w1=13.479712433422353\n",
      "GD iter. 19/49: loss=15.385887868829403, w0=73.2939219995496, w1=13.47971243451904\n",
      "GD iter. 20/49: loss=15.385887868829398, w0=73.29392200133852, w1=13.479712434848047\n",
      "GD iter. 21/49: loss=15.3858878688294, w0=73.29392200187519, w1=13.479712434946748\n",
      "GD iter. 22/49: loss=15.3858878688294, w0=73.2939220020362, w1=13.479712434976358\n",
      "GD iter. 23/49: loss=15.3858878688294, w0=73.29392200208449, w1=13.479712434985242\n",
      "GD iter. 24/49: loss=15.3858878688294, w0=73.29392200209898, w1=13.479712434987906\n",
      "GD iter. 25/49: loss=15.385887868829398, w0=73.29392200210333, w1=13.479712434988706\n",
      "GD iter. 26/49: loss=15.3858878688294, w0=73.29392200210464, w1=13.479712434988945\n",
      "GD iter. 27/49: loss=15.3858878688294, w0=73.29392200210502, w1=13.479712434989018\n",
      "GD iter. 28/49: loss=15.3858878688294, w0=73.29392200210513, w1=13.47971243498904\n",
      "GD iter. 29/49: loss=15.3858878688294, w0=73.29392200210518, w1=13.479712434989047\n",
      "GD iter. 30/49: loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 31/49: loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 32/49: loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 33/49: loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 34/49: loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 35/49: loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 36/49: loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 37/49: loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 38/49: loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 39/49: loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 40/49: loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 41/49: loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 42/49: loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 43/49: loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 44/49: loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 45/49: loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 46/49: loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 47/49: loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 48/49: loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 49/49: loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD: execution time=0.012 seconds\n"
     ]
    }
   ],
   "source": [
    "# from gradient_descent import *\n",
    "from plots import gradient_descent_visualization\n",
    "\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters = 50\n",
    "gamma = 0.7\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.array([0, 0])\n",
    "\n",
    "# Start gradient descent.\n",
    "start_time = datetime.datetime.now()\n",
    "gd_losses, gd_ws = gradient_descent(y, tx, w_initial, max_iters, gamma)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"GD: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7acda657d3e34d39920366d53dc1d6f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='n_iter', max=51, min=1), Output()), _dom_classes=('widge"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_figure(n_iter)>"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Time Visualization\n",
    "from ipywidgets import IntSlider, interact\n",
    "def plot_figure(n_iter):\n",
    "    fig = gradient_descent_visualization(\n",
    "        gd_losses, gd_ws, grid_losses, grid_w0, grid_w1, mean_x, std_x, height, weight, n_iter)\n",
    "    fig.set_size_inches(10.0, 6.0)\n",
    "\n",
    "interact(plot_figure, n_iter=IntSlider(min=1, max=len(gd_ws)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 4. Stochastic gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_stoch_gradient(y, tx, w):\n",
    "    \"\"\"Compute a stochastic gradient at w from just few examples n and their corresponding y_n labels.\n",
    "        \n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        w: numpy array of shape=(2, ). The vector of model parameters.\n",
    "        \n",
    "    Returns:\n",
    "        A numpy array of shape (2, ) (same shape as w), containing the stochastic gradient of the loss at w.\n",
    "    \"\"\"\n",
    "    \n",
    "    ### SOLUTION\n",
    "    err = y - tx.dot(w)\n",
    "    grad = -tx.T.dot(err) / len(err)\n",
    "    return grad, err\n",
    "\n",
    "    ### TEMPLATE\n",
    "    # # ***************************************************\n",
    "    # # INSERT YOUR CODE HERE\n",
    "    # # TODO: implement stochastic gradient computation. It's the same as the usual gradient.\n",
    "    # # ***************************************************\n",
    "    # raise NotImplementedError\n",
    "    ### END SOLUTION\n",
    "\n",
    "\n",
    "def stochastic_gradient_descent(y, tx, initial_w, batch_size, max_iters, gamma):\n",
    "    \"\"\"The Stochastic Gradient Descent algorithm (SGD).\n",
    "            \n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        initial_w: numpy array of shape=(2, ). The initial guess (or the initialization) for the model parameters\n",
    "        batch_size: a scalar denoting the number of data points in a mini-batch used for computing the stochastic gradient\n",
    "        max_iters: a scalar denoting the total number of iterations of SGD\n",
    "        gamma: a scalar denoting the stepsize\n",
    "        \n",
    "    Returns:\n",
    "        losses: a list of length max_iters containing the loss value (scalar) for each iteration of SGD\n",
    "        ws: a list of length max_iters containing the model parameters as numpy arrays of shape (2, ), for each iteration of SGD \n",
    "    \"\"\"\n",
    "    \n",
    "    # Define parameters to store w and loss\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    \n",
    "    for n_iter in range(max_iters):\n",
    "        ### SOLUTION\n",
    "        for y_batch, tx_batch in batch_iter(y, tx, batch_size=batch_size, num_batches=1):\n",
    "            # compute a stochastic gradient and loss\n",
    "            grad, _ = compute_stoch_gradient(y_batch, tx_batch, w)\n",
    "            # update w through the stochastic gradient update\n",
    "            w = w - gamma * grad\n",
    "            # calculate loss\n",
    "            loss = compute_loss(y, tx, w)\n",
    "            # store w and loss\n",
    "            ws.append(w)\n",
    "            losses.append(loss)\n",
    "            \n",
    "        ### TEMPLATE\n",
    "        # # ***************************************************\n",
    "        # # INSERT YOUR CODE HERE\n",
    "        # # TODO: implement stochastic gradient descent.\n",
    "        # # ***************************************************\n",
    "        # raise NotImplementedError\n",
    "        ### END SOLUTION\n",
    "\n",
    "        print(\"SGD iter. {bi}/{ti}: loss={l}, w0={w0}, w1={w1}\".format(\n",
    "              bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]))\n",
    "    return losses, ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD iter. 0/49: loss=2382.1923426902736, w0=5.98196950413687, w1=-0.7580528585584875\n",
      "SGD iter. 1/49: loss=1828.7350117433746, w0=13.739118465918366, w1=4.53971111981686\n",
      "SGD iter. 2/49: loss=1496.9744642972573, w0=19.3167026262254, w1=6.434363522053372\n",
      "SGD iter. 3/49: loss=1178.256706922229, w0=26.728591637717983, w1=26.026092238034884\n",
      "SGD iter. 4/49: loss=841.0833424876338, w0=32.852558758353126, w1=17.466070199227975\n",
      "SGD iter. 5/49: loss=749.2796639205893, w0=35.70133767335279, w1=20.867889178215954\n",
      "SGD iter. 6/49: loss=531.1771118178656, w0=41.313595535962556, w1=10.506302436853984\n",
      "SGD iter. 7/49: loss=431.81303085429835, w0=44.498651112565135, w1=15.399780238204546\n",
      "SGD iter. 8/49: loss=374.2549705624242, w0=47.24183339098254, w1=19.726859334606736\n",
      "SGD iter. 9/49: loss=267.3111877880608, w0=50.96131964562683, w1=15.739241550951029\n",
      "SGD iter. 10/49: loss=230.07062390443002, w0=52.59208757137665, w1=14.376107032197044\n",
      "SGD iter. 11/49: loss=191.75397296072055, w0=54.654633551284114, w1=15.784727886319097\n",
      "SGD iter. 12/49: loss=170.81362594619856, w0=56.01793309071532, w1=17.00046282209012\n",
      "SGD iter. 13/49: loss=127.75683234301606, w0=58.62230483076636, w1=16.55957258214651\n",
      "SGD iter. 14/49: loss=96.83099310450724, w0=60.582018632158565, w1=14.618888972886175\n",
      "SGD iter. 15/49: loss=81.45122251572171, w0=61.825785101580365, w1=14.262339628872247\n",
      "SGD iter. 16/49: loss=65.19267063398746, w0=63.36509481001321, w1=14.495564845475446\n",
      "SGD iter. 17/49: loss=65.89344924994349, w0=63.2968143392549, w1=14.515550810223418\n",
      "SGD iter. 18/49: loss=50.25550746545339, w0=65.2165940253875, w1=11.359332292235957\n",
      "SGD iter. 19/49: loss=36.57404743601873, w0=67.13489336042817, w1=11.371944581220335\n",
      "SGD iter. 20/49: loss=38.354406496265604, w0=66.8335396575567, w1=11.430200976831621\n",
      "SGD iter. 21/49: loss=33.148840214852314, w0=67.36468991961941, w1=12.871344450897881\n",
      "SGD iter. 22/49: loss=25.718195606342046, w0=68.78418086068525, w1=14.05142067562381\n",
      "SGD iter. 23/49: loss=22.2201808965477, w0=69.60574113980304, w1=13.22298693492175\n",
      "SGD iter. 24/49: loss=18.568753910725988, w0=70.9198584978398, w1=12.625572773394818\n",
      "SGD iter. 25/49: loss=18.1835932812984, w0=71.02521488080077, w1=12.81010148624296\n",
      "SGD iter. 26/49: loss=16.651363243828172, w0=71.71509334272253, w1=13.284134281021181\n",
      "SGD iter. 27/49: loss=16.473323167588454, w0=71.81917787125035, w1=13.47912419413401\n",
      "SGD iter. 28/49: loss=15.824258514419977, w0=72.95803072807813, w1=14.353736658459788\n",
      "SGD iter. 29/49: loss=16.30710304967142, w0=72.72622120164178, w1=14.712654511015833\n",
      "SGD iter. 30/49: loss=16.432672530277515, w0=72.86227582445103, w1=14.860744983560146\n",
      "SGD iter. 31/49: loss=15.993982837146998, w0=73.61032665800286, w1=14.536158377932402\n",
      "SGD iter. 32/49: loss=16.198378618275548, w0=74.5678402919752, w1=13.52568731537999\n",
      "SGD iter. 33/49: loss=16.245273135691072, w0=74.60150795462133, w1=13.574525464187627\n",
      "SGD iter. 34/49: loss=16.194305546550073, w0=74.56437520299745, w1=13.53247624629892\n",
      "SGD iter. 35/49: loss=18.73283021825412, w0=75.88024336597056, w1=13.549185463302349\n",
      "SGD iter. 36/49: loss=17.553817181126607, w0=75.19482603756285, w1=12.629758040580029\n",
      "SGD iter. 37/49: loss=16.41677394692107, w0=74.71359949182632, w1=13.264566016560895\n",
      "SGD iter. 38/49: loss=18.160243927267338, w0=75.60414510214684, w1=13.939692157270153\n",
      "SGD iter. 39/49: loss=18.14394649559831, w0=75.5981670769599, w1=13.934214237565948\n",
      "SGD iter. 40/49: loss=16.345120934408587, w0=74.31395603776075, w1=14.416727214628416\n",
      "SGD iter. 41/49: loss=17.11706178724607, w0=74.61283977812657, w1=14.792268619282945\n",
      "SGD iter. 42/49: loss=17.22890016505535, w0=74.68043254684241, w1=14.807723391940987\n",
      "SGD iter. 43/49: loss=15.920822121809461, w0=73.40419448321744, w1=14.508161989352056\n",
      "SGD iter. 44/49: loss=15.919318875289653, w0=73.40679999208689, w1=14.506416176239755\n",
      "SGD iter. 45/49: loss=16.281337807204647, w0=72.56571272769222, w1=14.602481836696017\n",
      "SGD iter. 46/49: loss=16.106637567409958, w0=72.72221818049458, w1=14.535484255751914\n",
      "SGD iter. 47/49: loss=17.10118467948676, w0=73.66284809558795, w1=15.294784653624514\n",
      "SGD iter. 48/49: loss=17.584260411023863, w0=73.87266916082993, w1=15.495102424707408\n",
      "SGD iter. 49/49: loss=19.40335839459817, w0=72.21471082329153, w1=16.10082751889154\n",
      "SGD: execution time=0.030 seconds\n"
     ]
    }
   ],
   "source": [
    "# from stochastic_gradient_descent import *\n",
    "\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters = 50\n",
    "gamma = 0.1\n",
    "batch_size = 1\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.array([0, 0])\n",
    "\n",
    "# Start SGD.\n",
    "start_time = datetime.datetime.now()\n",
    "sgd_losses, sgd_ws = stochastic_gradient_descent(\n",
    "    y, tx, w_initial, batch_size, max_iters, gamma)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"SGD: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f48782fc7ecd4ce68a7820714bc5e262",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='n_iter', max=51, min=1), Output()), _dom_classes=('widge"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_figure(n_iter)>"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Time Visualization\n",
    "from ipywidgets import IntSlider, interact\n",
    "def plot_figure(n_iter):\n",
    "    fig = gradient_descent_visualization(\n",
    "        sgd_losses, sgd_ws, grid_losses, grid_w0, grid_w1, mean_x, std_x, height, weight, n_iter)\n",
    "    fig.set_size_inches(10.0, 6.0)\n",
    "\n",
    "interact(plot_figure, n_iter=IntSlider(min=1, max=len(sgd_ws)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Effect of Outliers and MAE Cost Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from helpers import *\n",
    "\n",
    "### SOLUTION\n",
    "height, weight, gender = load_data(sub_sample=True, add_outlier=True)\n",
    "### TEMPLATE\n",
    "## ***************************************************\n",
    "## INSERT YOUR CODE HERE\n",
    "## TODO: reload the data by subsampling first, then by subsampling and adding outliers\n",
    "## ***************************************************\n",
    "#raise NotImplementedError\n",
    "### END SOLUTION\n",
    "\n",
    "x, mean_x, std_x = standardize(height)\n",
    "y, tx = build_model_data(x, weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((202,), (202, 2))"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape, tx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GD iter. 0/49: loss=2869.8351145358524, w0=51.847464098448484, w1=7.724426406192441\n",
      "GD iter. 1/49: loss=318.282124701595, w0=67.401703327983, w1=10.041754328050121\n",
      "GD iter. 2/49: loss=88.6423556165126, w0=72.06797509684336, w1=10.736952704607413\n",
      "GD iter. 3/49: loss=67.97477639885521, w0=73.46785662750146, w1=10.945512217574594\n",
      "GD iter. 4/49: loss=66.11469426926604, w0=73.88782108669889, w1=11.00808007146475\n",
      "GD iter. 5/49: loss=65.94728687760302, w0=74.01381042445813, w1=11.026850427631796\n",
      "GD iter. 6/49: loss=65.93222021235334, w0=74.05160722578589, w1=11.03248153448191\n",
      "GD iter. 7/49: loss=65.93086421248087, w0=74.06294626618423, w1=11.034170866536943\n",
      "GD iter. 8/49: loss=65.93074217249236, w0=74.06634797830372, w1=11.034677666153454\n",
      "GD iter. 9/49: loss=65.93073118889338, w0=74.06736849193958, w1=11.034829706038407\n",
      "GD iter. 10/49: loss=65.93073020036948, w0=74.06767464603033, w1=11.034875318003893\n",
      "GD iter. 11/49: loss=65.93073011140233, w0=74.06776649225756, w1=11.034889001593537\n",
      "GD iter. 12/49: loss=65.93073010339529, w0=74.06779404612573, w1=11.034893106670431\n",
      "GD iter. 13/49: loss=65.93073010267466, w0=74.06780231228618, w1=11.034894338193501\n",
      "GD iter. 14/49: loss=65.93073010260979, w0=74.06780479213431, w1=11.034894707650421\n",
      "GD iter. 15/49: loss=65.93073010260395, w0=74.06780553608876, w1=11.034894818487498\n",
      "GD iter. 16/49: loss=65.93073010260342, w0=74.06780575927509, w1=11.03489485173862\n",
      "GD iter. 17/49: loss=65.93073010260338, w0=74.06780582623098, w1=11.034894861713957\n",
      "GD iter. 18/49: loss=65.93073010260338, w0=74.06780584631775, w1=11.034894864706558\n",
      "GD iter. 19/49: loss=65.93073010260339, w0=74.06780585234378, w1=11.034894865604338\n",
      "GD iter. 20/49: loss=65.93073010260338, w0=74.06780585415159, w1=11.034894865873671\n",
      "GD iter. 21/49: loss=65.93073010260338, w0=74.06780585469393, w1=11.03489486595447\n",
      "GD iter. 22/49: loss=65.93073010260336, w0=74.06780585485663, w1=11.034894865978712\n",
      "GD iter. 23/49: loss=65.93073010260336, w0=74.06780585490544, w1=11.034894865985985\n",
      "GD iter. 24/49: loss=65.93073010260338, w0=74.0678058549201, w1=11.034894865988164\n",
      "GD iter. 25/49: loss=65.93073010260338, w0=74.06780585492449, w1=11.034894865988818\n",
      "GD iter. 26/49: loss=65.93073010260338, w0=74.06780585492581, w1=11.034894865989017\n",
      "GD iter. 27/49: loss=65.93073010260336, w0=74.06780585492619, w1=11.034894865989077\n",
      "GD iter. 28/49: loss=65.93073010260338, w0=74.06780585492632, w1=11.034894865989095\n",
      "GD iter. 29/49: loss=65.93073010260338, w0=74.06780585492635, w1=11.034894865989099\n",
      "GD iter. 30/49: loss=65.93073010260338, w0=74.06780585492636, w1=11.034894865989102\n",
      "GD iter. 31/49: loss=65.93073010260338, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 32/49: loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 33/49: loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 34/49: loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 35/49: loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 36/49: loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 37/49: loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 38/49: loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 39/49: loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 40/49: loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 41/49: loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 42/49: loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 43/49: loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 44/49: loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 45/49: loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 46/49: loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 47/49: loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 48/49: loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 49/49: loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD: execution time=0.004 seconds\n"
     ]
    }
   ],
   "source": [
    "from plots import gradient_descent_visualization\n",
    "\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters = 50\n",
    "gamma = 0.7\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.array([0, 0])\n",
    "\n",
    "# Start gradient descent.\n",
    "start_time = datetime.datetime.now()\n",
    "\n",
    "### SOLUTION\n",
    "gd_losses, gd_ws = gradient_descent(y, tx, w_initial, max_iters, gamma)\n",
    "### TEMPLATE\n",
    "# # ***************************************************\n",
    "# # INSERT YOUR CODE HERE\n",
    "# # TODO: fit the model to the subsampled data / subsampled data with outliers and visualize the cloud of points \n",
    "# #       and the model fit\n",
    "# # ***************************************************\n",
    "# raise NotImplementedError\n",
    "### END SOLUTION\n",
    "\n",
    "\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"GD: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "401d2c3a56d547d7971f947ef322ecb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='n_iter', max=51, min=1), Output()), _dom_classes=('widge"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_figure(n_iter)>"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Time Visualization\n",
    "from ipywidgets import IntSlider, interact\n",
    "def plot_figure(n_iter):\n",
    "    fig = gradient_descent_visualization(\n",
    "        gd_losses, gd_ws, grid_losses, grid_w0, grid_w1, mean_x, std_x, height, weight, n_iter)\n",
    "    fig.set_size_inches(10.0, 6.0)\n",
    "\n",
    "interact(plot_figure, n_iter=IntSlider(min=1, max=len(gd_ws)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 6. Subgradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_subgradient_mae(y, tx, w):\n",
    "    \"\"\"Compute a subgradient of the MAE at w.\n",
    "        \n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        w: numpy array of shape=(2, ). The vector of model parameters.\n",
    "        \n",
    "    Returns:\n",
    "        A numpy array of shape (2, ) (same shape as w), containing the subgradient of the MAE at w.\n",
    "    \"\"\"\n",
    "    ### SOLUTION\n",
    "    err = y - tx.dot(w)\n",
    "    grad = -np.dot(tx.T,np.sign(err)) / len(err)\n",
    "    return grad, err\n",
    "    ### TEMPLATE\n",
    "    # # ***************************************************\n",
    "    # # INSERT YOUR CODE HERE\n",
    "    # # TODO: compute subgradient gradient vector for MAE\n",
    "    # # ***************************************************\n",
    "    # raise NotImplementedError\n",
    "    ### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subgradient_descent(y, tx, initial_w, max_iters, gamma):\n",
    "    \"\"\"The SubGradient Descent (SubGD) algorithm.\n",
    "        \n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        initial_w: numpy array of shape=(2, ). The initial guess (or the initialization) for the model parameters\n",
    "        max_iters: a scalar denoting the total number of iterations of GD\n",
    "        gamma: a scalar denoting the stepsize\n",
    "        \n",
    "    Returns:\n",
    "        losses: a list of length max_iters containing the loss value (scalar) for each iteration of SubGD\n",
    "        ws: a list of length max_iters containing the model parameters as numpy arrays of shape (2, ), for each iteration of SubGD \n",
    "    \"\"\"\n",
    "    # Define parameters to store w and loss\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        ### SOLUTION\n",
    "        # compute loss, gradient\n",
    "        grad, err = compute_subgradient_mae(y, tx, w)\n",
    "        loss = calculate_mae(err)\n",
    "        # gradient w by descent update\n",
    "        w = w - gamma * grad\n",
    "        # store w and loss\n",
    "        \n",
    "        ### TEMPLATE\n",
    "        # # ***************************************************\n",
    "        # # INSERT YOUR CODE HERE\n",
    "        # # TODO: compute subgradient and loss\n",
    "        # # ***************************************************\n",
    "        # raise NotImplementedError\n",
    "        # # ***************************************************\n",
    "        # # INSERT YOUR CODE HERE\n",
    "        # # TODO: update w by subgradient\n",
    "        # # ***************************************************\n",
    "        # raise NotImplementedError\n",
    "        ### END SOLUTION\n",
    "        \n",
    "        ws.append(w)\n",
    "        losses.append(loss)\n",
    "        print(\"SubGD iter. {bi}/{ti}: loss={l}, w0={w0}, w1={w1}\".format(\n",
    "              bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]))\n",
    "\n",
    "    return losses, ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SubGD iter. 0/499: loss=74.06780585492638, w0=0.7, w1=8.756471895211877e-16\n",
      "SubGD iter. 1/499: loss=73.36780585492637, w0=1.4, w1=1.7512943790423754e-15\n",
      "SubGD iter. 2/499: loss=72.66780585492637, w0=2.0999999999999996, w1=2.626941568563563e-15\n",
      "SubGD iter. 3/499: loss=71.96780585492637, w0=2.8, w1=3.502588758084751e-15\n",
      "SubGD iter. 4/499: loss=71.26780585492638, w0=3.5, w1=4.378235947605939e-15\n",
      "SubGD iter. 5/499: loss=70.56780585492638, w0=4.2, w1=5.253883137127127e-15\n",
      "SubGD iter. 6/499: loss=69.86780585492637, w0=4.9, w1=6.1295303266483146e-15\n",
      "SubGD iter. 7/499: loss=69.16780585492639, w0=5.6000000000000005, w1=7.0051775161695025e-15\n",
      "SubGD iter. 8/499: loss=68.46780585492638, w0=6.300000000000001, w1=7.88082470569069e-15\n",
      "SubGD iter. 9/499: loss=67.76780585492638, w0=7.000000000000001, w1=8.756471895211878e-15\n",
      "SubGD iter. 10/499: loss=67.06780585492638, w0=7.700000000000001, w1=9.632119084733065e-15\n",
      "SubGD iter. 11/499: loss=66.36780585492637, w0=8.4, w1=1.0507766274254253e-14\n",
      "SubGD iter. 12/499: loss=65.66780585492639, w0=9.1, w1=1.1383413463775441e-14\n",
      "SubGD iter. 13/499: loss=64.96780585492637, w0=9.799999999999999, w1=1.2259060653296629e-14\n",
      "SubGD iter. 14/499: loss=64.26780585492638, w0=10.499999999999998, w1=1.3134707842817817e-14\n",
      "SubGD iter. 15/499: loss=63.567805854926384, w0=11.199999999999998, w1=1.4010355032339005e-14\n",
      "SubGD iter. 16/499: loss=62.86780585492639, w0=11.899999999999997, w1=1.488600222186019e-14\n",
      "SubGD iter. 17/499: loss=62.167805854926385, w0=12.599999999999996, w1=1.576164941138138e-14\n",
      "SubGD iter. 18/499: loss=61.46780585492638, w0=13.299999999999995, w1=1.6637296600902567e-14\n",
      "SubGD iter. 19/499: loss=60.767805854926394, w0=13.999999999999995, w1=1.7512943790423755e-14\n",
      "SubGD iter. 20/499: loss=60.067805854926384, w0=14.699999999999994, w1=1.8388590979944943e-14\n",
      "SubGD iter. 21/499: loss=59.36780585492639, w0=15.399999999999993, w1=1.926423816946613e-14\n",
      "SubGD iter. 22/499: loss=58.667805854926385, w0=16.099999999999994, w1=2.013988535898732e-14\n",
      "SubGD iter. 23/499: loss=57.96780585492638, w0=16.799999999999994, w1=2.1015532548508507e-14\n",
      "SubGD iter. 24/499: loss=57.267805854926394, w0=17.499999999999993, w1=2.1891179738029695e-14\n",
      "SubGD iter. 25/499: loss=56.567805854926384, w0=18.199999999999992, w1=2.2766826927550882e-14\n",
      "SubGD iter. 26/499: loss=55.86780585492639, w0=18.89999999999999, w1=2.364247411707207e-14\n",
      "SubGD iter. 27/499: loss=55.167805854926385, w0=19.59999999999999, w1=2.4518121306593258e-14\n",
      "SubGD iter. 28/499: loss=54.46780585492638, w0=20.29999999999999, w1=2.5393768496114446e-14\n",
      "SubGD iter. 29/499: loss=53.767805854926394, w0=20.99999999999999, w1=2.6269415685635634e-14\n",
      "SubGD iter. 30/499: loss=53.067805854926384, w0=21.69999999999999, w1=2.7145062875156822e-14\n",
      "SubGD iter. 31/499: loss=52.367805854926395, w0=22.399999999999988, w1=2.802071006467801e-14\n",
      "SubGD iter. 32/499: loss=51.667805854926385, w0=23.099999999999987, w1=2.8896357254199195e-14\n",
      "SubGD iter. 33/499: loss=50.96780585492638, w0=23.799999999999986, w1=2.977200444372038e-14\n",
      "SubGD iter. 34/499: loss=50.267805854926394, w0=24.499999999999986, w1=3.064765163324157e-14\n",
      "SubGD iter. 35/499: loss=49.567805854926405, w0=25.199999999999985, w1=3.152329882276276e-14\n",
      "SubGD iter. 36/499: loss=48.867805854926395, w0=25.899999999999984, w1=3.2398946012283946e-14\n",
      "SubGD iter. 37/499: loss=48.1678058549264, w0=26.599999999999984, w1=3.3274593201805134e-14\n",
      "SubGD iter. 38/499: loss=47.4678058549264, w0=27.299999999999983, w1=3.415024039132632e-14\n",
      "SubGD iter. 39/499: loss=46.7678058549264, w0=27.999999999999982, w1=3.502588758084751e-14\n",
      "SubGD iter. 40/499: loss=46.06780585492639, w0=28.69999999999998, w1=3.59015347703687e-14\n",
      "SubGD iter. 41/499: loss=45.367805854926395, w0=29.39999999999998, w1=3.6777181959889886e-14\n",
      "SubGD iter. 42/499: loss=44.6678058549264, w0=30.09999999999998, w1=3.7652829149411074e-14\n",
      "SubGD iter. 43/499: loss=43.9678058549264, w0=30.79999999999998, w1=3.852847633893226e-14\n",
      "SubGD iter. 44/499: loss=43.2678058549264, w0=31.49999999999998, w1=3.940412352845345e-14\n",
      "SubGD iter. 45/499: loss=42.567805854926405, w0=32.19999999999998, w1=4.027977071797464e-14\n",
      "SubGD iter. 46/499: loss=41.867805854926395, w0=32.899999999999984, w1=4.1155417907495825e-14\n",
      "SubGD iter. 47/499: loss=41.1678058549264, w0=33.59999999999999, w1=4.2031065097017013e-14\n",
      "SubGD iter. 48/499: loss=40.4678058549264, w0=34.29999999999999, w1=4.29067122865382e-14\n",
      "SubGD iter. 49/499: loss=39.767805854926394, w0=34.99999999999999, w1=4.378235947605939e-14\n",
      "SubGD iter. 50/499: loss=39.067805854926384, w0=35.699999999999996, w1=4.465800666558058e-14\n",
      "SubGD iter. 51/499: loss=38.36780585492639, w0=36.4, w1=4.5533653855101765e-14\n",
      "SubGD iter. 52/499: loss=37.66780585492638, w0=37.1, w1=4.640930104462295e-14\n",
      "SubGD iter. 53/499: loss=36.96780585492638, w0=37.800000000000004, w1=4.728494823414414e-14\n",
      "SubGD iter. 54/499: loss=36.26780585492637, w0=38.50000000000001, w1=4.816059542366533e-14\n",
      "SubGD iter. 55/499: loss=35.56780585492637, w0=39.20000000000001, w1=4.9036242613186517e-14\n",
      "SubGD iter. 56/499: loss=34.86780585492637, w0=39.90000000000001, w1=4.9911889802707705e-14\n",
      "SubGD iter. 57/499: loss=34.16780585492637, w0=40.600000000000016, w1=5.078753699222889e-14\n",
      "SubGD iter. 58/499: loss=33.46780585492636, w0=41.30000000000002, w1=5.166318418175008e-14\n",
      "SubGD iter. 59/499: loss=32.767805854926365, w0=42.00000000000002, w1=5.253883137127127e-14\n",
      "SubGD iter. 60/499: loss=32.067805854926355, w0=42.700000000000024, w1=5.3414478560792456e-14\n",
      "SubGD iter. 61/499: loss=31.36780585492636, w0=43.40000000000003, w1=5.4290125750313644e-14\n",
      "SubGD iter. 62/499: loss=30.667805854926346, w0=44.10000000000003, w1=5.516577293983483e-14\n",
      "SubGD iter. 63/499: loss=29.967805854926347, w0=44.80000000000003, w1=5.604142012935602e-14\n",
      "SubGD iter. 64/499: loss=29.267805854926348, w0=45.500000000000036, w1=5.691706731887721e-14\n",
      "SubGD iter. 65/499: loss=28.567805854926338, w0=46.20000000000004, w1=5.779271450839839e-14\n",
      "SubGD iter. 66/499: loss=27.867805854926342, w0=46.90000000000004, w1=5.866836169791957e-14\n",
      "SubGD iter. 67/499: loss=27.173270209668917, w0=47.59306930693074, w1=0.01114784567828894\n",
      "SubGD iter. 68/499: loss=26.4904515637512, w0=48.279207920792125, w1=0.03308574108991741\n",
      "SubGD iter. 69/499: loss=25.81721232277017, w0=48.96534653465351, w1=0.055023636501545875\n",
      "SubGD iter. 70/499: loss=25.15503943465645, w0=49.63069306930698, w1=0.1053832638830964\n",
      "SubGD iter. 71/499: loss=24.524103413894778, w0=50.28910891089114, w1=0.16746568532795278\n",
      "SubGD iter. 72/499: loss=23.899295346035586, w0=50.947524752475296, w1=0.22954810677280915\n",
      "SubGD iter. 73/499: loss=23.28439292565714, w0=51.59207920792084, w1=0.312425129327494\n",
      "SubGD iter. 74/499: loss=22.68687644418184, w0=52.22277227722777, w1=0.41195013288401805\n",
      "SubGD iter. 75/499: loss=22.10626756964055, w0=52.84653465346539, w1=0.5208167847923948\n",
      "SubGD iter. 76/499: loss=21.53781882800843, w0=53.4564356435644, w1=0.6457900912636185\n",
      "SubGD iter. 77/499: loss=20.986339874628463, w0=54.0594059405941, w1=0.7796904498577408\n",
      "SubGD iter. 78/499: loss=20.445560936620446, w0=54.655445544554496, w1=0.9197570104995888\n",
      "SubGD iter. 79/499: loss=19.91191015895784, w0=55.24455445544559, w1=1.067092029785011\n",
      "SubGD iter. 80/499: loss=19.389644090563227, w0=55.819801980198065, w1=1.2261255948210965\n",
      "SubGD iter. 81/499: loss=18.887989064395878, w0=56.36732673267331, w1=1.410709342622233\n",
      "SubGD iter. 82/499: loss=18.41596050185423, w0=56.900990099009945, w1=1.605853732220289\n",
      "SubGD iter. 83/499: loss=17.954898543040382, w0=57.42772277227727, w1=1.808762802293982\n",
      "SubGD iter. 84/499: loss=17.505757656579817, w0=57.933663366336674, w1=2.0285064197514897\n",
      "SubGD iter. 85/499: loss=17.074957426931608, w0=58.43267326732677, w1=2.2494370848672975\n",
      "SubGD iter. 86/499: loss=16.652967297509893, w0=58.91089108910895, w1=2.4837982986028537\n",
      "SubGD iter. 87/499: loss=16.248540731496718, w0=59.382178217821824, w1=2.7260245553531703\n",
      "SubGD iter. 88/499: loss=15.849105212654152, w0=59.83960396039608, w1=2.978742333469156\n",
      "SubGD iter. 89/499: loss=15.466919791231321, w0=60.262376237623805, w1=3.251528669355458\n",
      "SubGD iter. 90/499: loss=15.108294621512211, w0=60.67821782178222, w1=3.5270865794243\n",
      "SubGD iter. 91/499: loss=14.754896345922827, w0=61.087128712871326, w1=3.806459183951836\n",
      "SubGD iter. 92/499: loss=14.404528961620272, w0=61.49603960396043, w1=4.085831788479371\n",
      "SubGD iter. 93/499: loss=14.05578702812727, w0=61.891089108910926, w1=4.373839384328629\n",
      "SubGD iter. 94/499: loss=13.714620911605627, w0=62.27920792079211, w1=4.666037469532069\n",
      "SubGD iter. 95/499: loss=13.381236307284146, w0=62.65346534653469, w1=4.959829093241791\n",
      "SubGD iter. 96/499: loss=13.058821615166227, w0=63.02079207920796, w1=5.257057192056662\n",
      "SubGD iter. 97/499: loss=12.740251724339231, w0=63.38118811881192, w1=5.5604343163524295\n",
      "SubGD iter. 98/499: loss=12.423218888756102, w0=63.74158415841588, w1=5.863811440648197\n",
      "SubGD iter. 99/499: loss=12.107561731901159, w0=64.08811881188123, w1=6.172402175278572\n",
      "SubGD iter. 100/499: loss=11.800622097398126, w0=64.42772277227726, w1=6.4863693105165225\n",
      "SubGD iter. 101/499: loss=11.495041794646415, w0=64.7673267326733, w1=6.800336445754473\n",
      "SubGD iter. 102/499: loss=11.189461491894704, w0=65.10693069306933, w1=7.1143035809924235\n",
      "SubGD iter. 103/499: loss=10.883881189142992, w0=65.44653465346536, w1=7.428270716230374\n",
      "SubGD iter. 104/499: loss=10.58459340831319, w0=65.76534653465349, w1=7.747893210218651\n",
      "SubGD iter. 105/499: loss=10.295816534318933, w0=66.070297029703, w1=8.073669686866932\n",
      "SubGD iter. 106/499: loss=10.01135208122135, w0=66.37524752475251, w1=8.399446163515213\n",
      "SubGD iter. 107/499: loss=9.728084326668117, w0=66.6663366336634, w1=8.73297028041742\n",
      "SubGD iter. 108/499: loss=9.448125461122496, w0=66.9574257425743, w1=9.066494397319628\n",
      "SubGD iter. 109/499: loss=9.171041104096656, w0=67.23465346534658, w1=9.398630319470323\n",
      "SubGD iter. 110/499: loss=8.903656131158947, w0=67.51188118811886, w1=9.730766241621017\n",
      "SubGD iter. 111/499: loss=8.63627115822124, w0=67.78910891089114, w1=10.062902163771712\n",
      "SubGD iter. 112/499: loss=8.376151920302359, w0=68.06633663366343, w1=10.363999289979459\n",
      "SubGD iter. 113/499: loss=8.140540838751482, w0=68.32970297029709, w1=10.66046690927365\n",
      "SubGD iter. 114/499: loss=7.918544501597259, w0=68.59306930693076, w1=10.943174379960851\n",
      "SubGD iter. 115/499: loss=7.7052797283769845, w0=68.85643564356442, w1=11.225881850648053\n",
      "SubGD iter. 116/499: loss=7.493695831178626, w0=69.11287128712878, w1=11.504395843582245\n",
      "SubGD iter. 117/499: loss=7.2899924057434, w0=69.35544554455453, w1=11.78820189306779\n",
      "SubGD iter. 118/499: loss=7.097234035781528, w0=69.58415841584166, w1=12.06091146519101\n",
      "SubGD iter. 119/499: loss=6.919905294668907, w0=69.80594059405948, w1=12.324245668386087\n",
      "SubGD iter. 120/499: loss=6.7505735273154395, w0=70.0277227722773, w1=12.587579871581164\n",
      "SubGD iter. 121/499: loss=6.584744810805652, w0=70.25643564356443, w1=12.824765405096523\n",
      "SubGD iter. 122/499: loss=6.4303432763477915, w0=70.47821782178225, w1=13.065616959310187\n",
      "SubGD iter. 123/499: loss=6.27807148189034, w0=70.69306930693077, w1=13.302953389983951\n",
      "SubGD iter. 124/499: loss=6.133663329263311, w0=70.89405940594067, w1=13.525403099312957\n",
      "SubGD iter. 125/499: loss=6.005840798343018, w0=71.08811881188126, w1=13.742945617944251\n",
      "SubGD iter. 126/499: loss=5.885021825223206, w0=71.27524752475254, w1=13.953548196006883\n",
      "SubGD iter. 127/499: loss=5.771635252269647, w0=71.46237623762383, w1=14.164150774069515\n",
      "SubGD iter. 128/499: loss=5.667162061790248, w0=71.62178217821788, w1=14.349779559473214\n",
      "SubGD iter. 129/499: loss=5.586726765993136, w0=71.75346534653471, w1=14.516890107612351\n",
      "SubGD iter. 130/499: loss=5.523847812160378, w0=71.87128712871292, w1=14.670791185324227\n",
      "SubGD iter. 131/499: loss=5.480093708591866, w0=71.95445544554461, w1=14.780276456654562\n",
      "SubGD iter. 132/499: loss=5.453088003502018, w0=72.0376237623763, w1=14.889761727984897\n",
      "SubGD iter. 133/499: loss=5.4273926308629, w0=72.10693069306937, w1=14.985916181776767\n",
      "SubGD iter. 134/499: loss=5.407322445682747, w0=72.17623762376245, w1=15.082070635568638\n",
      "SubGD iter. 135/499: loss=5.387252260502595, w0=72.24554455445552, w1=15.178225089360508\n",
      "SubGD iter. 136/499: loss=5.370460780338691, w0=72.30099009900998, w1=15.25972348971595\n",
      "SubGD iter. 137/499: loss=5.3574065233347365, w0=72.34950495049513, w1=15.335091856448178\n",
      "SubGD iter. 138/499: loss=5.345929264022579, w0=72.39801980198028, w1=15.410460223180406\n",
      "SubGD iter. 139/499: loss=5.335714659517469, w0=72.43267326732682, w1=15.469961786755766\n",
      "SubGD iter. 140/499: loss=5.330043910465359, w0=72.46039603960405, w1=15.51864528583285\n",
      "SubGD iter. 141/499: loss=5.325676428273224, w0=72.48811881188128, w1=15.561592159086528\n",
      "SubGD iter. 142/499: loss=5.322176726526588, w0=72.5019801980199, w1=15.597828332032567\n",
      "SubGD iter. 143/499: loss=5.32011130964311, w0=72.52277227722782, w1=15.624722856626754\n",
      "SubGD iter. 144/499: loss=5.318478284898438, w0=72.55049504950505, w1=15.642690329098041\n",
      "SubGD iter. 145/499: loss=5.317240048565146, w0=72.56435643564366, w1=15.664356578291132\n",
      "SubGD iter. 146/499: loss=5.316406547951545, w0=72.58514851485158, w1=15.677095775361325\n",
      "SubGD iter. 147/499: loss=5.315557122666141, w0=72.6059405940595, w1=15.689834972431518\n",
      "SubGD iter. 148/499: loss=5.31470769738074, w0=72.62673267326743, w1=15.70257416950171\n",
      "SubGD iter. 149/499: loss=5.313876880922164, w0=72.64059405940604, w1=15.724240418694801\n",
      "SubGD iter. 150/499: loss=5.313052246871382, w0=72.66138613861396, w1=15.736979615764994\n",
      "SubGD iter. 151/499: loss=5.3123778390243865, w0=72.66831683168327, w1=15.74811029423132\n",
      "SubGD iter. 152/499: loss=5.312132229725042, w0=72.67524752475258, w1=15.759240972697647\n",
      "SubGD iter. 153/499: loss=5.311886620425695, w0=72.68217821782189, w1=15.770371651163973\n",
      "SubGD iter. 154/499: loss=5.311683566098434, w0=72.68217821782189, w1=15.774323911906727\n",
      "SubGD iter. 155/499: loss=5.311661251291322, w0=72.68217821782189, w1=15.77827617264948\n",
      "SubGD iter. 156/499: loss=5.311638936484209, w0=72.68217821782189, w1=15.782228433392234\n",
      "SubGD iter. 157/499: loss=5.311616621677096, w0=72.68217821782189, w1=15.786180694134988\n",
      "SubGD iter. 158/499: loss=5.311594306869984, w0=72.68217821782189, w1=15.790132954877741\n",
      "SubGD iter. 159/499: loss=5.311571992062872, w0=72.68217821782189, w1=15.794085215620495\n",
      "SubGD iter. 160/499: loss=5.311549677255758, w0=72.68217821782189, w1=15.798037476363248\n",
      "SubGD iter. 161/499: loss=5.311527362448647, w0=72.68217821782189, w1=15.801989737106002\n",
      "SubGD iter. 162/499: loss=5.311505047641535, w0=72.68217821782189, w1=15.805941997848755\n",
      "SubGD iter. 163/499: loss=5.3114827328344205, w0=72.68217821782189, w1=15.809894258591509\n",
      "SubGD iter. 164/499: loss=5.311460418027309, w0=72.68217821782189, w1=15.813846519334263\n",
      "SubGD iter. 165/499: loss=5.311438103220198, w0=72.68217821782189, w1=15.817798780077016\n",
      "SubGD iter. 166/499: loss=5.311415788413085, w0=72.68217821782189, w1=15.82175104081977\n",
      "SubGD iter. 167/499: loss=5.311393473605971, w0=72.68217821782189, w1=15.825703301562523\n",
      "SubGD iter. 168/499: loss=5.31137115879886, w0=72.68217821782189, w1=15.829655562305277\n",
      "SubGD iter. 169/499: loss=5.311348843991747, w0=72.68217821782189, w1=15.83360782304803\n",
      "SubGD iter. 170/499: loss=5.311326529184636, w0=72.68217821782189, w1=15.837560083790784\n",
      "SubGD iter. 171/499: loss=5.311304214377522, w0=72.68217821782189, w1=15.841512344533538\n",
      "SubGD iter. 172/499: loss=5.31128189957041, w0=72.68217821782189, w1=15.845464605276291\n",
      "SubGD iter. 173/499: loss=5.311259584763298, w0=72.68217821782189, w1=15.849416866019045\n",
      "SubGD iter. 174/499: loss=5.311237269956185, w0=72.68217821782189, w1=15.853369126761798\n",
      "SubGD iter. 175/499: loss=5.311214955149072, w0=72.68217821782189, w1=15.857321387504552\n",
      "SubGD iter. 176/499: loss=5.31119264034196, w0=72.68217821782189, w1=15.861273648247305\n",
      "SubGD iter. 177/499: loss=5.311170325534848, w0=72.68217821782189, w1=15.865225908990059\n",
      "SubGD iter. 178/499: loss=5.311148010727736, w0=72.68217821782189, w1=15.869178169732812\n",
      "SubGD iter. 179/499: loss=5.311125695920622, w0=72.68217821782189, w1=15.873130430475566\n",
      "SubGD iter. 180/499: loss=5.31110338111351, w0=72.68217821782189, w1=15.87708269121832\n",
      "SubGD iter. 181/499: loss=5.311081066306398, w0=72.68217821782189, w1=15.881034951961073\n",
      "SubGD iter. 182/499: loss=5.311058751499286, w0=72.68217821782189, w1=15.884987212703827\n",
      "SubGD iter. 183/499: loss=5.311036436692172, w0=72.68217821782189, w1=15.88893947344658\n",
      "SubGD iter. 184/499: loss=5.31101412188506, w0=72.68217821782189, w1=15.892891734189334\n",
      "SubGD iter. 185/499: loss=5.310991807077948, w0=72.68217821782189, w1=15.896843994932087\n",
      "SubGD iter. 186/499: loss=5.310969492270836, w0=72.68217821782189, w1=15.900796255674841\n",
      "SubGD iter. 187/499: loss=5.310947177463723, w0=72.68217821782189, w1=15.904748516417595\n",
      "SubGD iter. 188/499: loss=5.310924862656612, w0=72.68217821782189, w1=15.908700777160348\n",
      "SubGD iter. 189/499: loss=5.310902547849499, w0=72.68217821782189, w1=15.912653037903102\n",
      "SubGD iter. 190/499: loss=5.310913706061381, w0=72.67524752475258, w1=15.910526938117364\n",
      "SubGD iter. 191/499: loss=5.310892237186269, w0=72.67524752475258, w1=15.914479198860118\n",
      "SubGD iter. 192/499: loss=5.3108699223791564, w0=72.67524752475258, w1=15.918431459602871\n",
      "SubGD iter. 193/499: loss=5.310862636053835, w0=72.66831683168327, w1=15.916305359817134\n",
      "SubGD iter. 194/499: loss=5.310859611715928, w0=72.66831683168327, w1=15.920257620559887\n",
      "SubGD iter. 195/499: loss=5.310837296908815, w0=72.66831683168327, w1=15.924209881302641\n",
      "SubGD iter. 196/499: loss=5.310814982101703, w0=72.66831683168327, w1=15.928162142045394\n",
      "SubGD iter. 197/499: loss=5.310823570190172, w0=72.66138613861396, w1=15.926036042259657\n",
      "SubGD iter. 198/499: loss=5.310804671438475, w0=72.66138613861396, w1=15.92998830300241\n",
      "SubGD iter. 199/499: loss=5.3107823566313614, w0=72.66138613861396, w1=15.933940563745164\n",
      "SubGD iter. 200/499: loss=5.310772500182623, w0=72.65445544554466, w1=15.931814463959427\n",
      "SubGD iter. 201/499: loss=5.3107720459681325, w0=72.65445544554466, w1=15.93576672470218\n",
      "SubGD iter. 202/499: loss=5.310749731161019, w0=72.65445544554466, w1=15.939718985444934\n",
      "SubGD iter. 203/499: loss=5.310727416353907, w0=72.65445544554466, w1=15.943671246187687\n",
      "SubGD iter. 204/499: loss=5.310733434318959, w0=72.64752475247535, w1=15.94154514640195\n",
      "SubGD iter. 205/499: loss=5.310717105690678, w0=72.64752475247535, w1=15.945497407144703\n",
      "SubGD iter. 206/499: loss=5.3106947908835656, w0=72.64752475247535, w1=15.949449667887457\n",
      "SubGD iter. 207/499: loss=5.310682364311412, w0=72.64059405940604, w1=15.94732356810172\n",
      "SubGD iter. 208/499: loss=5.310684480220336, w0=72.64059405940604, w1=15.951275828844473\n",
      "SubGD iter. 209/499: loss=5.310662165413224, w0=72.64059405940604, w1=15.955228089587226\n",
      "SubGD iter. 210/499: loss=5.310639850606112, w0=72.64059405940604, w1=15.95918035032998\n",
      "SubGD iter. 211/499: loss=5.310643298447746, w0=72.63366336633673, w1=15.957054250544243\n",
      "SubGD iter. 212/499: loss=5.310629539942882, w0=72.63366336633673, w1=15.961006511286996\n",
      "SubGD iter. 213/499: loss=5.3106072251357705, w0=72.63366336633673, w1=15.96495877202975\n",
      "SubGD iter. 214/499: loss=5.3105922284402, w0=72.62673267326743, w1=15.962832672244012\n",
      "SubGD iter. 215/499: loss=5.310633183099026, w0=72.63366336633673, w1=15.967301372051416\n",
      "SubGD iter. 216/499: loss=5.3105993435850625, w0=72.62673267326743, w1=15.965175272265679\n",
      "SubGD iter. 217/499: loss=5.31061822827579, w0=72.63366336633673, w1=15.969643972073083\n",
      "SubGD iter. 218/499: loss=5.310606458729926, w0=72.62673267326743, w1=15.967517872287345\n",
      "SubGD iter. 219/499: loss=5.3106032734525535, w0=72.63366336633673, w1=15.97198657209475\n",
      "SubGD iter. 220/499: loss=5.3106135738747895, w0=72.62673267326743, w1=15.969860472309012\n",
      "SubGD iter. 221/499: loss=5.310588318629316, w0=72.63366336633673, w1=15.974329172116416\n",
      "SubGD iter. 222/499: loss=5.310620689019653, w0=72.62673267326743, w1=15.972203072330679\n",
      "SubGD iter. 223/499: loss=5.310574966149885, w0=72.62673267326743, w1=15.970593411609592\n",
      "SubGD iter. 224/499: loss=5.31058363964973, w0=72.63366336633673, w1=15.975062111416996\n",
      "SubGD iter. 225/499: loss=5.310622915165494, w0=72.62673267326743, w1=15.972936011631258\n",
      "SubGD iter. 226/499: loss=5.310576651555034, w0=72.62673267326743, w1=15.971326350910171\n",
      "SubGD iter. 227/499: loss=5.310578960670142, w0=72.63366336633673, w1=15.975795050717576\n",
      "SubGD iter. 228/499: loss=5.310625141311338, w0=72.62673267326743, w1=15.973668950931838\n",
      "SubGD iter. 229/499: loss=5.31057833696018, w0=72.62673267326743, w1=15.972059290210751\n",
      "SubGD iter. 230/499: loss=5.310574635520698, w0=72.62673267326743, w1=15.970449629489664\n",
      "SubGD iter. 231/499: loss=5.310584557534202, w0=72.63366336633673, w1=15.974918329297068\n",
      "SubGD iter. 232/499: loss=5.31062247845816, w0=72.62673267326743, w1=15.972792229511331\n",
      "SubGD iter. 233/499: loss=5.310576320925845, w0=72.62673267326743, w1=15.971182568790244\n",
      "SubGD iter. 234/499: loss=5.3105798785546146, w0=72.63366336633673, w1=15.975651268597648\n",
      "SubGD iter. 235/499: loss=5.310624704604003, w0=72.62673267326743, w1=15.97352516881191\n",
      "SubGD iter. 236/499: loss=5.310578006330994, w0=72.62673267326743, w1=15.971915508090824\n",
      "SubGD iter. 237/499: loss=5.310575199575027, w0=72.63366336633673, w1=15.976384207898228\n",
      "SubGD iter. 238/499: loss=5.310626930749844, w0=72.62673267326743, w1=15.97425810811249\n",
      "SubGD iter. 239/499: loss=5.310579691736141, w0=72.62673267326743, w1=15.972648447391403\n",
      "SubGD iter. 240/499: loss=5.310575990296659, w0=72.62673267326743, w1=15.971038786670317\n",
      "SubGD iter. 241/499: loss=5.310580796439089, w0=72.63366336633673, w1=15.97550748647772\n",
      "SubGD iter. 242/499: loss=5.310624267896666, w0=72.62673267326743, w1=15.973381386691983\n",
      "SubGD iter. 243/499: loss=5.310577675701806, w0=72.62673267326743, w1=15.971771725970896\n",
      "SubGD iter. 244/499: loss=5.3105761174595, w0=72.63366336633673, w1=15.9762404257783\n",
      "SubGD iter. 245/499: loss=5.310626494042512, w0=72.62673267326743, w1=15.974114325992563\n",
      "SubGD iter. 246/499: loss=5.310579361106954, w0=72.62673267326743, w1=15.972504665271476\n",
      "SubGD iter. 247/499: loss=5.310575659667473, w0=72.62673267326743, w1=15.970895004550389\n",
      "SubGD iter. 248/499: loss=5.310581714323562, w0=72.63366336633673, w1=15.975363704357793\n",
      "SubGD iter. 249/499: loss=5.310623831189333, w0=72.62673267326743, w1=15.973237604572056\n",
      "SubGD iter. 250/499: loss=5.310577345072619, w0=72.62673267326743, w1=15.971627943850969\n",
      "SubGD iter. 251/499: loss=5.310577035343974, w0=72.63366336633673, w1=15.976096643658373\n",
      "SubGD iter. 252/499: loss=5.310626057335176, w0=72.62673267326743, w1=15.973970543872635\n",
      "SubGD iter. 253/499: loss=5.310579030477768, w0=72.62673267326743, w1=15.972360883151548\n",
      "SubGD iter. 254/499: loss=5.3105753290382856, w0=72.62673267326743, w1=15.970751222430462\n",
      "SubGD iter. 255/499: loss=5.310582632208036, w0=72.63366336633673, w1=15.975219922237866\n",
      "SubGD iter. 256/499: loss=5.310623394482, w0=72.62673267326743, w1=15.973093822452128\n",
      "SubGD iter. 257/499: loss=5.310577014443433, w0=72.62673267326743, w1=15.971484161731041\n",
      "SubGD iter. 258/499: loss=5.3105779532284485, w0=72.63366336633673, w1=15.975952861538445\n",
      "SubGD iter. 259/499: loss=5.3106256206278415, w0=72.62673267326743, w1=15.973826761752708\n",
      "SubGD iter. 260/499: loss=5.310578699848579, w0=72.62673267326743, w1=15.972217101031621\n",
      "SubGD iter. 261/499: loss=5.310574998409098, w0=72.62673267326743, w1=15.970607440310534\n",
      "SubGD iter. 262/499: loss=5.3105835500925105, w0=72.63366336633673, w1=15.975076140117938\n",
      "SubGD iter. 263/499: loss=5.3106229577746635, w0=72.62673267326743, w1=15.9729500403322\n",
      "SubGD iter. 264/499: loss=5.310576683814246, w0=72.62673267326743, w1=15.971340379611114\n",
      "SubGD iter. 265/499: loss=5.310578871112923, w0=72.63366336633673, w1=15.975809079418518\n",
      "SubGD iter. 266/499: loss=5.310625183920507, w0=72.62673267326743, w1=15.97368297963278\n",
      "SubGD iter. 267/499: loss=5.310578369219393, w0=72.62673267326743, w1=15.972073318911693\n",
      "SubGD iter. 268/499: loss=5.310574667779911, w0=72.62673267326743, w1=15.970463658190607\n",
      "SubGD iter. 269/499: loss=5.310584467976983, w0=72.63366336633673, w1=15.97493235799801\n",
      "SubGD iter. 270/499: loss=5.310622521067329, w0=72.62673267326743, w1=15.972806258212273\n",
      "SubGD iter. 271/499: loss=5.310576353185058, w0=72.62673267326743, w1=15.971196597491186\n",
      "SubGD iter. 272/499: loss=5.310579788997396, w0=72.63366336633673, w1=15.97566529729859\n",
      "SubGD iter. 273/499: loss=5.310624747213171, w0=72.62673267326743, w1=15.973539197512853\n",
      "SubGD iter. 274/499: loss=5.310578038590206, w0=72.62673267326743, w1=15.971929536791766\n",
      "SubGD iter. 275/499: loss=5.310575110017808, w0=72.63366336633673, w1=15.97639823659917\n",
      "SubGD iter. 276/499: loss=5.310626973359014, w0=72.62673267326743, w1=15.974272136813433\n",
      "SubGD iter. 277/499: loss=5.310579723995353, w0=72.62673267326743, w1=15.972662476092346\n",
      "SubGD iter. 278/499: loss=5.310576022555872, w0=72.62673267326743, w1=15.971052815371259\n",
      "SubGD iter. 279/499: loss=5.31058070688187, w0=72.63366336633673, w1=15.975521515178663\n",
      "SubGD iter. 280/499: loss=5.310624310505836, w0=72.62673267326743, w1=15.973395415392925\n",
      "SubGD iter. 281/499: loss=5.310577707961019, w0=72.62673267326743, w1=15.971785754671838\n",
      "SubGD iter. 282/499: loss=5.3105760279022824, w0=72.63366336633673, w1=15.976254454479243\n",
      "SubGD iter. 283/499: loss=5.31062653665168, w0=72.62673267326743, w1=15.974128354693505\n",
      "SubGD iter. 284/499: loss=5.310579393366167, w0=72.62673267326743, w1=15.972518693972418\n",
      "SubGD iter. 285/499: loss=5.310575691926685, w0=72.62673267326743, w1=15.970909033251331\n",
      "SubGD iter. 286/499: loss=5.3105816247663435, w0=72.63366336633673, w1=15.975377733058735\n",
      "SubGD iter. 287/499: loss=5.310623873798502, w0=72.62673267326743, w1=15.973251633272998\n",
      "SubGD iter. 288/499: loss=5.310577377331832, w0=72.62673267326743, w1=15.971641972551911\n",
      "SubGD iter. 289/499: loss=5.310576945786756, w0=72.63366336633673, w1=15.976110672359315\n",
      "SubGD iter. 290/499: loss=5.310626099944345, w0=72.62673267326743, w1=15.973984572573578\n",
      "SubGD iter. 291/499: loss=5.310579062736981, w0=72.62673267326743, w1=15.97237491185249\n",
      "SubGD iter. 292/499: loss=5.310575361297499, w0=72.62673267326743, w1=15.970765251131404\n",
      "SubGD iter. 293/499: loss=5.310582542650818, w0=72.63366336633673, w1=15.975233950938808\n",
      "SubGD iter. 294/499: loss=5.310623437091167, w0=72.62673267326743, w1=15.97310785115307\n",
      "SubGD iter. 295/499: loss=5.310577046702646, w0=72.62673267326743, w1=15.971498190431983\n",
      "SubGD iter. 296/499: loss=5.31057786367123, w0=72.63366336633673, w1=15.975966890239388\n",
      "SubGD iter. 297/499: loss=5.310625663237009, w0=72.62673267326743, w1=15.97384079045365\n",
      "SubGD iter. 298/499: loss=5.310578732107793, w0=72.62673267326743, w1=15.972231129732563\n",
      "SubGD iter. 299/499: loss=5.310575030668311, w0=72.62673267326743, w1=15.970621469011476\n",
      "SubGD iter. 300/499: loss=5.31058346053529, w0=72.63366336633673, w1=15.97509016881888\n",
      "SubGD iter. 301/499: loss=5.310623000383833, w0=72.62673267326743, w1=15.972964069033143\n",
      "SubGD iter. 302/499: loss=5.3105767160734585, w0=72.62673267326743, w1=15.971354408312056\n",
      "SubGD iter. 303/499: loss=5.310578781555704, w0=72.63366336633673, w1=15.97582310811946\n",
      "SubGD iter. 304/499: loss=5.310625226529675, w0=72.62673267326743, w1=15.973697008333723\n",
      "SubGD iter. 305/499: loss=5.3105784014786055, w0=72.62673267326743, w1=15.972087347612636\n",
      "SubGD iter. 306/499: loss=5.310574700039124, w0=72.62673267326743, w1=15.970477686891549\n",
      "SubGD iter. 307/499: loss=5.310584378419764, w0=72.63366336633673, w1=15.974946386698953\n",
      "SubGD iter. 308/499: loss=5.310622563676497, w0=72.62673267326743, w1=15.972820286913215\n",
      "SubGD iter. 309/499: loss=5.3105763854442705, w0=72.62673267326743, w1=15.971210626192129\n",
      "SubGD iter. 310/499: loss=5.310579699440178, w0=72.63366336633673, w1=15.975679325999533\n",
      "SubGD iter. 311/499: loss=5.310624789822341, w0=72.62673267326743, w1=15.973553226213795\n",
      "SubGD iter. 312/499: loss=5.310578070849419, w0=72.62673267326743, w1=15.971943565492708\n",
      "SubGD iter. 313/499: loss=5.310575020460591, w0=72.63366336633673, w1=15.976412265300112\n",
      "SubGD iter. 314/499: loss=5.310627015968183, w0=72.62673267326743, w1=15.974286165514375\n",
      "SubGD iter. 315/499: loss=5.310579756254565, w0=72.62673267326743, w1=15.972676504793288\n",
      "SubGD iter. 316/499: loss=5.310576054815084, w0=72.62673267326743, w1=15.971066844072201\n",
      "SubGD iter. 317/499: loss=5.310580617324651, w0=72.63366336633673, w1=15.975535543879605\n",
      "SubGD iter. 318/499: loss=5.3106243531150055, w0=72.62673267326743, w1=15.973409444093868\n",
      "SubGD iter. 319/499: loss=5.310577740220233, w0=72.62673267326743, w1=15.97179978337278\n",
      "SubGD iter. 320/499: loss=5.310575938345063, w0=72.63366336633673, w1=15.976268483180185\n",
      "SubGD iter. 321/499: loss=5.310626579260847, w0=72.62673267326743, w1=15.974142383394447\n",
      "SubGD iter. 322/499: loss=5.31057942562538, w0=72.62673267326743, w1=15.97253272267336\n",
      "SubGD iter. 323/499: loss=5.310575724185898, w0=72.62673267326743, w1=15.970923061952274\n",
      "SubGD iter. 324/499: loss=5.310581535209124, w0=72.63366336633673, w1=15.975391761759678\n",
      "SubGD iter. 325/499: loss=5.310623916407669, w0=72.62673267326743, w1=15.97326566197394\n",
      "SubGD iter. 326/499: loss=5.310577409591045, w0=72.62673267326743, w1=15.971656001252853\n",
      "SubGD iter. 327/499: loss=5.310576856229536, w0=72.63366336633673, w1=15.976124701060257\n",
      "SubGD iter. 328/499: loss=5.310626142553513, w0=72.62673267326743, w1=15.97399860127452\n",
      "SubGD iter. 329/499: loss=5.310579094996192, w0=72.62673267326743, w1=15.972388940553433\n",
      "SubGD iter. 330/499: loss=5.31057539355671, w0=72.62673267326743, w1=15.970779279832346\n",
      "SubGD iter. 331/499: loss=5.310582453093599, w0=72.63366336633673, w1=15.97524797963975\n",
      "SubGD iter. 332/499: loss=5.310623479700335, w0=72.62673267326743, w1=15.973121879854013\n",
      "SubGD iter. 333/499: loss=5.310577078961858, w0=72.62673267326743, w1=15.971512219132926\n",
      "SubGD iter. 334/499: loss=5.3105777741140106, w0=72.63366336633673, w1=15.97598091894033\n",
      "SubGD iter. 335/499: loss=5.310625705846178, w0=72.62673267326743, w1=15.973854819154592\n",
      "SubGD iter. 336/499: loss=5.310578764367005, w0=72.62673267326743, w1=15.972245158433505\n",
      "SubGD iter. 337/499: loss=5.310575062927524, w0=72.62673267326743, w1=15.970635497712419\n",
      "SubGD iter. 338/499: loss=5.3105833709780725, w0=72.63366336633673, w1=15.975104197519823\n",
      "SubGD iter. 339/499: loss=5.310623042993, w0=72.62673267326743, w1=15.972978097734085\n",
      "SubGD iter. 340/499: loss=5.310576748332672, w0=72.62673267326743, w1=15.971368437012998\n",
      "SubGD iter. 341/499: loss=5.310578691998485, w0=72.63366336633673, w1=15.975837136820402\n",
      "SubGD iter. 342/499: loss=5.310625269138844, w0=72.62673267326743, w1=15.973711037034665\n",
      "SubGD iter. 343/499: loss=5.310578433737819, w0=72.62673267326743, w1=15.972101376313578\n",
      "SubGD iter. 344/499: loss=5.3105747322983365, w0=72.62673267326743, w1=15.970491715592491\n",
      "SubGD iter. 345/499: loss=5.310584288862545, w0=72.63366336633673, w1=15.974960415399895\n",
      "SubGD iter. 346/499: loss=5.3106226062856665, w0=72.62673267326743, w1=15.972834315614158\n",
      "SubGD iter. 347/499: loss=5.3105764177034835, w0=72.62673267326743, w1=15.97122465489307\n",
      "SubGD iter. 348/499: loss=5.310579609882959, w0=72.63366336633673, w1=15.975693354700475\n",
      "SubGD iter. 349/499: loss=5.310624832431509, w0=72.62673267326743, w1=15.973567254914737\n",
      "SubGD iter. 350/499: loss=5.3105781031086305, w0=72.62673267326743, w1=15.97195759419365\n",
      "SubGD iter. 351/499: loss=5.310574930903369, w0=72.63366336633673, w1=15.976426294001055\n",
      "SubGD iter. 352/499: loss=5.310627058577351, w0=72.62673267326743, w1=15.974300194215317\n",
      "SubGD iter. 353/499: loss=5.310579788513779, w0=72.62673267326743, w1=15.97269053349423\n",
      "SubGD iter. 354/499: loss=5.310576087074297, w0=72.62673267326743, w1=15.971080872773143\n",
      "SubGD iter. 355/499: loss=5.310580527767431, w0=72.63366336633673, w1=15.975549572580547\n",
      "SubGD iter. 356/499: loss=5.310624395724175, w0=72.62673267326743, w1=15.97342347279481\n",
      "SubGD iter. 357/499: loss=5.310577772479444, w0=72.62673267326743, w1=15.971813812073723\n",
      "SubGD iter. 358/499: loss=5.3105758487878445, w0=72.63366336633673, w1=15.976282511881127\n",
      "SubGD iter. 359/499: loss=5.310626621870016, w0=72.62673267326743, w1=15.97415641209539\n",
      "SubGD iter. 360/499: loss=5.310579457884592, w0=72.62673267326743, w1=15.972546751374303\n",
      "SubGD iter. 361/499: loss=5.31057575644511, w0=72.62673267326743, w1=15.970937090653216\n",
      "SubGD iter. 362/499: loss=5.310581445651906, w0=72.63366336633673, w1=15.97540579046062\n",
      "SubGD iter. 363/499: loss=5.310623959016838, w0=72.62673267326743, w1=15.973279690674882\n",
      "SubGD iter. 364/499: loss=5.310577441850257, w0=72.62673267326743, w1=15.971670029953795\n",
      "SubGD iter. 365/499: loss=5.310576766672318, w0=72.63366336633673, w1=15.9761387297612\n",
      "SubGD iter. 366/499: loss=5.310626185162682, w0=72.62673267326743, w1=15.974012629975462\n",
      "SubGD iter. 367/499: loss=5.310579127255404, w0=72.62673267326743, w1=15.972402969254375\n",
      "SubGD iter. 368/499: loss=5.310575425815924, w0=72.62673267326743, w1=15.970793308533288\n",
      "SubGD iter. 369/499: loss=5.310582363536379, w0=72.63366336633673, w1=15.975262008340692\n",
      "SubGD iter. 370/499: loss=5.310623522309504, w0=72.62673267326743, w1=15.973135908554955\n",
      "SubGD iter. 371/499: loss=5.310577111221071, w0=72.62673267326743, w1=15.971526247833868\n",
      "SubGD iter. 372/499: loss=5.310577684556792, w0=72.63366336633673, w1=15.975994947641272\n",
      "SubGD iter. 373/499: loss=5.310625748455347, w0=72.62673267326743, w1=15.973868847855535\n",
      "SubGD iter. 374/499: loss=5.310578796626218, w0=72.62673267326743, w1=15.972259187134448\n",
      "SubGD iter. 375/499: loss=5.310575095186737, w0=72.62673267326743, w1=15.97064952641336\n",
      "SubGD iter. 376/499: loss=5.310583281420854, w0=72.63366336633673, w1=15.975118226220765\n",
      "SubGD iter. 377/499: loss=5.3106230856021694, w0=72.62673267326743, w1=15.972992126435027\n",
      "SubGD iter. 378/499: loss=5.310576780591883, w0=72.62673267326743, w1=15.97138246571394\n",
      "SubGD iter. 379/499: loss=5.310578602441266, w0=72.63366336633673, w1=15.975851165521345\n",
      "SubGD iter. 380/499: loss=5.310625311748013, w0=72.62673267326743, w1=15.973725065735607\n",
      "SubGD iter. 381/499: loss=5.310578465997031, w0=72.62673267326743, w1=15.97211540501452\n",
      "SubGD iter. 382/499: loss=5.31057476455755, w0=72.62673267326743, w1=15.970505744293433\n",
      "SubGD iter. 383/499: loss=5.310584199305326, w0=72.63366336633673, w1=15.974974444100837\n",
      "SubGD iter. 384/499: loss=5.310622648894835, w0=72.62673267326743, w1=15.9728483443151\n",
      "SubGD iter. 385/499: loss=5.310576449962697, w0=72.62673267326743, w1=15.971238683594013\n",
      "SubGD iter. 386/499: loss=5.310579520325739, w0=72.63366336633673, w1=15.975707383401417\n",
      "SubGD iter. 387/499: loss=5.310624875040677, w0=72.62673267326743, w1=15.97358128361568\n",
      "SubGD iter. 388/499: loss=5.3105781353678445, w0=72.62673267326743, w1=15.971971622894593\n",
      "SubGD iter. 389/499: loss=5.310574841346153, w0=72.63366336633673, w1=15.976440322701997\n",
      "SubGD iter. 390/499: loss=5.31062710118652, w0=72.62673267326743, w1=15.97431422291626\n",
      "SubGD iter. 391/499: loss=5.3105798207729915, w0=72.62673267326743, w1=15.972704562195172\n",
      "SubGD iter. 392/499: loss=5.3105761193335095, w0=72.62673267326743, w1=15.971094901474086\n",
      "SubGD iter. 393/499: loss=5.310580438210212, w0=72.63366336633673, w1=15.97556360128149\n",
      "SubGD iter. 394/499: loss=5.310624438333342, w0=72.62673267326743, w1=15.973437501495752\n",
      "SubGD iter. 395/499: loss=5.3105778047386565, w0=72.62673267326743, w1=15.971827840774665\n",
      "SubGD iter. 396/499: loss=5.310575759230625, w0=72.63366336633673, w1=15.97629654058207\n",
      "SubGD iter. 397/499: loss=5.3106266644791855, w0=72.62673267326743, w1=15.974170440796332\n",
      "SubGD iter. 398/499: loss=5.310579490143805, w0=72.62673267326743, w1=15.972560780075245\n",
      "SubGD iter. 399/499: loss=5.310575788704324, w0=72.62673267326743, w1=15.970951119354158\n",
      "SubGD iter. 400/499: loss=5.310581356094687, w0=72.63366336633673, w1=15.975419819161562\n",
      "SubGD iter. 401/499: loss=5.310624001626008, w0=72.62673267326743, w1=15.973293719375825\n",
      "SubGD iter. 402/499: loss=5.31057747410947, w0=72.62673267326743, w1=15.971684058654738\n",
      "SubGD iter. 403/499: loss=5.310576677115099, w0=72.63366336633673, w1=15.976152758462142\n",
      "SubGD iter. 404/499: loss=5.31062622777185, w0=72.62673267326743, w1=15.974026658676404\n",
      "SubGD iter. 405/499: loss=5.310579159514617, w0=72.62673267326743, w1=15.972416997955317\n",
      "SubGD iter. 406/499: loss=5.310575458075137, w0=72.62673267326743, w1=15.97080733723423\n",
      "SubGD iter. 407/499: loss=5.31058227397916, w0=72.63366336633673, w1=15.975276037041635\n",
      "SubGD iter. 408/499: loss=5.310623564918673, w0=72.62673267326743, w1=15.973149937255897\n",
      "SubGD iter. 409/499: loss=5.310577143480284, w0=72.62673267326743, w1=15.97154027653481\n",
      "SubGD iter. 410/499: loss=5.310577594999573, w0=72.63366336633673, w1=15.976008976342214\n",
      "SubGD iter. 411/499: loss=5.310625791064515, w0=72.62673267326743, w1=15.973882876556477\n",
      "SubGD iter. 412/499: loss=5.310578828885431, w0=72.62673267326743, w1=15.97227321583539\n",
      "SubGD iter. 413/499: loss=5.310575127445949, w0=72.62673267326743, w1=15.970663555114303\n",
      "SubGD iter. 414/499: loss=5.310583191863635, w0=72.63366336633673, w1=15.975132254921707\n",
      "SubGD iter. 415/499: loss=5.310623128211339, w0=72.62673267326743, w1=15.97300615513597\n",
      "SubGD iter. 416/499: loss=5.310576812851096, w0=72.62673267326743, w1=15.971396494414883\n",
      "SubGD iter. 417/499: loss=5.310578512884047, w0=72.63366336633673, w1=15.975865194222287\n",
      "SubGD iter. 418/499: loss=5.31062535435718, w0=72.62673267326743, w1=15.97373909443655\n",
      "SubGD iter. 419/499: loss=5.310578498256244, w0=72.62673267326743, w1=15.972129433715462\n",
      "SubGD iter. 420/499: loss=5.310574796816762, w0=72.62673267326743, w1=15.970519772994376\n",
      "SubGD iter. 421/499: loss=5.310584109748109, w0=72.63366336633673, w1=15.97498847280178\n",
      "SubGD iter. 422/499: loss=5.310622691504003, w0=72.62673267326743, w1=15.972862373016042\n",
      "SubGD iter. 423/499: loss=5.31057648222191, w0=72.62673267326743, w1=15.971252712294955\n",
      "SubGD iter. 424/499: loss=5.310579430768521, w0=72.63366336633673, w1=15.97572141210236\n",
      "SubGD iter. 425/499: loss=5.310624917649846, w0=72.62673267326743, w1=15.973595312316622\n",
      "SubGD iter. 426/499: loss=5.310578167627058, w0=72.62673267326743, w1=15.971985651595535\n",
      "SubGD iter. 427/499: loss=5.310574751788933, w0=72.63366336633673, w1=15.97645435140294\n",
      "SubGD iter. 428/499: loss=5.310627143795689, w0=72.62673267326743, w1=15.974328251617202\n",
      "SubGD iter. 429/499: loss=5.310579853032204, w0=72.62673267326743, w1=15.972718590896115\n",
      "SubGD iter. 430/499: loss=5.3105761515927234, w0=72.62673267326743, w1=15.971108930175028\n",
      "SubGD iter. 431/499: loss=5.310580348652993, w0=72.63366336633673, w1=15.975577629982432\n",
      "SubGD iter. 432/499: loss=5.310624480942511, w0=72.62673267326743, w1=15.973451530196694\n",
      "SubGD iter. 433/499: loss=5.310577836997869, w0=72.62673267326743, w1=15.971841869475607\n",
      "SubGD iter. 434/499: loss=5.310575669673406, w0=72.63366336633673, w1=15.976310569283012\n",
      "SubGD iter. 435/499: loss=5.310626707088355, w0=72.62673267326743, w1=15.974184469497274\n",
      "SubGD iter. 436/499: loss=5.310579522403018, w0=72.62673267326743, w1=15.972574808776187\n",
      "SubGD iter. 437/499: loss=5.310575820963535, w0=72.62673267326743, w1=15.9709651480551\n",
      "SubGD iter. 438/499: loss=5.3105812665374685, w0=72.63366336633673, w1=15.975433847862504\n",
      "SubGD iter. 439/499: loss=5.310624044235176, w0=72.62673267326743, w1=15.973307748076767\n",
      "SubGD iter. 440/499: loss=5.310577506368682, w0=72.62673267326743, w1=15.97169808735568\n",
      "SubGD iter. 441/499: loss=5.310576587557881, w0=72.63366336633673, w1=15.976166787163084\n",
      "SubGD iter. 442/499: loss=5.310626270381018, w0=72.62673267326743, w1=15.974040687377347\n",
      "SubGD iter. 443/499: loss=5.310579191773829, w0=72.62673267326743, w1=15.97243102665626\n",
      "SubGD iter. 444/499: loss=5.310575490334348, w0=72.62673267326743, w1=15.970821365935173\n",
      "SubGD iter. 445/499: loss=5.310582184421942, w0=72.63366336633673, w1=15.975290065742577\n",
      "SubGD iter. 446/499: loss=5.310623607527842, w0=72.62673267326743, w1=15.97316396595684\n",
      "SubGD iter. 447/499: loss=5.310577175739496, w0=72.62673267326743, w1=15.971554305235752\n",
      "SubGD iter. 448/499: loss=5.310577505442354, w0=72.63366336633673, w1=15.976023005043157\n",
      "SubGD iter. 449/499: loss=5.310625833673685, w0=72.62673267326743, w1=15.97389690525742\n",
      "SubGD iter. 450/499: loss=5.310578861144642, w0=72.62673267326743, w1=15.972287244536332\n",
      "SubGD iter. 451/499: loss=5.310575159705164, w0=72.62673267326743, w1=15.970677583815245\n",
      "SubGD iter. 452/499: loss=5.310583102306416, w0=72.63366336633673, w1=15.97514628362265\n",
      "SubGD iter. 453/499: loss=5.310623170820506, w0=72.62673267326743, w1=15.973020183836912\n",
      "SubGD iter. 454/499: loss=5.31057684511031, w0=72.62673267326743, w1=15.971410523115825\n",
      "SubGD iter. 455/499: loss=5.310578423326829, w0=72.63366336633673, w1=15.97587922292323\n",
      "SubGD iter. 456/499: loss=5.3106253969663495, w0=72.62673267326743, w1=15.973753123137492\n",
      "SubGD iter. 457/499: loss=5.310578530515457, w0=72.62673267326743, w1=15.972143462416405\n",
      "SubGD iter. 458/499: loss=5.310574829075975, w0=72.62673267326743, w1=15.970533801695318\n",
      "SubGD iter. 459/499: loss=5.31058402019089, w0=72.63366336633673, w1=15.975002501502722\n",
      "SubGD iter. 460/499: loss=5.3106227341131715, w0=72.62673267326743, w1=15.972876401716984\n",
      "SubGD iter. 461/499: loss=5.310576514481121, w0=72.62673267326743, w1=15.971266740995897\n",
      "SubGD iter. 462/499: loss=5.310579341211301, w0=72.63366336633673, w1=15.975735440803302\n",
      "SubGD iter. 463/499: loss=5.310624960259015, w0=72.62673267326743, w1=15.973609341017564\n",
      "SubGD iter. 464/499: loss=5.310578199886269, w0=72.62673267326743, w1=15.971999680296477\n",
      "SubGD iter. 465/499: loss=5.310574662231715, w0=72.63366336633673, w1=15.976468380103881\n",
      "SubGD iter. 466/499: loss=5.310627186404856, w0=72.62673267326743, w1=15.974342280318144\n",
      "SubGD iter. 467/499: loss=5.310579885291418, w0=72.62673267326743, w1=15.972732619597057\n",
      "SubGD iter. 468/499: loss=5.310576183851936, w0=72.62673267326743, w1=15.97112295887597\n",
      "SubGD iter. 469/499: loss=5.310580259095775, w0=72.63366336633673, w1=15.975591658683374\n",
      "SubGD iter. 470/499: loss=5.31062452355168, w0=72.62673267326743, w1=15.973465558897637\n",
      "SubGD iter. 471/499: loss=5.310577869257082, w0=72.62673267326743, w1=15.97185589817655\n",
      "SubGD iter. 472/499: loss=5.310575580116187, w0=72.63366336633673, w1=15.976324597983954\n",
      "SubGD iter. 473/499: loss=5.310626749697523, w0=72.62673267326743, w1=15.974198498198216\n",
      "SubGD iter. 474/499: loss=5.310579554662228, w0=72.62673267326743, w1=15.97258883747713\n",
      "SubGD iter. 475/499: loss=5.3105758532227485, w0=72.62673267326743, w1=15.970979176756043\n",
      "SubGD iter. 476/499: loss=5.310581176980249, w0=72.63366336633673, w1=15.975447876563447\n",
      "SubGD iter. 477/499: loss=5.310624086844345, w0=72.62673267326743, w1=15.97332177677771\n",
      "SubGD iter. 478/499: loss=5.3105775386278955, w0=72.62673267326743, w1=15.971712116056622\n",
      "SubGD iter. 479/499: loss=5.310576498000661, w0=72.63366336633673, w1=15.976180815864026\n",
      "SubGD iter. 480/499: loss=5.310626312990188, w0=72.62673267326743, w1=15.974054716078289\n",
      "SubGD iter. 481/499: loss=5.3105792240330425, w0=72.62673267326743, w1=15.972445055357202\n",
      "SubGD iter. 482/499: loss=5.3105755225935605, w0=72.62673267326743, w1=15.970835394636115\n",
      "SubGD iter. 483/499: loss=5.310582094864723, w0=72.63366336633673, w1=15.97530409444352\n",
      "SubGD iter. 484/499: loss=5.31062365013701, w0=72.62673267326743, w1=15.973177994657782\n",
      "SubGD iter. 485/499: loss=5.3105772079987075, w0=72.62673267326743, w1=15.971568333936695\n",
      "SubGD iter. 486/499: loss=5.3105774158851355, w0=72.63366336633673, w1=15.976037033744099\n",
      "SubGD iter. 487/499: loss=5.310625876282853, w0=72.62673267326743, w1=15.973910933958361\n",
      "SubGD iter. 488/499: loss=5.310578893403855, w0=72.62673267326743, w1=15.972301273237274\n",
      "SubGD iter. 489/499: loss=5.310575191964375, w0=72.62673267326743, w1=15.970691612516188\n",
      "SubGD iter. 490/499: loss=5.310583012749197, w0=72.63366336633673, w1=15.975160312323592\n",
      "SubGD iter. 491/499: loss=5.310623213429675, w0=72.62673267326743, w1=15.973034212537854\n",
      "SubGD iter. 492/499: loss=5.310576877369521, w0=72.62673267326743, w1=15.971424551816767\n",
      "SubGD iter. 493/499: loss=5.310578333769609, w0=72.63366336633673, w1=15.975893251624171\n",
      "SubGD iter. 494/499: loss=5.310625439575519, w0=72.62673267326743, w1=15.973767151838434\n",
      "SubGD iter. 495/499: loss=5.310578562774669, w0=72.62673267326743, w1=15.972157491117347\n",
      "SubGD iter. 496/499: loss=5.310574861335188, w0=72.62673267326743, w1=15.97054783039626\n",
      "SubGD iter. 497/499: loss=5.310583930633671, w0=72.63366336633673, w1=15.975016530203664\n",
      "SubGD iter. 498/499: loss=5.310622776722341, w0=72.62673267326743, w1=15.972890430417927\n",
      "SubGD iter. 499/499: loss=5.310576546740335, w0=72.62673267326743, w1=15.97128076969684\n",
      "SubGD: execution time=0.024 seconds\n"
     ]
    }
   ],
   "source": [
    "# Define the parameters of the algorithm.\n",
    "max_iters = 500\n",
    "gamma = 0.7\n",
    "batch_size = 1\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.array([0, 0])\n",
    "\n",
    "# Start SubSGD.\n",
    "start_time = datetime.datetime.now()\n",
    "subgd_losses, subgd_ws = subgradient_descent(\n",
    "    y, tx, w_initial, max_iters, gamma)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"SubGD: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52cf6103baae413a8030feb173d087ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='n_iter', max=501, min=1), Output()), _dom_classes=('widg"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_figure(n_iter)>"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ipywidgets import IntSlider, interact\n",
    "def plot_figure(n_iter):\n",
    "    fig = gradient_descent_visualization(\n",
    "        subgd_losses, subgd_ws, grid_losses, grid_w0, grid_w1, mean_x, std_x, height, weight, n_iter)\n",
    "    fig.set_size_inches(10.0, 6.0)\n",
    "\n",
    "interact(plot_figure, n_iter=IntSlider(min=1, max=len(subgd_ws)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic Subgradient Descent\n",
    "\n",
    "**NB** for the computation of the subgradient you can reuse the `compute_subgradient` method that you implemented above, just making sure that you pass in a minibatch as opposed to the full data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stochastic_subgradient_descent(y, tx, initial_w, batch_size, max_iters, gamma):\n",
    "    \"\"\"The Stochastic SubGradient Descent algorithm (SubSGD).\n",
    "            \n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        initial_w: numpy array of shape=(2, ). The initial guess (or the initialization) for the model parameters\n",
    "        batch_size: a scalar denoting the number of data points in a mini-batch used for computing the stochastic subgradient\n",
    "        max_iters: a scalar denoting the total number of iterations of SubSGD\n",
    "        gamma: a scalar denoting the stepsize\n",
    "        \n",
    "    Returns:\n",
    "        losses: a list of length max_iters containing the loss value (scalar) for each iteration of SubSGD\n",
    "        ws: a list of length max_iters containing the model parameters as numpy arrays of shape (2, ), for each iteration of SubSGD \n",
    "    \"\"\"\n",
    "    \n",
    "    # Define parameters to store w and loss\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    \n",
    "    for n_iter in range(max_iters):\n",
    "\n",
    "        ### SOLUTION\n",
    "        for y_batch, tx_batch in batch_iter(y, tx, batch_size=batch_size, num_batches=1):\n",
    "            # compute a stochastic subgradient and loss\n",
    "            grad, err = compute_subgradient_mae(y_batch, tx_batch, w)\n",
    "            # update w through the stochastic subgradient update\n",
    "            w = w - gamma * grad\n",
    "            # calculate loss\n",
    "            loss = calculate_mae(err)\n",
    "            # store w and loss\n",
    "            ws.append(w)\n",
    "            losses.append(loss)\n",
    "\n",
    "        ### TEMPLATE\n",
    "        # # ***************************************************\n",
    "        # # INSERT YOUR CODE HERE\n",
    "        # # TODO: implement stochastic subgradient descent.\n",
    "        # # ***************************************************\n",
    "        # raise NotImplementedError\n",
    "        ### END SOLUTION\n",
    "        \n",
    "        print(\"SubSGD iter. {bi}/{ti}: loss={l}, w0={w0}, w1={w1}\".format(\n",
    "              bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]))\n",
    "    return losses, ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SubSGD iter. 0/499: loss=113.47561257055952, w0=0.7, w1=1.239316300978001\n",
      "SubSGD iter. 1/499: loss=86.90328827780351, w0=1.4, w1=1.4836290246871353\n",
      "SubSGD iter. 2/499: loss=72.91204801413195, w0=2.0999999999999996, w1=1.4596631848840445\n",
      "SubSGD iter. 3/499: loss=47.775558408903585, w0=2.8, w1=0.7790640072279618\n",
      "SubSGD iter. 4/499: loss=66.59107058192123, w0=3.5, w1=0.25443108251622226\n",
      "SubSGD iter. 5/499: loss=57.8916499072674, w0=4.2, w1=-0.02811673657000291\n",
      "SubSGD iter. 6/499: loss=80.14237651298956, w0=4.9, w1=0.4999390789406458\n",
      "SubSGD iter. 7/499: loss=50.66140782945712, w0=5.6000000000000005, w1=-0.008562038799187599\n",
      "SubSGD iter. 8/499: loss=83.87658020169829, w0=6.300000000000001, w1=0.5156185126525306\n",
      "SubSGD iter. 9/499: loss=52.36411899371199, w0=7.000000000000001, w1=-0.011338523116317334\n",
      "SubSGD iter. 10/499: loss=86.76081304462494, w0=7.700000000000001, w1=0.6016539740672424\n",
      "SubSGD iter. 11/499: loss=81.73151045025607, w0=8.4, w1=1.233928402548933\n",
      "SubSGD iter. 12/499: loss=48.41830377189213, w0=9.1, w1=0.6984087197227276\n",
      "SubSGD iter. 13/499: loss=73.95574112821993, w0=9.799999999999999, w1=1.2907266809909057\n",
      "SubSGD iter. 14/499: loss=73.46385098755688, w0=10.499999999999998, w1=1.9218920209857582\n",
      "SubSGD iter. 15/499: loss=66.50492419168556, w0=11.199999999999998, w1=2.0876982805532553\n",
      "SubSGD iter. 16/499: loss=69.91780835824298, w0=11.899999999999997, w1=2.5363573193379656\n",
      "SubSGD iter. 17/499: loss=54.97557655293258, w0=12.599999999999996, w1=1.9520447817920743\n",
      "SubSGD iter. 18/499: loss=67.94461644040676, w0=13.299999999999995, w1=2.2221832320817443\n",
      "SubSGD iter. 19/499: loss=48.00413316998153, w0=13.999999999999995, w1=2.004926227901994\n",
      "SubSGD iter. 20/499: loss=77.9951628093251, w0=14.699999999999994, w1=2.617918725085554\n",
      "SubSGD iter. 21/499: loss=68.66238015098207, w0=15.399999999999993, w1=3.1271045690318062\n",
      "SubSGD iter. 22/499: loss=59.717821284635306, w0=16.099999999999994, w1=2.7449785844814714\n",
      "SubSGD iter. 23/499: loss=42.733225358845736, w0=16.799999999999994, w1=1.9451478504187618\n",
      "SubSGD iter. 24/499: loss=56.284747153240204, w0=17.499999999999993, w1=1.9104569910772906\n",
      "SubSGD iter. 25/499: loss=87.11935647464284, w0=18.199999999999992, w1=3.4505193294644765\n",
      "SubSGD iter. 26/499: loss=74.74131214316577, w0=18.89999999999999, w1=4.4629576673308975\n",
      "SubSGD iter. 27/499: loss=42.268260148564394, w0=19.59999999999999, w1=3.806076699577934\n",
      "SubSGD iter. 28/499: loss=49.25523264966933, w0=20.29999999999999, w1=4.066788920959798\n",
      "SubSGD iter. 29/499: loss=44.439090166993516, w0=20.99999999999999, w1=3.4630228509932843\n",
      "SubSGD iter. 30/499: loss=59.47821918542657, w0=21.69999999999999, w1=3.8415626343500158\n",
      "SubSGD iter. 31/499: loss=58.69610324441404, w0=22.399999999999988, w1=4.433880595618194\n",
      "SubSGD iter. 32/499: loss=37.6499067457161, w0=23.099999999999987, w1=3.64964786757887\n",
      "SubSGD iter. 33/499: loss=38.455058641659605, w0=23.799999999999986, w1=3.0285962940082767\n",
      "SubSGD iter. 34/499: loss=28.13060553809388, w0=24.499999999999986, w1=1.9388012709350564\n",
      "SubSGD iter. 35/499: loss=32.85755133605062, w0=25.199999999999985, w1=1.403281588108851\n",
      "SubSGD iter. 36/499: loss=23.405243350029593, w0=25.899999999999984, w1=0.27734917460768016\n",
      "SubSGD iter. 37/499: loss=44.09541668289721, w0=26.599999999999984, w1=-0.05901378306839655\n",
      "SubSGD iter. 38/499: loss=38.97029788748411, w0=27.299999999999983, w1=-0.7457084185745332\n",
      "SubSGD iter. 39/499: loss=55.45417950398215, w0=27.999999999999982, w1=-0.36716863521780174\n",
      "SubSGD iter. 40/499: loss=54.09967250212482, w0=28.69999999999998, w1=-0.21427487810322018\n",
      "SubSGD iter. 41/499: loss=44.27772968162031, w0=29.39999999999998, w1=-0.24896573744469128\n",
      "SubSGD iter. 42/499: loss=33.10928233929561, w0=30.09999999999998, w1=-0.19265612239938779\n",
      "SubSGD iter. 43/499: loss=25.376638895970594, w0=30.79999999999998, w1=-0.9924868564620973\n",
      "SubSGD iter. 44/499: loss=46.89524170687437, w0=31.49999999999998, w1=-0.8266805968946003\n",
      "SubSGD iter. 45/499: loss=45.107449480347796, w0=32.19999999999998, w1=-0.2782860931367894\n",
      "SubSGD iter. 46/499: loss=24.41051074924303, w0=32.899999999999984, w1=-1.31580028695033\n",
      "SubSGD iter. 47/499: loss=78.81740980012556, w0=33.59999999999999, w1=0.22426205143685585\n",
      "SubSGD iter. 48/499: loss=31.60633053278226, w0=34.29999999999999, w1=0.10375749995781353\n",
      "SubSGD iter. 49/499: loss=39.96480547140267, w0=34.99999999999999, w1=0.07979166015472262\n",
      "SubSGD iter. 50/499: loss=47.307772902606224, w0=35.699999999999996, w1=0.45833144351145405\n",
      "SubSGD iter. 51/499: loss=31.075377334717494, w0=36.4, w1=0.377517138022743\n",
      "SubSGD iter. 52/499: loss=72.81288362327265, w0=37.1, w1=1.5026470981530502\n",
      "SubSGD iter. 53/499: loss=53.83869568747268, w0=37.800000000000004, w1=2.0436041501241924\n",
      "SubSGD iter. 54/499: loss=58.445266518239514, w0=38.50000000000001, w1=3.406355545515939\n",
      "SubSGD iter. 55/499: loss=42.008863227624836, w0=39.20000000000001, w1=3.7848953288726706\n",
      "SubSGD iter. 56/499: loss=30.169115724229464, w0=39.90000000000001, w1=3.983656194691195\n",
      "SubSGD iter. 57/499: loss=97.6717696132567, w0=40.600000000000016, w1=1.2133564978022058\n",
      "SubSGD iter. 58/499: loss=48.27899064511986, w0=41.30000000000002, w1=1.8456309262838966\n",
      "SubSGD iter. 59/499: loss=34.961711854242544, w0=42.00000000000002, w1=2.0959120427459528\n",
      "SubSGD iter. 60/499: loss=33.90921072275537, w0=42.700000000000024, w1=1.8549321280793047\n",
      "SubSGD iter. 61/499: loss=23.59476348801033, w0=43.40000000000003, w1=1.300825054082724\n",
      "SubSGD iter. 62/499: loss=18.273844803720486, w0=44.10000000000003, w1=0.8611366489975396\n",
      "SubSGD iter. 63/499: loss=26.401838051442887, w0=44.80000000000003, w1=0.4308050605368036\n",
      "SubSGD iter. 64/499: loss=64.3272324259388, w0=45.500000000000036, w1=1.5559350206671108\n",
      "SubSGD iter. 65/499: loss=29.24085903857864, w0=46.20000000000004, w1=2.1043295244249216\n",
      "SubSGD iter. 66/499: loss=29.712108499922074, w0=46.90000000000004, w1=1.8633496097582736\n",
      "SubSGD iter. 67/499: loss=42.772654075925225, w0=47.600000000000044, w1=3.031680410275742\n",
      "SubSGD iter. 68/499: loss=33.48140008417258, w0=48.30000000000005, w1=3.6239983715439203\n",
      "SubSGD iter. 69/499: loss=26.939329051564762, w0=49.00000000000005, w1=3.9592264268471755\n",
      "SubSGD iter. 70/499: loss=38.73814344299382, w0=49.70000000000005, w1=4.602156125610971\n",
      "SubSGD iter. 71/499: loss=40.8218302786604, w0=50.400000000000055, w1=5.481404091507144\n",
      "SubSGD iter. 72/499: loss=17.831263612681056, w0=51.10000000000006, w1=5.742116312889007\n",
      "SubSGD iter. 73/499: loss=18.49598281468362, w0=51.80000000000006, w1=5.537282793013785\n",
      "SubSGD iter. 74/499: loss=31.334198549667136, w0=52.500000000000064, w1=6.298639743632854\n",
      "SubSGD iter. 75/499: loss=7.273758928649485, w0=53.20000000000007, w1=5.79013862589302\n",
      "SubSGD iter. 76/499: loss=25.299476196820166, w0=53.90000000000007, w1=5.816511276517841\n",
      "SubSGD iter. 77/499: loss=24.094961484438713, w0=54.60000000000007, w1=6.160223897701232\n",
      "SubSGD iter. 78/499: loss=10.317499924922103, w0=55.300000000000075, w1=6.0097672212501045\n",
      "SubSGD iter. 79/499: loss=29.685234896142212, w0=56.00000000000008, w1=6.287894190051793\n",
      "SubSGD iter. 80/499: loss=4.093830446353834, w0=56.70000000000008, w1=5.553779867050803\n",
      "SubSGD iter. 81/499: loss=6.544436033885461, w0=57.400000000000084, w1=4.93272829348021\n",
      "SubSGD iter. 82/499: loss=10.793350675645414, w0=58.10000000000009, w1=4.55714245519364\n",
      "SubSGD iter. 83/499: loss=18.280722738086006, w0=58.80000000000009, w1=4.722948714761137\n",
      "SubSGD iter. 84/499: loss=30.320569296477387, w0=59.50000000000009, w1=5.162873157645319\n",
      "SubSGD iter. 85/499: loss=13.935152404525695, w0=60.200000000000095, w1=5.21503350317503\n",
      "SubSGD iter. 86/499: loss=0.5429680970792816, w0=60.9000000000001, w1=4.935104510738567\n",
      "SubSGD iter. 87/499: loss=0.16213590148907286, w0=61.6000000000001, w1=4.365144386202581\n",
      "SubSGD iter. 88/499: loss=4.05961351446647, w0=60.9000000000001, w1=4.796936964486001\n",
      "SubSGD iter. 89/499: loss=15.870105710510884, w0=61.6000000000001, w1=4.999373854527\n",
      "SubSGD iter. 90/499: loss=29.829865664912376, w0=62.300000000000104, w1=6.117684075138481\n",
      "SubSGD iter. 91/499: loss=22.149160765052216, w0=63.00000000000011, w1=6.749958503620172\n",
      "SubSGD iter. 92/499: loss=5.794761158981018, w0=62.300000000000104, w1=7.8758909171213425\n",
      "SubSGD iter. 93/499: loss=11.59655367288314, w0=63.00000000000011, w1=7.9861909505821345\n",
      "SubSGD iter. 94/499: loss=8.33545724333377, w0=63.70000000000011, w1=7.992618831460481\n",
      "SubSGD iter. 95/499: loss=8.22171315301371, w0=64.4000000000001, w1=8.034945388063973\n",
      "SubSGD iter. 96/499: loss=16.736059813135256, w0=65.10000000000011, w1=8.876135556288615\n",
      "SubSGD iter. 97/499: loss=11.036006145873003, w0=65.80000000000011, w1=9.468453517556792\n",
      "SubSGD iter. 98/499: loss=17.888843165925636, w0=66.50000000000011, w1=10.021659433632905\n",
      "SubSGD iter. 99/499: loss=14.199802778735972, w0=67.20000000000012, w1=10.854997016192481\n",
      "SubSGD iter. 100/499: loss=3.9875789162422137, w0=66.50000000000011, w1=11.402159318426351\n",
      "SubSGD iter. 101/499: loss=10.255249253744978, w0=67.20000000000012, w1=12.16351626904542\n",
      "SubSGD iter. 102/499: loss=0.8593262202099083, w0=66.50000000000011, w1=12.982174638851049\n",
      "SubSGD iter. 103/499: loss=9.522861526076802, w0=67.20000000000012, w1=12.726742757749479\n",
      "SubSGD iter. 104/499: loss=2.4084073059309787, w0=67.90000000000012, w1=12.105691184178886\n",
      "SubSGD iter. 105/499: loss=9.514533786668459, w0=68.60000000000012, w1=11.675359595718149\n",
      "SubSGD iter. 106/499: loss=7.858102210849012, w0=69.30000000000013, w1=12.436716546337218\n",
      "SubSGD iter. 107/499: loss=10.64421132317922, w0=70.00000000000013, w1=13.449154884203638\n",
      "SubSGD iter. 108/499: loss=12.377364676297006, w0=70.70000000000013, w1=14.112299201246685\n",
      "SubSGD iter. 109/499: loss=10.149825790666874, w0=71.40000000000013, w1=13.712841428860106\n",
      "SubSGD iter. 110/499: loss=12.722925159711764, w0=72.10000000000014, w1=14.06080847525052\n",
      "SubSGD iter. 111/499: loss=3.2819238237811987, w0=72.80000000000014, w1=14.939980787481327\n",
      "SubSGD iter. 112/499: loss=4.154385941799141, w0=73.50000000000014, w1=14.366970704466704\n",
      "SubSGD iter. 113/499: loss=4.758167483676623, w0=72.80000000000014, w1=14.936930829002689\n",
      "SubSGD iter. 114/499: loss=6.37077646601378, w0=73.50000000000014, w1=14.409742190255463\n",
      "SubSGD iter. 115/499: loss=1.9758680980570773, w0=74.20000000000014, w1=15.24307977281504\n",
      "SubSGD iter. 116/499: loss=8.765013883898618, w0=73.50000000000014, w1=15.268377538203882\n",
      "SubSGD iter. 117/499: loss=4.344853579498427, w0=72.80000000000014, w1=16.109796047929827\n",
      "SubSGD iter. 118/499: loss=9.460937241144151, w0=72.10000000000014, w1=15.561401544172016\n",
      "SubSGD iter. 119/499: loss=3.1926013757943963, w0=72.80000000000014, w1=15.831539994461686\n",
      "SubSGD iter. 120/499: loss=5.632503778610086, w0=73.50000000000014, w1=16.844218335814453\n",
      "SubSGD iter. 121/499: loss=4.600822427223193, w0=72.80000000000014, w1=15.964488205981976\n",
      "SubSGD iter. 122/499: loss=4.519346479111107, w0=73.50000000000014, w1=16.129940997914602\n",
      "SubSGD iter. 123/499: loss=4.529155215227689, w0=72.80000000000014, w1=16.553180422687117\n",
      "SubSGD iter. 124/499: loss=3.3709619447216284, w0=73.50000000000014, w1=17.19611012145091\n",
      "SubSGD iter. 125/499: loss=0.31568007945460863, w0=72.80000000000014, w1=16.99367323140991\n",
      "SubSGD iter. 126/499: loss=0.7054422116146242, w0=73.50000000000014, w1=17.788842956527258\n",
      "SubSGD iter. 127/499: loss=7.7287836009708215, w0=72.80000000000014, w1=18.06877194896372\n",
      "SubSGD iter. 128/499: loss=0.5937668723320613, w0=72.10000000000014, w1=18.508460354048907\n",
      "SubSGD iter. 129/499: loss=2.679127848314991, w0=72.80000000000014, w1=18.651790944643423\n",
      "SubSGD iter. 130/499: loss=1.107392256066106, w0=72.10000000000014, w1=18.308078323460034\n",
      "SubSGD iter. 131/499: loss=1.087025654572777, w0=71.40000000000013, w1=18.93289442327098\n",
      "SubSGD iter. 132/499: loss=5.385654560592535, w0=72.10000000000014, w1=18.031262158858222\n",
      "SubSGD iter. 133/499: loss=6.110140716605983, w0=72.80000000000014, w1=17.35291516288368\n",
      "SubSGD iter. 134/499: loss=4.191176312805524, w0=73.50000000000014, w1=17.518367954816306\n",
      "SubSGD iter. 135/499: loss=9.706253299547072, w0=74.20000000000014, w1=16.779035155372362\n",
      "SubSGD iter. 136/499: loss=2.817114948676604, w0=73.50000000000014, w1=17.204821302663103\n",
      "SubSGD iter. 137/499: loss=3.3833822771515685, w0=72.80000000000014, w1=16.36363113443846\n",
      "SubSGD iter. 138/499: loss=2.7151710812563437, w0=73.50000000000014, w1=16.18016535538804\n",
      "SubSGD iter. 139/499: loss=0.20453579158151314, w0=72.80000000000014, w1=17.123671838925162\n",
      "SubSGD iter. 140/499: loss=1.0369700685206595, w0=73.50000000000014, w1=17.088980979583692\n",
      "SubSGD iter. 141/499: loss=0.3205158992866046, w0=74.20000000000014, w1=15.999185956510471\n",
      "SubSGD iter. 142/499: loss=7.3700842013719665, w0=73.50000000000014, w1=15.820833835130657\n",
      "SubSGD iter. 143/499: loss=1.8294408429636206, w0=74.20000000000014, w1=15.964164425725173\n",
      "SubSGD iter. 144/499: loss=2.351964964536137, w0=74.90000000000015, w1=16.696129019232806\n",
      "SubSGD iter. 145/499: loss=2.24712698959263, w0=74.20000000000014, w1=16.352416398049417\n",
      "SubSGD iter. 146/499: loss=3.5303571442481285, w0=73.50000000000014, w1=17.192443969509227\n",
      "SubSGD iter. 147/499: loss=3.886736434489407, w0=72.80000000000014, w1=17.615683394281742\n",
      "SubSGD iter. 148/499: loss=2.733540238813333, w0=72.10000000000014, w1=16.075621055894555\n",
      "SubSGD iter. 149/499: loss=2.9969835712749955, w0=72.80000000000014, w1=15.47185498592804\n",
      "SubSGD iter. 150/499: loss=3.6296995049960543, w0=72.10000000000014, w1=15.116827508398215\n",
      "SubSGD iter. 151/499: loss=12.314648873972288, w0=71.40000000000013, w1=15.236719321886547\n",
      "SubSGD iter. 152/499: loss=1.9499931546771663, w0=70.70000000000013, w1=15.806679446422532\n",
      "SubSGD iter. 153/499: loss=3.2057288484863875, w0=70.00000000000013, w1=16.36480315017157\n",
      "SubSGD iter. 154/499: loss=2.7915369644746875, w0=70.70000000000013, w1=16.25462903329083\n",
      "SubSGD iter. 155/499: loss=7.949557152849067, w0=70.00000000000013, w1=15.296493942389022\n",
      "SubSGD iter. 156/499: loss=0.193718156147213, w0=69.30000000000013, w1=15.691930734332027\n",
      "SubSGD iter. 157/499: loss=10.166534714149023, w0=70.00000000000013, w1=15.957414436556258\n",
      "SubSGD iter. 158/499: loss=4.676727805794478, w0=69.30000000000013, w1=16.43546263898238\n",
      "SubSGD iter. 159/499: loss=0.9672228513181551, w0=68.60000000000012, w1=15.734528641547627\n",
      "SubSGD iter. 160/499: loss=0.9599675148094065, w0=67.90000000000012, w1=16.017076460633852\n",
      "SubSGD iter. 161/499: loss=3.548939998028814, w0=68.60000000000012, w1=16.30557372519127\n",
      "SubSGD iter. 162/499: loss=0.225651926613736, w0=69.30000000000013, w1=17.38944912188257\n",
      "SubSGD iter. 163/499: loss=4.042196060173808, w0=70.00000000000013, w1=17.370869662983694\n",
      "SubSGD iter. 164/499: loss=14.558002302108825, w0=70.70000000000013, w1=16.34393746539116\n",
      "SubSGD iter. 165/499: loss=8.550077025222564, w0=71.40000000000013, w1=15.490091194965892\n",
      "SubSGD iter. 166/499: loss=6.395874999500521, w0=70.70000000000013, w1=15.968139397392015\n",
      "SubSGD iter. 167/499: loss=4.18487318414266, w0=71.40000000000013, w1=16.05666691088114\n",
      "SubSGD iter. 168/499: loss=2.1146413722524358, w0=70.70000000000013, w1=15.721438855577887\n",
      "SubSGD iter. 169/499: loss=2.825840410111553, w0=70.00000000000013, w1=15.84194340705693\n",
      "SubSGD iter. 170/499: loss=12.058778869462579, w0=70.70000000000013, w1=15.459817422506594\n",
      "SubSGD iter. 171/499: loss=1.0826586040394375, w0=71.40000000000013, w1=16.205803076093993\n",
      "SubSGD iter. 172/499: loss=8.535116162832132, w0=72.10000000000014, w1=15.775471487633256\n",
      "SubSGD iter. 173/499: loss=2.8575266600237796, w0=71.40000000000013, w1=15.279845227164424\n",
      "SubSGD iter. 174/499: loss=6.34094029423494, w0=72.10000000000014, w1=15.998008551195612\n",
      "SubSGD iter. 175/499: loss=0.5354340894538439, w0=71.40000000000013, w1=15.378875155250945\n",
      "SubSGD iter. 176/499: loss=10.172602375159649, w0=72.10000000000014, w1=14.979417382864366\n",
      "SubSGD iter. 177/499: loss=4.792121190731692, w0=72.80000000000014, w1=15.595804331701615\n",
      "SubSGD iter. 178/499: loss=5.215962159382606, w0=72.10000000000014, w1=15.87835215078784\n",
      "SubSGD iter. 179/499: loss=10.888036320703996, w0=71.40000000000013, w1=15.822042535742536\n",
      "SubSGD iter. 180/499: loss=0.780636006802311, w0=70.70000000000013, w1=16.640700905548165\n",
      "SubSGD iter. 181/499: loss=2.6449417982132104, w0=71.40000000000013, w1=16.843137795589165\n",
      "SubSGD iter. 182/499: loss=7.400369005519622, w0=70.70000000000013, w1=16.5824255742073\n",
      "SubSGD iter. 183/499: loss=0.19006321448485153, w0=71.40000000000013, w1=15.84831125120631\n",
      "SubSGD iter. 184/499: loss=7.154853762309045, w0=72.10000000000014, w1=15.88559856282418\n",
      "SubSGD iter. 185/499: loss=5.0276309818418525, w0=71.40000000000013, w1=16.50838496964447\n",
      "SubSGD iter. 186/499: loss=0.9145664247282497, w0=70.70000000000013, w1=17.078345094180456\n",
      "SubSGD iter. 187/499: loss=9.587911337921696, w0=71.40000000000013, w1=17.19216542099477\n",
      "SubSGD iter. 188/499: loss=6.864368850289864, w0=72.10000000000014, w1=17.345059178109352\n",
      "SubSGD iter. 189/499: loss=1.241017858551146, w0=72.80000000000014, w1=17.326479719210475\n",
      "SubSGD iter. 190/499: loss=1.727672377864252, w0=72.10000000000014, w1=15.961110266546864\n",
      "SubSGD iter. 191/499: loss=1.619614467344789, w0=72.80000000000014, w1=16.339650049903597\n",
      "SubSGD iter. 192/499: loss=0.9981152262297996, w0=73.50000000000014, w1=16.304959190562126\n",
      "SubSGD iter. 193/499: loss=3.1275263474196393, w0=72.80000000000014, w1=16.661684708917964\n",
      "SubSGD iter. 194/499: loss=7.674535157450343, w0=72.10000000000014, w1=17.093477287201384\n",
      "SubSGD iter. 195/499: loss=0.11112755778880512, w0=71.40000000000013, w1=16.563776313139268\n",
      "SubSGD iter. 196/499: loss=3.195707732107195, w0=70.70000000000013, w1=15.862842315704516\n",
      "SubSGD iter. 197/499: loss=3.0727549370916023, w0=71.40000000000013, w1=16.241382099061248\n",
      "SubSGD iter. 198/499: loss=1.9779762006423596, w0=72.10000000000014, w1=15.297875615524127\n",
      "SubSGD iter. 199/499: loss=4.298756550491106, w0=71.40000000000013, w1=15.41838016700317\n",
      "SubSGD iter. 200/499: loss=0.880570600983134, w0=70.70000000000013, w1=14.81384806114901\n",
      "SubSGD iter. 201/499: loss=0.5222066292317749, w0=70.00000000000013, w1=13.619109224044584\n",
      "SubSGD iter. 202/499: loss=1.0416064027639607, w0=69.30000000000013, w1=12.961926520849598\n",
      "SubSGD iter. 203/499: loss=9.409002766475538, w0=70.00000000000013, w1=14.08023674146108\n",
      "SubSGD iter. 204/499: loss=3.3801874894745865, w0=69.30000000000013, w1=14.230693417912208\n",
      "SubSGD iter. 205/499: loss=7.7696757563433465, w0=70.00000000000013, w1=14.42721350069071\n",
      "SubSGD iter. 206/499: loss=4.069046990733838, w0=70.70000000000013, w1=15.152233690771514\n",
      "SubSGD iter. 207/499: loss=9.703941117900271, w0=71.40000000000013, w1=14.911253776104866\n",
      "SubSGD iter. 208/499: loss=1.388455758679882, w0=70.70000000000013, w1=13.71651493900044\n",
      "SubSGD iter. 209/499: loss=3.723353217722135, w0=70.00000000000013, w1=12.76960267725801\n",
      "SubSGD iter. 210/499: loss=2.7414746042556573, w0=70.70000000000013, w1=12.403681946527612\n",
      "SubSGD iter. 211/499: loss=2.315647986173964, w0=70.00000000000013, w1=12.76040746488345\n",
      "SubSGD iter. 212/499: loss=6.313448124112199, w0=70.70000000000013, w1=13.010005927570628\n",
      "SubSGD iter. 213/499: loss=4.310220877036805, w0=70.00000000000013, w1=13.160462604021756\n",
      "SubSGD iter. 214/499: loss=0.10940079639027545, w0=69.30000000000013, w1=13.510799537390147\n",
      "SubSGD iter. 215/499: loss=2.5064952134946665, w0=68.60000000000012, w1=13.857883523010138\n",
      "SubSGD iter. 216/499: loss=4.5225386607314135, w0=69.30000000000013, w1=12.881086044068219\n",
      "SubSGD iter. 217/499: loss=14.69347783995154, w0=70.00000000000013, w1=13.321010486952401\n",
      "SubSGD iter. 218/499: loss=6.766886788846428, w0=70.70000000000013, w1=12.28349629313886\n",
      "SubSGD iter. 219/499: loss=1.5121856449684685, w0=71.40000000000013, w1=12.415819819492699\n",
      "SubSGD iter. 220/499: loss=2.8706754747397127, w0=72.10000000000014, w1=13.610558656597124\n",
      "SubSGD iter. 221/499: loss=3.8474197868483486, w0=72.80000000000014, w1=14.489730968827931\n",
      "SubSGD iter. 222/499: loss=8.520613856206182, w0=73.50000000000014, w1=14.107604984277597\n",
      "SubSGD iter. 223/499: loss=3.3804264832500763, w0=74.20000000000014, w1=14.753752217664818\n",
      "SubSGD iter. 224/499: loss=8.28419229469165, w0=73.50000000000014, w1=15.262253335404651\n",
      "SubSGD iter. 225/499: loss=5.205712068492055, w0=72.80000000000014, w1=15.887069435215597\n",
      "SubSGD iter. 226/499: loss=13.140166431003152, w0=73.50000000000014, w1=16.231537682086064\n",
      "SubSGD iter. 227/499: loss=4.5116361180785205, w0=74.20000000000014, w1=15.65852759907144\n",
      "SubSGD iter. 228/499: loss=5.79909085342598, w0=74.90000000000015, w1=16.19948465104258\n",
      "SubSGD iter. 229/499: loss=3.474527376486037, w0=74.20000000000014, w1=16.15715809443909\n",
      "SubSGD iter. 230/499: loss=2.6165384152585744, w0=73.50000000000014, w1=15.395801143820021\n",
      "SubSGD iter. 231/499: loss=0.21175079239321093, w0=72.80000000000014, w1=16.195631877882732\n",
      "SubSGD iter. 232/499: loss=1.3903552311639658, w0=72.10000000000014, w1=16.545968811251125\n",
      "SubSGD iter. 233/499: loss=3.607916282635955, w0=71.40000000000013, w1=15.339952521480678\n",
      "SubSGD iter. 234/499: loss=11.576433356182193, w0=70.70000000000013, w1=15.45984433496901\n",
      "SubSGD iter. 235/499: loss=5.287331696361321, w0=70.00000000000013, w1=15.67710133914876\n",
      "SubSGD iter. 236/499: loss=8.713781235732597, w0=70.70000000000013, w1=14.323144445115531\n",
      "SubSGD iter. 237/499: loss=2.8535790884193943, w0=71.40000000000013, w1=13.957223714385133\n",
      "SubSGD iter. 238/499: loss=6.393785785228147, w0=72.10000000000014, w1=14.12267650631776\n",
      "SubSGD iter. 239/499: loss=7.692753535565991, w0=72.80000000000014, w1=13.095744308725227\n",
      "SubSGD iter. 240/499: loss=3.0874246875959415, w0=73.50000000000014, w1=13.239074899319743\n",
      "SubSGD iter. 241/499: loss=5.857180466266712, w0=72.80000000000014, w1=14.365007312820914\n",
      "SubSGD iter. 242/499: loss=2.0174498417570277, w0=73.50000000000014, w1=14.874193156767166\n",
      "SubSGD iter. 243/499: loss=3.3890177249894435, w0=74.20000000000014, w1=15.070713239545668\n",
      "SubSGD iter. 244/499: loss=1.9981256171578252, w0=74.90000000000015, w1=15.722770136988123\n",
      "SubSGD iter. 245/499: loss=10.739204960131516, w0=75.60000000000015, w1=16.962086437966125\n",
      "SubSGD iter. 246/499: loss=0.4710166937196192, w0=74.90000000000015, w1=17.08910088319415\n",
      "SubSGD iter. 247/499: loss=9.244634541762743, w0=75.60000000000015, w1=16.062168685601616\n",
      "SubSGD iter. 248/499: loss=2.9841705384602335, w0=74.90000000000015, w1=16.26700220547684\n",
      "SubSGD iter. 249/499: loss=7.045040023081903, w0=74.20000000000014, w1=16.549550024563064\n",
      "SubSGD iter. 250/499: loss=2.730227166957718, w0=74.90000000000015, w1=15.365547824169173\n",
      "SubSGD iter. 251/499: loss=6.6837670131051965, w0=75.60000000000015, w1=15.576712218735377\n",
      "SubSGD iter. 252/499: loss=11.921648740366308, w0=74.90000000000015, w1=14.61857712783357\n",
      "SubSGD iter. 253/499: loss=108.97978193960157, w0=75.60000000000015, w1=11.245433450243892\n",
      "SubSGD iter. 254/499: loss=1.9094569767800635, w0=74.90000000000015, w1=12.282947644057433\n",
      "SubSGD iter. 255/499: loss=0.08017536356513233, w0=74.20000000000014, w1=11.939235022874042\n",
      "SubSGD iter. 256/499: loss=7.762972338466135, w0=73.50000000000014, w1=12.362474447646557\n",
      "SubSGD iter. 257/499: loss=1.846180008758374, w0=74.20000000000014, w1=11.324960253833016\n",
      "SubSGD iter. 258/499: loss=1.4977662844883497, w0=74.90000000000015, w1=11.944093649777683\n",
      "SubSGD iter. 259/499: loss=10.485444121160484, w0=74.20000000000014, w1=12.502217353526722\n",
      "SubSGD iter. 260/499: loss=10.137807876472806, w0=73.50000000000014, w1=13.125003760347015\n",
      "SubSGD iter. 261/499: loss=8.787057614384537, w0=72.80000000000014, w1=12.51960060890303\n",
      "SubSGD iter. 262/499: loss=8.722342083394722, w0=72.10000000000014, w1=13.142387015723322\n",
      "SubSGD iter. 263/499: loss=11.348900239486426, w0=72.80000000000014, w1=13.386699739432457\n",
      "SubSGD iter. 264/499: loss=0.5452172111940854, w0=73.50000000000014, w1=14.47057513612376\n",
      "SubSGD iter. 265/499: loss=2.9405777831470203, w0=72.80000000000014, w1=15.127456103876723\n",
      "SubSGD iter. 266/499: loss=8.093068372608123, w0=72.10000000000014, w1=15.407385096313186\n",
      "SubSGD iter. 267/499: loss=3.2520381822854034, w0=72.80000000000014, w1=15.677523546602856\n",
      "SubSGD iter. 268/499: loss=4.930378784162599, w0=73.50000000000014, w1=16.201704098054574\n",
      "SubSGD iter. 269/499: loss=4.274192591413019, w0=72.80000000000014, w1=16.706721829878894\n",
      "SubSGD iter. 270/499: loss=2.1063138401361954, w0=73.50000000000014, w1=16.956320292566073\n",
      "SubSGD iter. 271/499: loss=5.539224820734972, w0=72.80000000000014, w1=17.13818413614124\n",
      "SubSGD iter. 272/499: loss=0.1512979751799861, w0=72.10000000000014, w1=17.569194820889855\n",
      "SubSGD iter. 273/499: loss=5.670104789296488, w0=71.40000000000013, w1=17.390842699510042\n",
      "SubSGD iter. 274/499: loss=7.560626781326221, w0=72.10000000000014, w1=16.81783261649542\n",
      "SubSGD iter. 275/499: loss=3.1791615140475926, w0=71.40000000000013, w1=16.482604561192165\n",
      "SubSGD iter. 276/499: loss=1.1916651169363348, w0=72.10000000000014, w1=16.534764906721875\n",
      "SubSGD iter. 277/499: loss=4.1369603555541445, w0=71.40000000000013, w1=16.8173127258081\n",
      "SubSGD iter. 278/499: loss=8.427490883249128, w0=72.10000000000014, w1=15.963466455382832\n",
      "SubSGD iter. 279/499: loss=7.686137444356646, w0=72.80000000000014, w1=15.533134866922095\n",
      "SubSGD iter. 280/499: loss=5.638555754150772, w0=72.10000000000014, w1=14.87595216372711\n",
      "SubSGD iter. 281/499: loss=3.4229129115410615, w0=72.80000000000014, w1=15.019282754321626\n",
      "SubSGD iter. 282/499: loss=8.308116142191423, w0=73.50000000000014, w1=13.992350556729093\n",
      "SubSGD iter. 283/499: loss=13.829373649601294, w0=74.20000000000014, w1=15.1174805168594\n",
      "SubSGD iter. 284/499: loss=10.936550078348041, w0=73.50000000000014, w1=15.388960698841979\n",
      "SubSGD iter. 285/499: loss=9.596219893209692, w0=72.80000000000014, w1=14.840566195084168\n",
      "SubSGD iter. 286/499: loss=3.9446369362594353, w0=72.10000000000014, w1=15.680593766543979\n",
      "SubSGD iter. 287/499: loss=5.2297932897471355, w0=72.80000000000014, w1=16.398757090575167\n",
      "SubSGD iter. 288/499: loss=0.24640693299376437, w0=72.10000000000014, w1=16.288457057114375\n",
      "SubSGD iter. 289/499: loss=3.4710029000258587, w0=71.40000000000013, w1=16.840428489052055\n",
      "SubSGD iter. 290/499: loss=7.102006070010447, w0=72.10000000000014, w1=16.877715800669925\n",
      "SubSGD iter. 291/499: loss=2.2029660953937054, w0=72.80000000000014, w1=17.32789554687622\n",
      "SubSGD iter. 292/499: loss=8.994016140789022, w0=73.50000000000014, w1=16.803262622164482\n",
      "SubSGD iter. 293/499: loss=2.1023782061851293, w0=72.80000000000014, w1=17.229048769455222\n",
      "SubSGD iter. 294/499: loss=11.5499445280292, w0=73.50000000000014, w1=16.20211657186269\n",
      "SubSGD iter. 295/499: loss=4.782825790552387, w0=74.20000000000014, w1=15.617804034316798\n",
      "SubSGD iter. 296/499: loss=1.3640989735490336, w0=73.50000000000014, w1=14.892783844235995\n",
      "SubSGD iter. 297/499: loss=8.773967418179367, w0=72.80000000000014, w1=14.63207162285413\n",
      "SubSGD iter. 298/499: loss=0.8869433857703797, w0=73.50000000000014, w1=13.44806942246024\n",
      "SubSGD iter. 299/499: loss=6.82504252110018, w0=72.80000000000014, w1=14.072885522271186\n",
      "SubSGD iter. 300/499: loss=5.649040877821321, w0=73.50000000000014, w1=14.715815221034982\n",
      "SubSGD iter. 301/499: loss=0.28808660283816323, w0=74.20000000000014, w1=15.245516195097098\n",
      "SubSGD iter. 302/499: loss=9.605341182102578, w0=73.50000000000014, w1=14.984803973715234\n",
      "SubSGD iter. 303/499: loss=5.7526516330296715, w0=72.80000000000014, w1=15.105308525194276\n",
      "SubSGD iter. 304/499: loss=0.944452497289241, w0=72.10000000000014, w1=16.006940789607036\n",
      "SubSGD iter. 305/499: loss=7.712863700505174, w0=72.80000000000014, w1=15.576609201146299\n",
      "SubSGD iter. 306/499: loss=8.284168583611475, w0=73.50000000000014, w1=15.772953794663527\n",
      "SubSGD iter. 307/499: loss=0.22407741492563105, w0=74.20000000000014, w1=15.938760054231023\n",
      "SubSGD iter. 308/499: loss=2.378529393756196, w0=74.90000000000015, w1=16.67072464773866\n",
      "SubSGD iter. 309/499: loss=0.35678100347647046, w0=75.60000000000015, w1=16.317608896171343\n",
      "SubSGD iter. 310/499: loss=6.515685996218487, w0=74.90000000000015, w1=16.740848320943858\n",
      "SubSGD iter. 311/499: loss=1.0621977212496816, w0=74.20000000000014, w1=17.525081048983182\n",
      "SubSGD iter. 312/499: loss=9.32471957993448, w0=73.50000000000014, w1=17.101539795929167\n",
      "SubSGD iter. 313/499: loss=6.230794323579026, w0=72.80000000000014, w1=17.251996472380295\n",
      "SubSGD iter. 314/499: loss=2.090995346477328, w0=73.50000000000014, w1=17.868383421217544\n",
      "SubSGD iter. 315/499: loss=5.66094380066037, w0=74.20000000000014, w1=16.684381220823653\n",
      "SubSGD iter. 316/499: loss=2.099959954338132, w0=74.90000000000015, w1=17.40254454485484\n",
      "SubSGD iter. 317/499: loss=5.30244803305046, w0=74.20000000000014, w1=17.892228078251428\n",
      "SubSGD iter. 318/499: loss=9.546865169693518, w0=73.50000000000014, w1=17.468686825197413\n",
      "SubSGD iter. 319/499: loss=2.8891031340000666, w0=74.20000000000014, w1=17.99286737664913\n",
      "SubSGD iter. 320/499: loss=2.055696909625617, w0=74.90000000000015, w1=17.19303664258642\n",
      "SubSGD iter. 321/499: loss=8.949405741475132, w0=74.20000000000014, w1=17.41029364676617\n",
      "SubSGD iter. 322/499: loss=2.4702137794634638, w0=73.50000000000014, w1=18.251712156492115\n",
      "SubSGD iter. 323/499: loss=1.909661084578076, w0=72.80000000000014, w1=18.022590464205358\n",
      "SubSGD iter. 324/499: loss=5.103746814040434, w0=73.50000000000014, w1=17.25176739813717\n",
      "SubSGD iter. 325/499: loss=9.652561478952023, w0=72.80000000000014, w1=16.991055176755303\n",
      "SubSGD iter. 326/499: loss=4.276705948510724, w0=73.50000000000014, w1=17.15650796868793\n",
      "SubSGD iter. 327/499: loss=8.547899063319718, w0=74.20000000000014, w1=17.400820692397065\n",
      "SubSGD iter. 328/499: loss=7.650842409816363, w0=74.90000000000015, w1=18.52595065252737\n",
      "SubSGD iter. 329/499: loss=9.741579592964868, w0=75.60000000000015, w1=18.870418899397837\n",
      "SubSGD iter. 330/499: loss=11.15791000603592, w0=76.30000000000015, w1=17.843486701805304\n",
      "SubSGD iter. 331/499: loss=4.21050029889534, w0=75.60000000000015, w1=17.499774080621915\n",
      "SubSGD iter. 332/499: loss=5.437694936633889, w0=74.90000000000015, w1=18.004791812446236\n",
      "SubSGD iter. 333/499: loss=4.536405163006123, w0=74.20000000000014, w1=17.27977162236543\n",
      "SubSGD iter. 334/499: loss=4.533935712258561, w0=73.50000000000014, w1=17.703011047137945\n",
      "SubSGD iter. 335/499: loss=1.848056756407928, w0=74.20000000000014, w1=16.759504563600824\n",
      "SubSGD iter. 336/499: loss=7.279146412708997, w0=74.90000000000015, w1=16.40956663212051\n",
      "SubSGD iter. 337/499: loss=1.4333433535002698, w0=74.20000000000014, w1=17.193799360159833\n",
      "SubSGD iter. 338/499: loss=5.102139092445782, w0=73.50000000000014, w1=16.601481398891654\n",
      "SubSGD iter. 339/499: loss=5.770380128146371, w0=74.20000000000014, w1=17.142438450862794\n",
      "SubSGD iter. 340/499: loss=0.06829316909389149, w0=74.90000000000015, w1=17.79449534830525\n",
      "SubSGD iter. 341/499: loss=6.00557948162519, w0=75.60000000000015, w1=18.23441979118943\n",
      "SubSGD iter. 342/499: loss=0.6437024028475378, w0=74.90000000000015, w1=17.221741449836664\n",
      "SubSGD iter. 343/499: loss=0.8411513416976248, w0=74.20000000000014, w1=17.322018549076873\n",
      "SubSGD iter. 344/499: loss=4.704288306821752, w0=74.90000000000015, w1=16.74900846606225\n",
      "SubSGD iter. 345/499: loss=4.616134060895021, w0=74.20000000000014, w1=17.361806637063662\n",
      "SubSGD iter. 346/499: loss=2.844808929818612, w0=73.50000000000014, w1=17.31948008046017\n",
      "SubSGD iter. 347/499: loss=7.650030903684225, w0=74.20000000000014, w1=17.07850016579352\n",
      "SubSGD iter. 348/499: loss=2.5436744936779263, w0=73.50000000000014, w1=16.46458575241594\n",
      "SubSGD iter. 349/499: loss=2.3013984012174404, w0=72.80000000000014, w1=16.904274157501124\n",
      "SubSGD iter. 350/499: loss=3.9205581278886683, w0=72.10000000000014, w1=16.56904610219787\n",
      "SubSGD iter. 351/499: loss=3.900984552550753, w0=71.40000000000013, w1=15.868112104763117\n",
      "SubSGD iter. 352/499: loss=7.845433506796645, w0=70.70000000000013, w1=16.139592286745696\n",
      "SubSGD iter. 353/499: loss=5.588316569923826, w0=70.00000000000013, w1=16.41952127918216\n",
      "SubSGD iter. 354/499: loss=11.309184916584229, w0=70.70000000000013, w1=16.069583347701844\n",
      "SubSGD iter. 355/499: loss=3.193798554947847, w0=71.40000000000013, w1=15.969306248461635\n",
      "SubSGD iter. 356/499: loss=5.0068820455758924, w0=72.10000000000014, w1=16.583967229592023\n",
      "SubSGD iter. 357/499: loss=1.0537006415093089, w0=71.40000000000013, w1=15.964833833647356\n",
      "SubSGD iter. 358/499: loss=7.15479170315173, w0=72.10000000000014, w1=16.51803974972347\n",
      "SubSGD iter. 359/499: loss=8.27705780216369, w0=72.80000000000014, w1=16.12206350650164\n",
      "SubSGD iter. 360/499: loss=6.7744377986917215, w0=73.50000000000014, w1=14.768106612468411\n",
      "SubSGD iter. 361/499: loss=2.562505462795329, w0=74.20000000000014, w1=16.133476065132022\n",
      "SubSGD iter. 362/499: loss=1.5690825779472206, w0=73.50000000000014, w1=16.754527638702616\n",
      "SubSGD iter. 363/499: loss=3.00738670672456, w0=74.20000000000014, w1=15.983704572634428\n",
      "SubSGD iter. 364/499: loss=6.490180806699016, w0=74.90000000000015, w1=15.74272465796778\n",
      "SubSGD iter. 365/499: loss=0.8526930447780288, w0=75.60000000000015, w1=14.765927179025862\n",
      "SubSGD iter. 366/499: loss=1.637392133040379, w0=74.90000000000015, w1=14.600120919458366\n",
      "SubSGD iter. 367/499: loss=2.2011681706559614, w0=74.20000000000014, w1=13.854135265870969\n",
      "SubSGD iter. 368/499: loss=3.1313799871737586, w0=74.90000000000015, w1=14.506192163313424\n",
      "SubSGD iter. 369/499: loss=1.156684878104457, w0=74.20000000000014, w1=15.109958233279936\n",
      "SubSGD iter. 370/499: loss=1.2772642348183751, w0=73.50000000000014, w1=14.581902417769287\n",
      "SubSGD iter. 371/499: loss=3.311189457973967, w0=72.80000000000014, w1=13.880968420334534\n",
      "SubSGD iter. 372/499: loss=0.22010516392710144, w0=72.10000000000014, w1=12.797093023643232\n",
      "SubSGD iter. 373/499: loss=0.12399862471129097, w0=71.40000000000013, w1=12.096159026208479\n",
      "SubSGD iter. 374/499: loss=1.6649027900105153, w0=72.10000000000014, w1=12.384656290765895\n",
      "SubSGD iter. 375/499: loss=2.4266501798158515, w0=72.80000000000014, w1=12.55046255033339\n",
      "SubSGD iter. 376/499: loss=2.8275077298175546, w0=73.50000000000014, w1=13.164376963710973\n",
      "SubSGD iter. 377/499: loss=0.5708458357110544, w0=74.20000000000014, w1=13.78351035965564\n",
      "SubSGD iter. 378/499: loss=7.271275843332383, w0=73.50000000000014, w1=14.13059434527563\n",
      "SubSGD iter. 379/499: loss=7.876671981770741, w0=74.20000000000014, w1=14.743586842459191\n",
      "SubSGD iter. 380/499: loss=1.626660192124092, w0=74.90000000000015, w1=14.71115783469183\n",
      "SubSGD iter. 381/499: loss=1.5566746543400711, w0=75.60000000000015, w1=16.251220173079016\n",
      "SubSGD iter. 382/499: loss=8.290442022960057, w0=74.90000000000015, w1=16.49524699778817\n",
      "SubSGD iter. 383/499: loss=112.18727848385458, w0=75.60000000000015, w1=13.72494730089918\n",
      "SubSGD iter. 384/499: loss=8.425414709311127, w0=74.90000000000015, w1=13.301406047845164\n",
      "SubSGD iter. 385/499: loss=2.465755478371449, w0=74.20000000000014, w1=12.670240707850311\n",
      "SubSGD iter. 386/499: loss=0.7827522506308924, w0=73.50000000000014, w1=12.441119015563553\n",
      "SubSGD iter. 387/499: loss=7.518680433373142, w0=72.80000000000014, w1=12.685145840272703\n",
      "SubSGD iter. 388/499: loss=12.010078356739207, w0=72.10000000000014, w1=13.365745017928786\n",
      "SubSGD iter. 389/499: loss=2.1431000561853537, w0=72.80000000000014, w1=12.011788123895558\n",
      "SubSGD iter. 390/499: loss=1.5152033137987644, w0=72.10000000000014, w1=12.782611189963745\n",
      "SubSGD iter. 391/499: loss=0.3052147443984694, w0=72.80000000000014, w1=12.407025351677175\n",
      "SubSGD iter. 392/499: loss=2.822988338342668, w0=73.50000000000014, w1=13.613041641447623\n",
      "SubSGD iter. 393/499: loss=5.371394146073129, w0=74.20000000000014, w1=14.255971340211419\n",
      "SubSGD iter. 394/499: loss=4.38868718457509, w0=73.50000000000014, w1=14.695659745296604\n",
      "SubSGD iter. 395/499: loss=5.489068788602239, w0=74.20000000000014, w1=14.168471106549378\n",
      "SubSGD iter. 396/499: loss=2.514749635937889, w0=74.90000000000015, w1=14.963640831666725\n",
      "SubSGD iter. 397/499: loss=5.407406937484112, w0=75.60000000000015, w1=14.276946196160589\n",
      "SubSGD iter. 398/499: loss=4.616226572955107, w0=74.90000000000015, w1=14.897997769731182\n",
      "SubSGD iter. 399/499: loss=3.7054746693353593, w0=75.60000000000015, w1=14.935285081349052\n",
      "SubSGD iter. 400/499: loss=5.132465630869575, w0=74.90000000000015, w1=16.025080104422273\n",
      "SubSGD iter. 401/499: loss=1.0846486046844888, w0=74.20000000000014, w1=14.90676988381079\n",
      "SubSGD iter. 402/499: loss=4.948061455100856, w0=74.90000000000015, w1=14.379581245063564\n",
      "SubSGD iter. 403/499: loss=0.8574798763263516, w0=74.20000000000014, w1=13.849880271001448\n",
      "SubSGD iter. 404/499: loss=3.857817381172339, w0=73.50000000000014, w1=13.514652215698193\n",
      "SubSGD iter. 405/499: loss=8.496972712313536, w0=74.20000000000014, w1=12.89551106028381\n",
      "SubSGD iter. 406/499: loss=6.851716003681986, w0=73.50000000000014, w1=11.937375969382003\n",
      "SubSGD iter. 407/499: loss=2.0196858302483633, w0=72.80000000000014, w1=13.291332863415231\n",
      "SubSGD iter. 408/499: loss=3.0531009120807155, w0=73.50000000000014, w1=12.70702032586934\n",
      "SubSGD iter. 409/499: loss=14.005060633372437, w0=74.20000000000014, w1=13.051488572739808\n",
      "SubSGD iter. 410/499: loss=4.389764271943264, w0=74.90000000000015, w1=13.667875521577058\n",
      "SubSGD iter. 411/499: loss=1.814716441455289, w0=74.20000000000014, w1=14.364460272817821\n",
      "SubSGD iter. 412/499: loss=2.548874911649385, w0=74.90000000000015, w1=13.78014773527193\n",
      "SubSGD iter. 413/499: loss=1.3737888056032403, w0=74.20000000000014, w1=13.161014339327263\n",
      "SubSGD iter. 414/499: loss=5.600071303996472, w0=73.50000000000014, w1=13.556451131270268\n",
      "SubSGD iter. 415/499: loss=1.914712365229505, w0=72.80000000000014, w1=13.514124574666777\n",
      "SubSGD iter. 416/499: loss=2.6367736627196052, w0=72.10000000000014, w1=13.159097097136952\n",
      "SubSGD iter. 417/499: loss=7.453385340421292, w0=71.40000000000013, w1=13.781883503957244\n",
      "SubSGD iter. 418/499: loss=11.84329009911201, w0=70.70000000000013, w1=13.901775317445576\n",
      "SubSGD iter. 419/499: loss=4.169933516435762, w0=71.40000000000013, w1=13.718309538395157\n",
      "SubSGD iter. 420/499: loss=0.5409577188663093, w0=72.10000000000014, w1=13.850633064748996\n",
      "SubSGD iter. 421/499: loss=0.24181570307345623, w0=72.80000000000014, w1=14.139130329306411\n",
      "SubSGD iter. 422/499: loss=0.13645564101129537, w0=73.50000000000014, w1=14.36825202159317\n",
      "SubSGD iter. 423/499: loss=2.212693313897887, w0=72.80000000000014, w1=14.103466912402402\n",
      "SubSGD iter. 424/499: loss=3.190477445276805, w0=73.50000000000014, w1=13.54935983840582\n",
      "SubSGD iter. 425/499: loss=3.605337779247897, w0=74.20000000000014, w1=12.51184564459228\n",
      "SubSGD iter. 426/499: loss=4.046234438525005, w0=74.90000000000015, w1=12.5382182952171\n",
      "SubSGD iter. 427/499: loss=3.0462704574309356, w0=75.60000000000015, w1=11.798885495773156\n",
      "SubSGD iter. 428/499: loss=1.893888222130819, w0=74.90000000000015, w1=12.652731766198425\n",
      "SubSGD iter. 429/499: loss=0.057897707590470304, w0=74.20000000000014, w1=13.206838840195006\n",
      "SubSGD iter. 430/499: loss=3.8914746723016265, w0=74.90000000000015, w1=12.77650725173427\n",
      "SubSGD iter. 431/499: loss=1.6076611430329564, w0=74.20000000000014, w1=13.129623003301583\n",
      "SubSGD iter. 432/499: loss=0.9708686030926827, w0=73.50000000000014, w1=13.14820246220046\n",
      "SubSGD iter. 433/499: loss=1.4512728829603248, w0=72.80000000000014, w1=13.015878935846622\n",
      "SubSGD iter. 434/499: loss=1.510728806987018, w0=72.10000000000014, w1=13.009451054968276\n",
      "SubSGD iter. 435/499: loss=2.4072858095092613, w0=72.80000000000014, w1=13.537506870478925\n",
      "SubSGD iter. 436/499: loss=4.8524277832130664, w0=72.10000000000014, w1=14.356165240284554\n",
      "SubSGD iter. 437/499: loss=2.225752761543056, w0=71.40000000000013, w1=14.79585364536974\n",
      "SubSGD iter. 438/499: loss=5.132095832688506, w0=72.10000000000014, w1=15.80829198323616\n",
      "SubSGD iter. 439/499: loss=12.196218974185044, w0=71.40000000000013, w1=15.928183796724491\n",
      "SubSGD iter. 440/499: loss=0.12420800343287652, w0=70.70000000000013, w1=16.278520730092882\n",
      "SubSGD iter. 441/499: loss=2.4164007526863145, w0=71.40000000000013, w1=17.643890182756493\n",
      "SubSGD iter. 442/499: loss=5.788767897166032, w0=70.70000000000013, w1=18.324489360412578\n",
      "SubSGD iter. 443/499: loss=3.76019078814096, w0=70.00000000000013, w1=18.8025375628387\n",
      "SubSGD iter. 444/499: loss=2.6671306026265853, w0=70.70000000000013, w1=18.362849157753516\n",
      "SubSGD iter. 445/499: loss=9.604546625715344, w0=71.40000000000013, w1=19.487979117883825\n",
      "SubSGD iter. 446/499: loss=12.768167794535131, w0=72.10000000000014, w1=19.83244736475429\n",
      "SubSGD iter. 447/499: loss=2.1984739952075216, w0=72.80000000000014, w1=19.098333041753303\n",
      "SubSGD iter. 448/499: loss=14.29227053613809, w0=73.50000000000014, w1=18.07140084416077\n",
      "SubSGD iter. 449/499: loss=8.457451067265723, w0=72.80000000000014, w1=16.708649448769023\n",
      "SubSGD iter. 450/499: loss=8.65372336458124, w0=73.50000000000014, w1=16.358711517288707\n",
      "SubSGD iter. 451/499: loss=0.7094218073442278, w0=72.80000000000014, w1=16.46888563416945\n",
      "SubSGD iter. 452/499: loss=3.993788858787049, w0=73.50000000000014, w1=15.284883433775558\n",
      "SubSGD iter. 453/499: loss=1.6219679908391385, w0=72.80000000000014, w1=15.905935007346152\n",
      "SubSGD iter. 454/499: loss=4.29860498326741, w0=73.50000000000014, w1=16.62409833137734\n",
      "SubSGD iter. 455/499: loss=2.5662331170616284, w0=72.80000000000014, w1=17.358212654378327\n",
      "SubSGD iter. 456/499: loss=4.75791056016871, w0=73.50000000000014, w1=16.679865658403784\n",
      "SubSGD iter. 457/499: loss=2.2473606277885665, w0=74.20000000000014, w1=17.30335378068996\n",
      "SubSGD iter. 458/499: loss=7.659317956679828, w0=74.90000000000015, w1=15.949396886656732\n",
      "SubSGD iter. 459/499: loss=1.2556597113519246, w0=75.60000000000015, w1=16.37691397187415\n",
      "SubSGD iter. 460/499: loss=0.4914321074183903, w0=74.90000000000015, w1=16.730029723441465\n",
      "SubSGD iter. 461/499: loss=3.0012342416190307, w0=75.60000000000015, w1=16.39366676576539\n",
      "SubSGD iter. 462/499: loss=4.667883088179622, w0=76.30000000000015, w1=15.866478127018162\n",
      "SubSGD iter. 463/499: loss=0.5016780877142395, w0=77.00000000000016, w1=16.509407825781956\n",
      "SubSGD iter. 464/499: loss=4.738454129582372, w0=76.30000000000015, w1=16.94041851053057\n",
      "SubSGD iter. 465/499: loss=6.839118020612496, w0=75.60000000000015, w1=17.363657935303085\n",
      "SubSGD iter. 466/499: loss=7.158749800629153, w0=76.30000000000015, w1=16.326143741489545\n",
      "SubSGD iter. 467/499: loss=4.294712563889604, w0=75.60000000000015, w1=15.798087925978896\n",
      "SubSGD iter. 468/499: loss=3.8410723044277546, w0=74.90000000000015, w1=16.924020339480066\n",
      "SubSGD iter. 469/499: loss=3.646616046127434, w0=74.20000000000014, w1=16.91759245860172\n",
      "SubSGD iter. 470/499: loss=120.75822340769061, w0=74.90000000000015, w1=13.544448781012044\n",
      "SubSGD iter. 471/499: loss=5.305637162563713, w0=74.20000000000014, w1=13.728609559330364\n",
      "SubSGD iter. 472/499: loss=0.5926212226713545, w0=74.90000000000015, w1=14.453629749411167\n",
      "SubSGD iter. 473/499: loss=0.9135135404417412, w0=74.20000000000014, w1=13.92392877534905\n",
      "SubSGD iter. 474/499: loss=5.442161819574466, w0=73.50000000000014, w1=14.4508858111179\n",
      "SubSGD iter. 475/499: loss=1.40907690538576, w0=72.80000000000014, w1=13.088134415726152\n",
      "SubSGD iter. 476/499: loss=7.072520315633042, w0=72.10000000000014, w1=13.47341855605428\n",
      "SubSGD iter. 477/499: loss=0.17914690273259737, w0=72.80000000000014, w1=14.104583896049133\n",
      "SubSGD iter. 478/499: loss=5.847737912692395, w0=73.50000000000014, w1=14.141871207667004\n",
      "SubSGD iter. 479/499: loss=10.614919496638684, w0=72.80000000000014, w1=14.413351389649582\n",
      "SubSGD iter. 480/499: loss=2.9356477895051114, w0=73.50000000000014, w1=14.683489839939252\n",
      "SubSGD iter. 481/499: loss=2.162724052537584, w0=72.80000000000014, w1=13.599614443247951\n",
      "SubSGD iter. 482/499: loss=4.549336029926053, w0=73.50000000000014, w1=14.245761676635173\n",
      "SubSGD iter. 483/499: loss=9.145656704971792, w0=72.80000000000014, w1=14.525690669071636\n",
      "SubSGD iter. 484/499: loss=3.696735015178433, w0=72.10000000000014, w1=15.344349038877265\n",
      "SubSGD iter. 485/499: loss=0.2523476893502874, w0=72.80000000000014, w1=16.185539207101908\n",
      "SubSGD iter. 486/499: loss=4.838607426798646, w0=73.50000000000014, w1=15.631432133105326\n",
      "SubSGD iter. 487/499: loss=0.8254984065877267, w0=72.80000000000014, w1=15.521132099644534\n",
      "SubSGD iter. 488/499: loss=8.296793333898549, w0=73.50000000000014, w1=14.78179930020059\n",
      "SubSGD iter. 489/499: loss=0.8597258555283247, w0=72.80000000000014, w1=14.162665904255922\n",
      "SubSGD iter. 490/499: loss=0.3650249228167439, w0=73.50000000000014, w1=14.908651557843319\n",
      "SubSGD iter. 491/499: loss=1.27167935610683, w0=74.20000000000014, w1=14.884685718040227\n",
      "SubSGD iter. 492/499: loss=2.628624841854105, w0=73.50000000000014, w1=15.786317982452985\n",
      "SubSGD iter. 493/499: loss=3.721325783564552, w0=74.20000000000014, w1=16.504481306484173\n",
      "SubSGD iter. 494/499: loss=5.400661165371709, w0=73.50000000000014, w1=17.056452738421854\n",
      "SubSGD iter. 495/499: loss=5.418780193055483, w0=74.20000000000014, w1=17.597409790392994\n",
      "SubSGD iter. 496/499: loss=6.406446636868949, w0=73.50000000000014, w1=16.692234687205932\n",
      "SubSGD iter. 497/499: loss=2.498515962425145, w0=72.80000000000014, w1=15.930877736586863\n",
      "SubSGD iter. 498/499: loss=9.645467911408524, w0=73.50000000000014, w1=14.90394553899433\n",
      "SubSGD iter. 499/499: loss=6.7254962655059245, w0=72.80000000000014, w1=15.46206924274337\n",
      "SubSGD: execution time=0.038 seconds\n"
     ]
    }
   ],
   "source": [
    "# Define the parameters of the algorithm.\n",
    "max_iters = 500\n",
    "gamma = 0.7\n",
    "batch_size = 1\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.array([0, 0])\n",
    "\n",
    "# Start SubSGD.\n",
    "start_time = datetime.datetime.now()\n",
    "subsgd_losses, subsgd_ws = stochastic_subgradient_descent(\n",
    "    y, tx, w_initial, batch_size, max_iters, gamma)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"SubSGD: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "601ef20afc564b3fac9612d255cc37ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='n_iter', max=501, min=1), Output()), _dom_classes=('widg"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_figure(n_iter)>"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ipywidgets import IntSlider, interact\n",
    "def plot_figure(n_iter):\n",
    "    fig = gradient_descent_visualization(\n",
    "        subsgd_losses, subsgd_ws, grid_losses, grid_w0, grid_w1, mean_x, std_x, height, weight, n_iter)\n",
    "    fig.set_size_inches(10.0, 6.0)\n",
    "\n",
    "interact(plot_figure, n_iter=IntSlider(min=1, max=len(subsgd_ws)))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "vscode": {
   "interpreter": {
    "hash": "220387e6c3d14f2586cf2004f001028ce90f312409fe8a3fd0eb443ac44e4308"
   }
  },
  "widgets": {
   "state": {
    "d2b2c3aea192430e81437f33ba0b0e69": {
     "views": [
      {
       "cell_index": 22
      }
     ]
    },
    "e4a6a7a70ccd42ddb112989c04f2ed3f": {
     "views": [
      {
       "cell_index": 18
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
